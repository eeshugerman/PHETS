Filename: /home/elliott/programming/PHETS/PH/BuildFiltration.py

Line #    Mem usage    Increment   Line Contents
================================================
    37     71.9 MiB      0.0 MiB   @profile(stream=f)
    38                             def build_filtration(input_file_name, parameter_set, silent=False):
    39     71.9 MiB      0.0 MiB   	num_threads = 2
    40                             	global d
    41     71.9 MiB      0.0 MiB   	d = []
    42                             
    43                             
    44   1191.1 MiB   1119.3 MiB   	def get_param(key):
    45   1191.1 MiB      0.0 MiB   		return parameter_set.get(key)
    46                             
    47     71.9 MiB  -1119.3 MiB   	input_file = open(input_file_name)
    48     71.9 MiB      0.0 MiB   	speed_amplify = float(get_param("d_speed_amplify"))
    49     71.9 MiB      0.0 MiB   	orientation_amplify = float(get_param("d_orientation_amplify"))
    50     71.9 MiB      0.0 MiB   	stretch = float(get_param("d_stretch"))
    51     71.9 MiB      0.0 MiB   	ray_distance_amplify = get_param("d_ray_distance_amplify")
    52     71.9 MiB      0.0 MiB   	use_hamiltonian = float(get_param("d_use_hamiltonian"))
    53     71.9 MiB      0.0 MiB   	m2_d = float(get_param("m2_d"))
    54     71.9 MiB      0.0 MiB   	straight_VB = float(get_param("straight_VB"))
    55     71.9 MiB      0.0 MiB   	d_cov = get_param("d_cov")
    56     71.9 MiB      0.0 MiB   	graph_induced = get_param("graph_induced")
    57     71.9 MiB      0.0 MiB   	always_euclidean = speed_amplify == orientation_amplify == stretch == ray_distance_amplify == 1.0 and use_hamiltonian == d_cov==0.
    58                             
    59                             
    60                             
    61     71.9 MiB      0.0 MiB   	filtration = Set()
    62     71.9 MiB      0.0 MiB   	extra_data = None
    63     71.9 MiB      0.0 MiB   	min_filtration_param = float(get_param("min_filtration_param"))
    64     71.9 MiB      0.0 MiB   	max_filtration_param = float(get_param("max_filtration_param"))
    65     71.9 MiB      0.0 MiB   	print "Max filtration parameter is ", max_filtration_param
    66     71.9 MiB      0.0 MiB   	if max_filtration_param < 0 and min_filtration_param != 0:
    67                             		raise Exception("Argument 'min_filtration_param' is incompatible with automatic max_filtration_param selection.")
    68     71.9 MiB      0.0 MiB   	number_of_vertices = 0
    69     71.9 MiB      0.0 MiB   	start = get_param("start")
    70     71.9 MiB      0.0 MiB   	worm_length = get_param("worm_length")
    71     71.9 MiB      0.0 MiB   	store_top_simplices = get_param("store_top_simplices")
    72     71.9 MiB      0.0 MiB   	sort_output = get_param("sort_output")
    73     71.9 MiB      0.0 MiB   	absolute = get_param("absolute")
    74     71.9 MiB      0.0 MiB   	num_divisions = get_param("num_divisions")
    75     71.9 MiB      0.0 MiB   	simplex_cutoff = get_param("simplex_cutoff")
    76                             
    77                             
    78                             	'''=============== This code written by Sam ======================'''
    79                             
    80                             	## Read data into witness and landmark lists.
    81     71.9 MiB      0.0 MiB   	witnesses = []
    82     71.9 MiB      0.0 MiB   	landmarks = []
    83     71.9 MiB      0.0 MiB   	landmark_indices = []
    84     71.9 MiB      0.0 MiB   	ls = get_param("landmark_selector")
    85     71.9 MiB      0.0 MiB   	downsample_rate = get_param("ds_rate")
    86     71.9 MiB      0.0 MiB   	maxmin = False
    87     71.9 MiB      0.0 MiB   	counter = 0
    88     71.9 MiB      0.0 MiB   	for i in xrange(start):           #  Where to start reading data
    89                             		input_file.readline()
    90                             		counter+=1
    91     71.9 MiB      0.0 MiB   	landmark_indices=[]
    92                             
    93                             
    94     72.3 MiB      0.4 MiB   	for line in input_file.read().split("\n"):
    95     72.3 MiB      0.0 MiB   		if line != "" and counter>=start:
    96     72.3 MiB      0.0 MiB   			string_witness = line.split(" ")
    97     72.3 MiB      0.0 MiB   			witness = []
    98     72.3 MiB      0.0 MiB   			d.append([])
    99     72.3 MiB      0.0 MiB   			for coordinate in string_witness:
   100     72.3 MiB      0.0 MiB   				if coordinate != "":
   101     72.3 MiB      0.0 MiB   					witness.append(float(coordinate))
   102     72.3 MiB      0.0 MiB   			witnesses.append(witness)
   103     72.3 MiB      0.0 MiB   			counter += 1
   104     72.3 MiB      0.0 MiB   			if counter == worm_length:
   105                             				break
   106                             
   107     72.3 MiB      0.0 MiB   	number_of_datapoints = len(witnesses)
   108     72.3 MiB      0.0 MiB   	number_of_vertices = int(number_of_datapoints/downsample_rate)
   109     72.3 MiB      0.0 MiB   	num_coordinates = len(witnesses[0])
   110     72.3 MiB      0.0 MiB   	stop = start + counter
   111                             
   112     72.3 MiB      0.0 MiB   	if max_filtration_param < 0:
   113                             		if float(number_of_vertices) < abs(max_filtration_param) + 1:
   114                             			print '''ERROR: 'max_filtration_param' ({}) and number of landmarks ({})
   115                             			are incompatible. Try decreasing 'ds_rate' or increasing 'worm_length'.'''.format(max_filtration_param,
   116                             			number_of_vertices)
   117                             			sys.exit()
   118                             
   119     72.3 MiB      0.0 MiB   	num_threads = 2
   120                             	# for more information about these parameters type ./find_landmarks --help in the terminal
   121                             	# the distance calculations are calculated and outputted to a file called find_landmarks.txt
   122     72.3 MiB      0.0 MiB   	print os.getcwd()
   123                             
   124     72.3 MiB      0.0 MiB   	if ls=="EST":
   125                             		if always_euclidean:
   126                             			if graph_induced:
   127                             				find_landmarks_cmd = [
   128                             					"./find_landmarks",
   129                             					"-n {}".format(num_threads),
   130                             					"-l {}".format(number_of_vertices),
   131                             					"-w {}-{}".format(start,stop),
   132                             					"-i{}".format(input_file_name),
   133                             					"-olandmark_outputs.txt",
   134                             					"-m {}".format(int(m2_d)),
   135                             					"-a {}".format(speed_amplify),
   136                             					"-y {}".format(orientation_amplify),
   137                             					"-h {}".format(use_hamiltonian),
   138                             					"-r {}".format(ray_distance_amplify),
   139                             					"-v {}".format(straight_VB),
   140                             					"-s {}".format(stretch),
   141                             					"-e {}".format(downsample_rate),
   142                             					"-x {}".format(d_cov),
   143                             					"-c",
   144                             					"-f {}".format(max_filtration_param)
   145                             				]
   146                             			else:
   147                             				find_landmarks_cmd = [
   148                             					"./find_landmarks",
   149                             					"-n {}".format(num_threads),
   150                             					"-l {}".format(number_of_vertices),
   151                             					"-w {}-{}".format(start,stop),
   152                             					"-i{}".format(input_file_name),
   153                             					"-olandmark_outputs.txt",
   154                             					"-m {}".format(int(m2_d)),
   155                             					"-a {}".format(speed_amplify),
   156                             					"-y {}".format(orientation_amplify),
   157                             					"-h {}".format(use_hamiltonian),
   158                             					"-r {}".format(ray_distance_amplify),
   159                             					"-v {}".format(straight_VB),
   160                             					"-s {}".format(stretch),
   161                             					"-e {}".format(downsample_rate),
   162                             					"-x {}".format(d_cov),
   163                             					"-c"
   164                             				]
   165                             		else:
   166                             			if graph_induced:
   167                             				find_landmarks_cmd = [
   168                             					"./find_landmarks",
   169                             					"-n {}".format(num_threads),
   170                             					"-l {}".format(number_of_vertices),
   171                             					"-w {}-{}".format(start,stop),
   172                             					"-i{}".format(input_file_name),
   173                             					"-olandmark_outputs.txt",
   174                             					"-m {}".format(int(m2_d)),
   175                             					"-a {}".format(speed_amplify),
   176                             					"-y {}".format(orientation_amplify),
   177                             					"-h {}".format(use_hamiltonian),
   178                             					"-r {}".format(ray_distance_amplify),
   179                             					"-v {}".format(straight_VB),
   180                             					"-s {}".format(stretch),
   181                             					"-x {}".format(d_cov),
   182                             					"-e {}".format(downsample_rate),
   183                             					"-f {}".format(max_filtration_param)
   184                             				]
   185                             			else:
   186                             				find_landmarks_cmd = [
   187                             					"./find_landmarks",
   188                             					"-n {}".format(num_threads),
   189                             					"-l {}".format(number_of_vertices),
   190                             					"-w {}-{}".format(start,stop),
   191                             					"-i{}".format(input_file_name),
   192                             					"-olandmark_outputs.txt",
   193                             					"-m {}".format(int(m2_d)),
   194                             					"-a {}".format(speed_amplify),
   195                             					"-y {}".format(orientation_amplify),
   196                             					"-h {}".format(use_hamiltonian),
   197                             					"-r {}".format(ray_distance_amplify),
   198                             					"-v {}".format(straight_VB),
   199                             					"-s {}".format(stretch),
   200                             					"-x {}".format(d_cov),
   201                             					"-e {}".format(downsample_rate)
   202                             				]
   203                             	else:
   204     72.3 MiB      0.0 MiB   		if always_euclidean and m2_d!=0:
   205                             			if graph_induced:
   206                             				find_landmarks_cmd = [
   207                             					"./find_landmarks",
   208                             					"-n {}".format(num_threads),
   209                             					"-l {}".format(number_of_vertices),
   210                             					"-w {}-{}".format(start,stop),
   211                             					"-i{}".format(input_file_name),
   212                             					"-olandmark_outputs.txt",
   213                             					"-m {}".format(int(m2_d)),
   214                             					"-a {}".format(speed_amplify),
   215                             					"-y {}".format(orientation_amplify),
   216                             					"-h {}".format(use_hamiltonian),
   217                             					"-r {}".format(ray_distance_amplify),
   218                             					"-v {}".format(straight_VB),
   219                             					"-x {}".format(d_cov),
   220                             					"-s {}".format(stretch),
   221                             					"-f {}".format(max_filtration_param)
   222                             				]
   223                             			else:
   224                             				find_landmarks_cmd = [
   225                             					"./find_landmarks",
   226                             					"-n {}".format(num_threads),
   227                             					"-l {}".format(number_of_vertices),
   228                             					"-w {}-{}".format(start,stop),
   229                             					"-i{}".format(input_file_name),
   230                             					"-olandmark_outputs.txt",
   231                             					"-m {}".format(int(m2_d)),
   232                             					"-a {}".format(speed_amplify),
   233                             					"-y {}".format(orientation_amplify),
   234                             					"-h {}".format(use_hamiltonian),
   235                             					"-r {}".format(ray_distance_amplify),
   236                             					"-v {}".format(straight_VB),
   237                             					"-x {}".format(d_cov),
   238                             					"-s {}".format(stretch)
   239                             				]
   240     72.3 MiB      0.0 MiB   		elif always_euclidean:
   241     72.3 MiB      0.0 MiB   			if graph_induced:
   242                             				find_landmarks_cmd = [
   243                             					"./find_landmarks",
   244                             					"-n {}".format(num_threads),
   245                             					"-l {}".format(number_of_vertices),
   246                             					"-w {}-{}".format(start,stop),
   247                             					"-i{}".format(input_file_name),
   248                             					"-olandmark_outputs.txt",
   249                             					"-m {}".format(int(m2_d)),
   250                             					"-a {}".format(speed_amplify),
   251                             					"-y {}".format(orientation_amplify),
   252                             					"-h {}".format(use_hamiltonian),
   253                             					"-r {}".format(ray_distance_amplify),
   254                             					"-v {}".format(straight_VB),
   255                             					"-x {}".format(d_cov),
   256                             					"-s {}".format(stretch),
   257                             					"-c",
   258                             					"-f {}".format(max_filtration_param)
   259                             				]
   260                             			else:
   261                             				find_landmarks_cmd = [
   262     72.3 MiB      0.0 MiB   					"./find_landmarks",
   263     72.3 MiB      0.0 MiB   					"-n {}".format(num_threads),
   264     72.3 MiB      0.0 MiB   					"-l {}".format(number_of_vertices),
   265     72.3 MiB      0.0 MiB   					"-w {}-{}".format(start,stop),
   266     72.3 MiB      0.0 MiB   					"-i{}".format(input_file_name),
   267     72.3 MiB      0.0 MiB   					"-olandmark_outputs.txt",
   268     72.3 MiB      0.0 MiB   					"-m {}".format(int(m2_d)),
   269     72.3 MiB      0.0 MiB   					"-a {}".format(speed_amplify),
   270     72.3 MiB      0.0 MiB   					"-y {}".format(orientation_amplify),
   271     72.3 MiB      0.0 MiB   					"-h {}".format(use_hamiltonian),
   272     72.3 MiB      0.0 MiB   					"-r {}".format(ray_distance_amplify),
   273     72.3 MiB      0.0 MiB   					"-v {}".format(straight_VB),
   274     72.3 MiB      0.0 MiB   					"-x {}".format(d_cov),
   275     72.3 MiB      0.0 MiB   					"-s {}".format(stretch),
   276     72.3 MiB      0.0 MiB   					"-c"
   277                             				]
   278                             		else:
   279                             			if graph_induced:
   280                             				find_landmarks_cmd = [
   281                             					"./find_landmarks",
   282                             					"-n {}".format(num_threads),
   283                             					"-l {}".format(number_of_vertices),
   284                             					"-w {}-{}".format(start,stop),
   285                             					"-i{}".format(input_file_name),
   286                             					"-olandmark_outputs.txt",
   287                             					"-m {}".format(int(m2_d)),
   288                             					"-a {}".format(speed_amplify),
   289                             					"-y {}".format(orientation_amplify),
   290                             					"-h {}".format(use_hamiltonian),
   291                             					"-r {}".format(ray_distance_amplify),
   292                             					"-v {}".format(straight_VB),
   293                             					"-x {}".format(d_cov),
   294                             					"-s {}".format(stretch),
   295                             					"-f {}".format(max_filtration_param)
   296                             				]
   297                             			else:
   298                             				find_landmarks_cmd = [
   299                             					"./find_landmarks",
   300                             					"-n {}".format(num_threads),
   301                             					"-l {}".format(number_of_vertices),
   302                             					"-w {}-{}".format(start,stop),
   303                             					"-i{}".format(input_file_name),
   304                             					"-olandmark_outputs.txt",
   305                             					"-m {}".format(int(m2_d)),
   306                             					"-a {}".format(speed_amplify),
   307                             					"-y {}".format(orientation_amplify),
   308                             					"-h {}".format(use_hamiltonian),
   309                             					"-r {}".format(ray_distance_amplify),
   310                             					"-v {}".format(straight_VB),
   311                             					"-x {}".format(d_cov),
   312                             					"-s {}".format(stretch)
   313                             				]
   314                             
   315     72.3 MiB      0.0 MiB   	if silent:
   316                             		p = subprocess.Popen(find_landmarks_cmd, stdout=subprocess.PIPE)
   317                             		out, err = p.communicate()
   318                             	else:
   319     72.4 MiB      0.2 MiB   		p = subprocess.Popen(find_landmarks_cmd)
   320     72.4 MiB      0.0 MiB   		p.communicate()
   321                             
   322                             	## Build and sort distance matrix.
   323     72.4 MiB      0.0 MiB   	landmarks_file = open("landmark_outputs.txt","rb")
   324                             
   325     99.2 MiB     26.8 MiB   	l = landmarks_file.readlines()
   326     99.2 MiB      0.0 MiB   	sys.stdout.write("Reading in distance calculations...")
   327     99.4 MiB      0.2 MiB   	sys.stdout.flush()
   328     99.4 MiB      0.0 MiB   	landmark_index = 0
   329   1178.7 MiB   1079.3 MiB   	for line in l:
   330   1178.2 MiB     -0.5 MiB   		f = line.strip('\n')
   331   1178.2 MiB      0.0 MiB   		if "#" not in f:
   332   1178.2 MiB      0.0 MiB   			landmark = int(f.split(":")[0])
   333                             
   334   1178.2 MiB      0.0 MiB   			distances = [float(i) for i in f.split(":")[1].split(",")]
   335   1178.7 MiB      0.5 MiB   			for witness_index in range(0,len(distances)):
   336                             
   337   1178.7 MiB      0.0 MiB   				d[witness_index].append(LandmarkDistance(landmark_index,distances[witness_index]))
   338   1178.7 MiB      0.0 MiB   			landmarks.append(witnesses[landmark])
   339   1178.7 MiB      0.0 MiB   			landmark_indices.append(landmark)
   340   1178.7 MiB      0.0 MiB   			landmark_index+=1
   341                             
   342   1178.7 MiB      0.0 MiB   	assert(len(d)>0)
   343   1178.7 MiB      0.0 MiB   	sys.stdout.write("done\n")
   344   1178.7 MiB      0.0 MiB   	sys.stdout.flush()
   345                             
   346                             
   347                             
   348   1178.7 MiB      0.0 MiB   	sys.stdout.write("Sorting distances...")
   349   1178.7 MiB      0.0 MiB   	sys.stdout.flush()
   350                             
   351                             	# p=multiprocessing.Pool(processes=4)   # commented out by Elliott 4/25
   352                             
   353   1178.7 MiB      0.0 MiB   	inputs=[]
   354   1178.7 MiB      0.0 MiB   	for w in range(0,len(witnesses)):
   355   1178.7 MiB      0.0 MiB   		inputs.append(w)
   356   1178.7 MiB      0.0 MiB   		d[w].sort()
   357                             
   358                             	# p.map(sort,inputs)  # was commented out as of 4/25
   359                             	# p.terminate()       # added by Elliott 4/25
   360                             
   361   1178.7 MiB      0.0 MiB   	sys.stdout.write("done\n")
   362   1178.7 MiB      0.0 MiB   	sys.stdout.flush()
   363   1178.7 MiB      0.0 MiB   	assert len(landmarks) == number_of_vertices
   364                             
   365                             	'''=============== End code written by Sam ======================'''
   366                             
   367                             	'''============= Start code written by Elliott =================='''
   368   1178.7 MiB      0.0 MiB   	if graph_induced:
   369                             		# import matplotlib.pyplot as plt
   370                             		import pandas as pd
   371                             
   372                             		g = nx.read_edgelist('edgelist.txt')
   373                             
   374                             		closest_wits = np.loadtxt('closest_wits.txt', dtype='int')	# witness, landmark
   375                             		wit_coords = np.array(witnesses)
   376                             		land_coords = np.array(landmarks)
   377                             
   378                             		# land = np.unique(closest_wits[:,1])
   379                             
   380                             		# closest_wits = pd.DataFrame(closest_wits, columns=('witness', 'landmark'))
   381                             		# print closest_wits
   382                             		# closest_wits = closest_wits.sort_values(by='landmark')
   383                             		# print closest_wits
   384                             		#
   385                             		# closest_wits = closest_wits.values
   386                             		# print closest_wits
   387                             
   388                             
   389                             
   390                             		# fig = plt.figure(figsize=(8, 8))
   391                             		# ax = fig.add_subplot(111)
   392                             		# ax.scatter(wit_coords[:, 0], wit_coords[:, 1], s=.1)
   393                             		# ax.scatter(land_coords[:, 0], land_coords[:, 1])
   394                             		# fig.savefig('veronoi_test.png')
   395                             
   396                             	'''=============== End code written by Elliott =================='''
   397                             
   398                             
   399                             
   400                             
   401   1178.7 MiB      0.0 MiB   	print("Building filtration...")
   402                             	## Build filtration
   403   1178.7 MiB      0.0 MiB   	weak = get_param("weak")
   404   1178.7 MiB      0.0 MiB   	dimension_cutoff = get_param("dimension_cutoff")
   405   1178.7 MiB      0.0 MiB   	reentry_filter = get_param("reentry_filter")
   406   1178.7 MiB      0.0 MiB   	if get_param("connect_time_1_skeleton") or reentry_filter: # Connect time-1-skeleton
   407                             		for i in xrange(number_of_vertices - 1):
   408                             			filtration.add(SimplexBirth(ImmutableSet([i, i + 1]), 0, sort_output))
   409   1178.7 MiB      0.0 MiB   	use_cliques = get_param("use_cliques")
   410   1178.7 MiB      0.0 MiB   	use_twr = get_param("use_twr")
   411                             
   412   1178.7 MiB      0.0 MiB   	print '%s' % use_twr
   413   1178.7 MiB      0.0 MiB   	if use_cliques: # AKA "Lazy" witness relation.
   414                             		g = nx.Graph()
   415                             		for l in xrange(number_of_vertices):
   416                             			g.add_node(l)
   417   1178.7 MiB      0.0 MiB   	def filter_and_build():
   418                             		g2 = None
   419                             		if reentry_filter:
   420                             			g2 = g.copy()
   421                             			to_remove = Set()
   422                             			for l1 in xrange(number_of_vertices):
   423                             				l2 = l1 + 2
   424                             				while l2 < number_of_vertices and g2.has_edge(l1, l2):
   425                             					to_remove.add(ImmutableSet([l1, l2]))
   426                             					l2 += 1
   427                             			for edge in to_remove:
   428                             				g2.remove_edge(*tuple(edge)) # May cause weird things to happen because removing edges doesn't remove them from the filtration.
   429                             		else:
   430                             			g2 = g
   431                             		for clique in nx.find_cliques(g2):
   432                             			filtration.add(SimplexBirth(clique, q, sort_output))
   433   1178.7 MiB      0.0 MiB   	if weak: # Builds filtration based on k nearest neighbors.
   434                             		if max_filtration_param % 1 != 0:
   435                             			raise Exception("Argument 'max_filtration_param' must be an integer if using the weak witness relation.")
   436                             		max_filtration_param = int(max_filtration_param)
   437                             		for k in xrange(int(math.fabs(max_filtration_param))):
   438                             			for witness_index in xrange(number_of_datapoints):
   439                             				if use_cliques:
   440                             					for i in xrange(k):
   441                             						g.add_edge(d[witness_index][i].id_num, d[witness_index][k].id_num)
   442                             				elif store_top_simplices:
   443                             					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(k + 1)], k, sort_output))
   444                             				else:
   445                             					if progress > 0:
   446                             						for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(k)], min(k, dimension_cutoff)):
   447                             							new_subset = ImmutableSet(base + (d[witness_index][k].id_num,))
   448                             							filtration.add(SimplexBirth(new_subset, k, sort_output))
   449                             			if use_cliques:
   450                             				filter_and_build()
   451   1178.7 MiB      0.0 MiB   	if use_twr:
   452                             		print 'Using TWR'
   453                             		if max_filtration_param < 0: # Automatically determine max.
   454                             			depth = int(-max_filtration_param)
   455                             			min_distance = None
   456                             			for w in xrange(number_of_datapoints):
   457                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   458                             				if min_distance is None or new_distance < min_distance:
   459                             					min_distance = new_distance
   460                             			max_filtration_param = min_distance
   461                             		print 'The max_filtration_param is %d ' % max_filtration_param
   462                             		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   463                             		print 'The step size is %f ' % step
   464                             		print 'There will be %d steps in the filtration' % num_divisions
   465                             		progress_index = [0]*number_of_datapoints
   466                             		done = False
   467                             
   468                             		good_landmarks = [[] for x in range(number_of_datapoints)]
   469                             
   470                             		epsilons = []
   471                             		for q in xrange(num_divisions):
   472                             			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   473                             			print 'The threshold is currently %f' % threshold
   474                             			epsilons.append(threshold)
   475                             			Pre_landmarks = []
   476                             			for witness_index in xrange(number_of_datapoints):
   477                             				pre_landmarks = []
   478                             				add_simplex = False
   479                             				progress = 0
   480                             				while True:
   481                             					progress = progress_index[witness_index]
   482                             					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   483                             						break
   484                             					if progress == number_of_vertices:
   485                             						done = True
   486                             						break
   487                             
   488                             					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   489                             						pre_landmarks.append(d[witness_index][progress].id_num) # PRE_LANDMARKS CONTAINS ID NUMBER
   490                             						progress_index[witness_index] += 1
   491                             					else:
   492                             
   493                             						pre_landmarks_size = len(pre_landmarks)
   494                             						pre_landmarks_string = str(pre_landmarks) # MAKE LIST TO STRING
   495                             						print 'At threshold value %f, witness %d has %d associated landmarks: %s ' % (threshold, witness_index, pre_landmarks_size, pre_landmarks_string)
   496                             						break
   497                             				Pre_landmarks.append(pre_landmarks)
   498                             				Pre_landmarks_size = len(Pre_landmarks)
   499                             
   500                             
   501                             
   502                             			for witness_index in xrange(number_of_datapoints - downsample_rate):
   503                             				if len(Pre_landmarks[witness_index]) == 1:
   504                             					set_range = 1
   505                             				else:
   506                             					set_range = len(Pre_landmarks[witness_index])
   507                             				for k in range(set_range):
   508                             					current_pre_landmark = Pre_landmarks[witness_index][k]
   509                             					next_pre_landmark = Pre_landmarks[witness_index][k]+1 # CHECKS ONE STEP UP FROM ID NUMBER *** SEE JAMIE'S COMMENTS ON SUCH OR CLARIFY ***
   510                             					#	print 'current pre landmark = %d , next pre landmark = %d' % (current_pre_landmark, next_pre_landmark)
   511                             					check_pre_landmark = str(Pre_landmarks[witness_index + downsample_rate]) # HMMMMM
   512                             					#		print 'We are considering the fate of landmark %d witnessed by witness %d...' % (current_pre_landmark, witness_index)
   513                             					#		print 'Should witness %d not witness landmark %d, it will be GONE!' % (witness_index + downsample_rate, current_pre_landmark + 1,)
   514                             					#		print 'Witness %d has landmark set %s' % (witness_index + downsample_rate, check_pre_landmark)
   515                             					print (Pre_landmarks[witness_index][k]) in Pre_landmarks[witness_index]
   516                             					if (Pre_landmarks[witness_index][k]+ 1) in Pre_landmarks[witness_index + downsample_rate]: # change from 1, downsample_rate to 0 to test!
   517                             						good_landmarks[witness_index].append(Pre_landmarks[witness_index][k])
   518                             				print 'Up to threshold value %f, witness %d has landmark set %s' % (threshold, witness_index, str(good_landmarks[witness_index]))
   519                             				if use_cliques:
   520                             					for i in xrange(len(good_landmarks[witness_index])):
   521                             						for j in xrange(i+1,len(good_landmarks[witness_index])):
   522                             							g.add_edge(good_landmarks[witness_index][i], good_landmarks[witness_index][j])
   523                             				else:
   524                             					if not store_top_simplices and len(good_landmarks[witness_index]) > 0:
   525                             						for base in itertools.combinations(good_landmarks[witness_index], min(len(good_landmarks[witness_index]), dimension_cutoff)):
   526                             							new_subset = ImmutableSet(base + (good_landmarks[witness_index][i],))
   527                             							filtration.add(SimplexBirth(new_subset, q, sort_output))
   528                             					add_simplex = True
   529                             				if (not use_cliques) and store_top_simplices and add_simplex and len(good_landmarks[witness_index])>= 2:
   530                             					filtration.add(SimplexBirth([good_landmarks[witness_index][i] for i in xrange(len(good_landmarks[witness_index]))], q, sort_output))
   531                             				if done:
   532                             					break
   533                             			if use_cliques:
   534                             				filter_and_build()
   535                             			if done:
   536                             				break
   537                             			#	print 'We are done with threshold %f' % threshold
   538                             	else:
   539   1178.7 MiB      0.0 MiB   		if max_filtration_param < 0: # Automatically determine max.
   540                             			depth = int(-max_filtration_param)
   541                             			min_distance = None
   542                             			for w in xrange(number_of_datapoints):
   543                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   544                             				if min_distance is None or new_distance < min_distance:
   545                             					min_distance = new_distance
   546                             					if min_distance ==0:
   547                             						print "witness ",w
   548                             			max_filtration_param = min_distance
   549                             
   550   1178.7 MiB      0.0 MiB   		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   551   1178.7 MiB      0.0 MiB   		progress_index = [0]*number_of_datapoints
   552   1178.7 MiB      0.0 MiB   		done = False
   553   1178.7 MiB      0.0 MiB   		epsilons = []
   554   1191.1 MiB     12.4 MiB   		for q in xrange(num_divisions):
   555   1182.8 MiB     -8.4 MiB   			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   556   1182.8 MiB      0.0 MiB   			print 'The threshold is currently %f' % threshold
   557   1182.8 MiB      0.0 MiB   			epsilons.append(threshold)
   558   1191.1 MiB      8.4 MiB   			for witness_index in xrange(number_of_datapoints):
   559   1191.1 MiB      0.0 MiB   				add_simplex = False
   560   1191.1 MiB      0.0 MiB   				progress = 0
   561   1191.1 MiB      0.0 MiB   				while True:
   562   1191.1 MiB      0.0 MiB   					progress = progress_index[witness_index]
   563   1191.1 MiB      0.0 MiB   					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   564                             						break
   565   1191.1 MiB      0.0 MiB   					if progress == number_of_vertices:
   566                             						done = True
   567                             						break
   568   1191.1 MiB      0.0 MiB   					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   569   1191.1 MiB      0.0 MiB   						if use_cliques:
   570                             							for i in xrange(progress):
   571                             								g.add_edge(d[witness_index][i].id_num, d[witness_index][progress].id_num)
   572                             						else:
   573   1191.1 MiB      0.0 MiB   							if not store_top_simplices and progress > 0:
   574                             								for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], min(progress, dimension_cutoff)):
   575                             									new_subset = ImmutableSet(base + (d[witness_index][progress].id_num,))
   576                             									filtration.add(SimplexBirth(new_subset, q, sort_output))
   577   1191.1 MiB      0.0 MiB   							add_simplex = True
   578   1191.1 MiB      0.0 MiB   						progress_index[witness_index] += 1
   579                             					else:
   580   1191.1 MiB      0.0 MiB   						break
   581   1191.1 MiB      0.0 MiB   				if (not use_cliques) and store_top_simplices and add_simplex and progress >= 2:
   582   1191.1 MiB      0.0 MiB   					list_o_landmarks = []
   583   1191.1 MiB      0.0 MiB   					for landmark_index in xrange(progress):
   584   1191.1 MiB      0.0 MiB   						list_o_landmarks.append(d[witness_index][landmark_index].id_num)
   585                             					#print 'At threshold %f, witness %d has landmark set %s' % (threshold, witness_index, str(list_o_landmarks))
   586   1191.1 MiB      0.0 MiB   					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], q, sort_output))
   587   1191.1 MiB      0.0 MiB   				if done:
   588                             					break
   589   1191.1 MiB      0.0 MiB   			if use_cliques:
   590                             				filter_and_build()
   591   1191.1 MiB      0.0 MiB   			if done:
   592                             				break
   593                             
   594                             
   595   1191.1 MiB      0.0 MiB   	extra_data = (landmarks, witnesses)
   596   1191.1 MiB      0.0 MiB   	if weak:
   597                             		max_epsilon = 0.0
   598                             		for w in xrange(number_of_datapoints):
   599                             			#print("type: %s" % type(d[w][max_filtration_param - 1].distance))
   600                             			#print("value: %f" % d[w][max_filtration_param - 1].distance)
   601                             			if (d[w][max_filtration_param - 1].distance) > max_epsilon:
   602                             				max_epsilon = d[w][max_filtration_param - 1].distance
   603                             		print("Done. Filtration contains %i top simplex birth events, with the largest epsilon equal to %f.\n" % (len(filtration), max_epsilon))
   604                             	else:
   605   1191.1 MiB      0.0 MiB   		max_sb_length = 0
   606   1191.1 MiB      0.0 MiB   		for sb in filtration:
   607   1191.1 MiB      0.0 MiB   			if len(sb.landmark_set) > max_sb_length:
   608   1191.1 MiB      0.0 MiB   				max_sb_length = len(sb.landmark_set)
   609   1191.1 MiB      0.0 MiB   		print("Done. Filtration contains %i top simplex birth events, with the largest one comprised of %i landmarks.\nMax filtration parameter: %s.\n" % (len(filtration), max_sb_length, max_filtration_param))
   610                             
   611                             	## Write to output file
   612   1191.1 MiB      0.0 MiB   	output_file_name = get_param("out")
   613                             
   614   1191.1 MiB      0.0 MiB   	if not output_file_name is None:
   615                             		output_file = open(output_file_name, "w")
   616                             		output_file.truncate()
   617                             		program = get_param("program")
   618                             		if dimension_cutoff is None:
   619                             			print("Writing filtration for input into %s..." % program)
   620                             			dimension_cutoff = number_of_vertices
   621                             		else:
   622                             			print("Writing filtration to file %s for input into %s, ignoring simplices above dimension %i..." % (output_file_name,program, dimension_cutoff))
   623                             		num_lines = 0
   624                             		if program == "Perseus":
   625                             			sets_printed_so_far = Set()
   626                             			num_lines = len(filtration) + 1
   627                             			output_file.write("1\n")
   628                             			list_filtration = None
   629                             			if (sort_output):
   630                             				list_filtration = list(filtration)
   631                             				list_filtration.sort()
   632                             			for simplex_birth in (list_filtration if sort_output else filtration):
   633                             				dimension = len(simplex_birth.landmark_set) - 1
   634                             				if dimension > dimension_cutoff:
   635                             					for subtuple in itertools.combinations(simplex_birth.landmark_set, dimension_cutoff + 1):
   636                             						subset = ImmutableSet(subtuple)
   637                             						if not ((subset, simplex_birth.birth_time) in sets_printed_so_far):
   638                             							output_file.write(str(dimension_cutoff) + " ")
   639                             							for landmark in subset:
   640                             								output_file.write(str(landmark + 1) + " ")
   641                             							output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   642                             							sets_printed_so_far.add((subset, simplex_birth.birth_time))
   643                             				else:
   644                             					if not ((simplex_birth.landmark_set, simplex_birth.birth_time) in sets_printed_so_far):
   645                             						output_file.write(str(dimension) + " ")
   646                             						for landmark in (simplex_birth.sll if sort_output else simplex_birth.landmark_set):
   647                             							output_file.write(str(landmark + 1) + " ")
   648                             						output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   649                             						sets_printed_so_far.add((simplex_birth.landmark_set, simplex_birth.birth_time))
   650                             		elif program == "PHAT":
   651                             			line_map = {}
   652                             			for i in xrange(number_of_vertices - 1):
   653                             				output_file.write("0\n")
   654                             				line_map[ImmutableSet([i])] = i
   655                             			output_file.write("0")
   656                             			line_map[ImmutableSet([number_of_vertices - 1])] = number_of_vertices - 1
   657                             			simultaneous_additions = []
   658                             			class Context: # Note: if upgrading to Python 3, one could just use the nonlocal keywo