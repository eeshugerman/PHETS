Filename: /home/elliott/programming/PHETS/PH/BuildFiltration.py

Line #    Mem usage    Increment   Line Contents
================================================
    37    260.1 MiB      0.0 MiB   @profile(stream=f)
    38                             def build_filtration(input_file_name, parameter_set, silent=False):
    39    260.1 MiB      0.0 MiB   	num_threads = 2
    40                             	global d
    41    260.1 MiB      0.0 MiB   	d = []
    42                             
    43                             
    44    311.4 MiB     51.3 MiB   	def get_param(key):
    45    311.4 MiB      0.0 MiB   		return parameter_set.get(key)
    46                             
    47    260.1 MiB    -51.3 MiB   	input_file = open(input_file_name)
    48    260.1 MiB      0.0 MiB   	speed_amplify = float(get_param("d_speed_amplify"))
    49    260.1 MiB      0.0 MiB   	orientation_amplify = float(get_param("d_orientation_amplify"))
    50    260.1 MiB      0.0 MiB   	stretch = float(get_param("d_stretch"))
    51    260.1 MiB      0.0 MiB   	ray_distance_amplify = get_param("d_ray_distance_amplify")
    52    260.1 MiB      0.0 MiB   	use_hamiltonian = get_param("d_use_hamiltonian")
    53    260.1 MiB      0.0 MiB   	print "use hamiltonian set to ", use_hamiltonian
    54    260.1 MiB      0.0 MiB   	m2_d = float(get_param("m2_d"))
    55    260.1 MiB      0.0 MiB   	straight_VB = float(get_param("straight_VB"))
    56    260.1 MiB      0.0 MiB   	d_cov = get_param("d_cov")
    57    260.1 MiB      0.0 MiB   	graph_induced = get_param("graph_induced")
    58                             	# always_euclidean = speed_amplify == orientation_amplify == stretch == ray_distance_amplify == 1.0 and use_hamiltonian == d_cov==0.
    59                             	# print "always_euclidean set to ", always_euclidean
    60    260.1 MiB      0.0 MiB   	always_euclidean = get_param('always_euclidean')
    61                             	# use_hamiltonian = -5
    62                             
    63    260.1 MiB      0.0 MiB   	filtration = Set()
    64    260.1 MiB      0.0 MiB   	extra_data = None
    65    260.1 MiB      0.0 MiB   	min_filtration_param = float(get_param("min_filtration_param"))
    66    260.1 MiB      0.0 MiB   	max_filtration_param = float(get_param("max_filtration_param"))
    67    260.1 MiB      0.0 MiB   	print "Max filtration parameter is ", max_filtration_param
    68    260.1 MiB      0.0 MiB   	if max_filtration_param < 0 and min_filtration_param != 0:
    69                             		raise Exception("Argument 'min_filtration_param' is incompatible with automatic max_filtration_param selection.")
    70    260.1 MiB      0.0 MiB   	number_of_vertices = 0
    71    260.1 MiB      0.0 MiB   	start = get_param("start")
    72    260.1 MiB      0.0 MiB   	worm_length = get_param("worm_length")
    73    260.1 MiB      0.0 MiB   	store_top_simplices = get_param("store_top_simplices")
    74    260.1 MiB      0.0 MiB   	sort_output = get_param("sort_output")
    75    260.1 MiB      0.0 MiB   	absolute = get_param("absolute")
    76    260.1 MiB      0.0 MiB   	num_divisions = get_param("num_divisions")
    77    260.1 MiB      0.0 MiB   	simplex_cutoff = get_param("simplex_cutoff")
    78                             
    79                             
    80                             	'''=============== This code written by Sam ======================'''
    81                             
    82                             	## Read data into witness and landmark lists.
    83    260.1 MiB      0.0 MiB   	witnesses = []
    84    260.1 MiB      0.0 MiB   	landmarks = []
    85    260.1 MiB      0.0 MiB   	landmark_indices = []
    86    260.1 MiB      0.0 MiB   	ls = get_param("landmark_selector")
    87    260.1 MiB      0.0 MiB   	downsample_rate = get_param("ds_rate")
    88    260.1 MiB      0.0 MiB   	maxmin = False
    89    260.1 MiB      0.0 MiB   	counter = 0
    90    260.1 MiB      0.0 MiB   	for i in xrange(start):           #  Where to start reading data
    91                             		input_file.readline()
    92                             		counter+=1
    93    260.1 MiB      0.0 MiB   	landmark_indices=[]
    94                             
    95                             
    96    506.3 MiB    246.3 MiB   	for line in input_file.read().split("\n"):
    97    506.3 MiB      0.0 MiB   		if line != "" and counter>=start:
    98    506.3 MiB      0.0 MiB   			string_witness = line.split(" ")
    99    506.3 MiB      0.0 MiB   			witness = []
   100    506.3 MiB      0.0 MiB   			d.append([])
   101    506.3 MiB      0.0 MiB   			for coordinate in string_witness:
   102    506.3 MiB      0.0 MiB   				if coordinate != "":
   103    506.3 MiB      0.0 MiB   					witness.append(float(coordinate))
   104    506.3 MiB      0.0 MiB   			witnesses.append(witness)
   105    506.3 MiB      0.0 MiB   			counter += 1
   106    506.3 MiB      0.0 MiB   			if counter == worm_length:
   107    277.6 MiB   -228.7 MiB   				break
   108                             
   109    277.6 MiB      0.0 MiB   	number_of_datapoints = len(witnesses)
   110    277.6 MiB      0.0 MiB   	number_of_vertices = int(number_of_datapoints/downsample_rate)
   111    277.6 MiB      0.0 MiB   	num_coordinates = len(witnesses[0])
   112    277.6 MiB      0.0 MiB   	stop = start + counter
   113                             
   114    277.6 MiB      0.0 MiB   	if max_filtration_param < 0:
   115                             		if float(number_of_vertices) < abs(max_filtration_param) + 1:
   116                             			print '''ERROR: 'max_filtration_param' ({}) and number of landmarks ({})
   117                             			are incompatible. Try decreasing 'ds_rate' or increasing 'worm_length'.'''.format(max_filtration_param,
   118                             			number_of_vertices)
   119                             			sys.exit()
   120                             
   121    277.6 MiB      0.0 MiB   	num_threads = 2
   122                             	# for more information about these parameters type ./find_landmarks --help in the terminal
   123                             	# the distance calculations are calculated and outputted to a file called find_landmarks.txt
   124    277.6 MiB      0.0 MiB   	print os.getcwd()
   125                             
   126    277.6 MiB      0.0 MiB   	if ls=="EST":
   127                             		if always_euclidean:
   128                             			if graph_induced:
   129                             				find_landmarks_cmd = [
   130                             					"./find_landmarks",
   131                             					"-n {}".format(num_threads),
   132                             					"-l {}".format(number_of_vertices),
   133                             					"-w {}-{}".format(start,stop),
   134                             					"-i{}".format(input_file_name),
   135                             					"-olandmark_outputs.txt",
   136                             					"-m {}".format(int(m2_d)),
   137                             					"-a {}".format(speed_amplify),
   138                             					"-y {}".format(orientation_amplify),
   139                             					"-h {}".format(use_hamiltonian),
   140                             					"-r {}".format(ray_distance_amplify),
   141                             					"-v {}".format(straight_VB),
   142                             					"-s {}".format(stretch),
   143                             					"-e {}".format(downsample_rate),
   144                             					"-x {}".format(d_cov),
   145                             					"-c",
   146                             					"-f {}".format(max_filtration_param)
   147                             				]
   148                             			else:
   149                             				find_landmarks_cmd = [
   150                             					"./find_landmarks",
   151                             					"-n {}".format(num_threads),
   152                             					"-l {}".format(number_of_vertices),
   153                             					"-w {}-{}".format(start,stop),
   154                             					"-i{}".format(input_file_name),
   155                             					"-olandmark_outputs.txt",
   156                             					"-m {}".format(int(m2_d)),
   157                             					"-a {}".format(speed_amplify),
   158                             					"-y {}".format(orientation_amplify),
   159                             					"-h {}".format(use_hamiltonian),
   160                             					"-r {}".format(ray_distance_amplify),
   161                             					"-v {}".format(straight_VB),
   162                             					"-s {}".format(stretch),
   163                             					"-e {}".format(downsample_rate),
   164                             					"-x {}".format(d_cov),
   165                             					"-c"
   166                             				]
   167                             		else:
   168                             			if graph_induced:
   169                             				find_landmarks_cmd = [
   170                             					"./find_landmarks",
   171                             					"-n {}".format(num_threads),
   172                             					"-l {}".format(number_of_vertices),
   173                             					"-w {}-{}".format(start,stop),
   174                             					"-i{}".format(input_file_name),
   175                             					"-olandmark_outputs.txt",
   176                             					"-m {}".format(int(m2_d)),
   177                             					"-a {}".format(speed_amplify),
   178                             					"-y {}".format(orientation_amplify),
   179                             					"-h {}".format(use_hamiltonian),
   180                             					"-r {}".format(ray_distance_amplify),
   181                             					"-v {}".format(straight_VB),
   182                             					"-s {}".format(stretch),
   183                             					"-x {}".format(d_cov),
   184                             					"-e {}".format(downsample_rate),
   185                             					"-f {}".format(max_filtration_param)
   186                             				]
   187                             			else:
   188                             				find_landmarks_cmd = [
   189                             					"./find_landmarks",
   190                             					"-n {}".format(num_threads),
   191                             					"-l {}".format(number_of_vertices),
   192                             					"-w {}-{}".format(start,stop),
   193                             					"-i{}".format(input_file_name),
   194                             					"-olandmark_outputs.txt",
   195                             					"-m {}".format(int(m2_d)),
   196                             					"-a {}".format(speed_amplify),
   197                             					"-y {}".format(orientation_amplify),
   198                             					"-h {}".format(use_hamiltonian),
   199                             					"-r {}".format(ray_distance_amplify),
   200                             					"-v {}".format(straight_VB),
   201                             					"-s {}".format(stretch),
   202                             					"-x {}".format(d_cov),
   203                             					"-e {}".format(downsample_rate)
   204                             				]
   205                             	else:
   206    277.6 MiB      0.0 MiB   		if always_euclidean and m2_d!=0:
   207                             			if graph_induced:
   208                             				find_landmarks_cmd = [
   209                             					"./find_landmarks",
   210                             					"-n {}".format(num_threads),
   211                             					"-l {}".format(number_of_vertices),
   212                             					"-w {}-{}".format(start,stop),
   213                             					"-i{}".format(input_file_name),
   214                             					"-olandmark_outputs.txt",
   215                             					"-m {}".format(int(m2_d)),
   216                             					"-a {}".format(speed_amplify),
   217                             					"-y {}".format(orientation_amplify),
   218                             					"-h {}".format(use_hamiltonian),
   219                             					"-r {}".format(ray_distance_amplify),
   220                             					"-v {}".format(straight_VB),
   221                             					"-x {}".format(d_cov),
   222                             					"-s {}".format(stretch),
   223                             					"-f {}".format(max_filtration_param)
   224                             				]
   225                             			else:
   226                             				find_landmarks_cmd = [
   227                             					"./find_landmarks",
   228                             					"-n {}".format(num_threads),
   229                             					"-l {}".format(number_of_vertices),
   230                             					"-w {}-{}".format(start,stop),
   231                             					"-i{}".format(input_file_name),
   232                             					"-olandmark_outputs.txt",
   233                             					"-m {}".format(int(m2_d)),
   234                             					"-a {}".format(speed_amplify),
   235                             					"-y {}".format(orientation_amplify),
   236                             					"-h {}".format(use_hamiltonian),
   237                             					"-r {}".format(ray_distance_amplify),
   238                             					"-v {}".format(straight_VB),
   239                             					"-x {}".format(d_cov),
   240                             					"-s {}".format(stretch)
   241                             				]
   242    277.6 MiB      0.0 MiB   		elif always_euclidean:
   243    277.6 MiB      0.0 MiB   			if graph_induced:
   244                             				find_landmarks_cmd = [
   245                             					"./find_landmarks",
   246                             					"-n {}".format(num_threads),
   247                             					"-l {}".format(number_of_vertices),
   248                             					"-w {}-{}".format(start,stop),
   249                             					"-i{}".format(input_file_name),
   250                             					"-olandmark_outputs.txt",
   251                             					"-m {}".format(int(m2_d)),
   252                             					"-a {}".format(speed_amplify),
   253                             					"-y {}".format(orientation_amplify),
   254                             					"-h {}".format(use_hamiltonian),
   255                             					"-r {}".format(ray_distance_amplify),
   256                             					"-v {}".format(straight_VB),
   257                             					"-x {}".format(d_cov),
   258                             					"-s {}".format(stretch),
   259                             					"-c",
   260                             					"-f {}".format(max_filtration_param)
   261                             				]
   262                             			else:
   263                             				find_landmarks_cmd = [
   264    277.6 MiB      0.0 MiB   					"./find_landmarks",
   265    277.6 MiB      0.0 MiB   					"-n {}".format(num_threads),
   266    277.6 MiB      0.0 MiB   					"-l {}".format(number_of_vertices),
   267    277.6 MiB      0.0 MiB   					"-w {}-{}".format(start,stop),
   268    277.6 MiB      0.0 MiB   					"-i{}".format(input_file_name),
   269    277.6 MiB      0.0 MiB   					"-olandmark_outputs.txt",
   270    277.6 MiB      0.0 MiB   					"-m {}".format(int(m2_d)),
   271    277.6 MiB      0.0 MiB   					"-a {}".format(speed_amplify),
   272    277.6 MiB      0.0 MiB   					"-y {}".format(orientation_amplify),
   273    277.6 MiB      0.0 MiB   					"-h {}".format(use_hamiltonian),
   274    277.6 MiB      0.0 MiB   					"-r {}".format(ray_distance_amplify),
   275    277.6 MiB      0.0 MiB   					"-v {}".format(straight_VB),
   276    277.6 MiB      0.0 MiB   					"-x {}".format(d_cov),
   277    277.6 MiB      0.0 MiB   					"-s {}".format(stretch),
   278    277.6 MiB      0.0 MiB   					"-c"
   279                             				]
   280                             		else:
   281                             			if graph_induced:
   282                             				find_landmarks_cmd = [
   283                             					"./find_landmarks",
   284                             					"-n {}".format(num_threads),
   285                             					"-l {}".format(number_of_vertices),
   286                             					"-w {}-{}".format(start,stop),
   287                             					"-i{}".format(input_file_name),
   288                             					"-olandmark_outputs.txt",
   289                             					"-m {}".format(int(m2_d)),
   290                             					"-a {}".format(speed_amplify),
   291                             					"-y {}".format(orientation_amplify),
   292                             					"-h {}".format(use_hamiltonian),
   293                             					"-r {}".format(ray_distance_amplify),
   294                             					"-v {}".format(straight_VB),
   295                             					"-x {}".format(d_cov),
   296                             					"-s {}".format(stretch),
   297                             					"-f {}".format(max_filtration_param)
   298                             				]
   299                             			else:
   300                             				find_landmarks_cmd = [
   301                             					"./find_landmarks",
   302                             					"-n {}".format(num_threads),
   303                             					"-l {}".format(number_of_vertices),
   304                             					"-w {}-{}".format(start,stop),
   305                             					"-i{}".format(input_file_name),
   306                             					"-olandmark_outputs.txt",
   307                             					"-m {}".format(int(m2_d)),
   308                             					"-a {}".format(speed_amplify),
   309                             					"-y {}".format(orientation_amplify),
   310                             					"-h {}".format(use_hamiltonian),
   311                             					"-r {}".format(ray_distance_amplify),
   312                             					"-v {}".format(straight_VB),
   313                             					"-x {}".format(d_cov),
   314                             					"-s {}".format(stretch)
   315                             				]
   316                             
   317    277.6 MiB      0.0 MiB   	print find_landmarks_cmd
   318                             
   319    277.6 MiB      0.0 MiB   	if silent:
   320                             		p = subprocess.Popen(find_landmarks_cmd, stdout=subprocess.PIPE)
   321                             		out, err = p.communicate()
   322                             	else:
   323    277.6 MiB      0.0 MiB   		p = subprocess.Popen(find_landmarks_cmd)
   324    277.6 MiB      0.0 MiB   		p.communicate()
   325                             
   326                             	## Build and sort distance matrix.
   327    277.6 MiB      0.0 MiB   	landmarks_file = open("landmark_outputs.txt","rb")
   328                             
   329    277.6 MiB      0.0 MiB   	l = landmarks_file.readlines()
   330    277.6 MiB      0.0 MiB   	sys.stdout.write("Reading in distance calculations...")
   331    277.6 MiB      0.0 MiB   	sys.stdout.flush()
   332    277.6 MiB      0.0 MiB   	landmark_index = 0
   333    311.2 MiB     33.7 MiB   	for line in l:
   334    310.5 MiB     -0.8 MiB   		f = line.strip('\n')
   335    310.5 MiB      0.0 MiB   		if "#" not in f:
   336    310.5 MiB      0.0 MiB   			landmark = int(f.split(":")[0])
   337                             
   338    310.7 MiB      0.2 MiB   			distances = [float(i) for i in f.split(":")[1].split(",")]
   339    311.2 MiB      0.5 MiB   			for witness_index in range(0,len(distances)):
   340                             
   341    311.2 MiB      0.0 MiB   				d[witness_index].append(LandmarkDistance(landmark_index,distances[witness_index]))
   342    311.2 MiB      0.0 MiB   			landmarks.append(witnesses[landmark])
   343    311.2 MiB      0.0 MiB   			landmark_indices.append(landmark)
   344    311.2 MiB      0.0 MiB   			landmark_index+=1
   345                             
   346    311.2 MiB      0.0 MiB   	assert(len(d)>0)
   347    311.2 MiB      0.0 MiB   	sys.stdout.write("done\n")
   348    311.2 MiB      0.0 MiB   	sys.stdout.flush()
   349                             
   350                             
   351                             
   352    311.2 MiB      0.0 MiB   	sys.stdout.write("Sorting distances...")
   353    311.2 MiB      0.0 MiB   	sys.stdout.flush()
   354                             
   355                             	# p=multiprocessing.Pool(processes=4)   # commented out by Elliott 4/25
   356                             
   357    311.2 MiB      0.0 MiB   	inputs=[]
   358    311.2 MiB      0.0 MiB   	for w in range(0,len(witnesses)):
   359    311.2 MiB      0.0 MiB   		inputs.append(w)
   360    311.2 MiB      0.0 MiB   		d[w].sort()
   361                             
   362                             	# p.map(sort,inputs)  # was commented out as of 4/25
   363                             	# p.terminate()       # added by Elliott 4/25
   364                             
   365    311.2 MiB      0.0 MiB   	sys.stdout.write("done\n")
   366    311.2 MiB      0.0 MiB   	sys.stdout.flush()
   367    311.2 MiB      0.0 MiB   	assert len(landmarks) == number_of_vertices
   368                             
   369                             	'''=============== End code written by Sam ======================'''
   370                             
   371                             	'''============= Start code written by Elliott =================='''
   372    311.2 MiB      0.0 MiB   	if graph_induced:
   373                             		# import matplotlib.pyplot as plt
   374                             		import pandas as pd
   375                             
   376                             		g = nx.read_edgelist('edgelist.txt')
   377                             
   378                             		closest_wits = np.loadtxt('closest_wits.txt', dtype='int')	# witness, landmark
   379                             		wit_coords = np.array(witnesses)
   380                             		land_coords = np.array(landmarks)
   381                             
   382                             		# land = np.unique(closest_wits[:,1])
   383                             
   384                             		# closest_wits = pd.DataFrame(closest_wits, columns=('witness', 'landmark'))
   385                             		# print closest_wits
   386                             		# closest_wits = closest_wits.sort_values(by='landmark')
   387                             		# print closest_wits
   388                             		#
   389                             		# closest_wits = closest_wits.values
   390                             		# print closest_wits
   391                             
   392                             
   393                             
   394                             		# fig = plt.figure(figsize=(8, 8))
   395                             		# ax = fig.add_subplot(111)
   396                             		# ax.scatter(wit_coords[:, 0], wit_coords[:, 1], s=.1)
   397                             		# ax.scatter(land_coords[:, 0], land_coords[:, 1])
   398                             		# fig.savefig('veronoi_test.png')
   399                             
   400                             	'''=============== End code written by Elliott =================='''
   401                             
   402                             
   403                             
   404                             
   405    311.2 MiB      0.0 MiB   	print("Building filtration...")
   406                             	## Build filtration
   407    311.2 MiB      0.0 MiB   	weak = get_param("weak")
   408    311.2 MiB      0.0 MiB   	dimension_cutoff = get_param("dimension_cutoff")
   409    311.2 MiB      0.0 MiB   	reentry_filter = get_param("reentry_filter")
   410    311.2 MiB      0.0 MiB   	if get_param("connect_time_1_skeleton") or reentry_filter: # Connect time-1-skeleton
   411                             		for i in xrange(number_of_vertices - 1):
   412                             			filtration.add(SimplexBirth(ImmutableSet([i, i + 1]), 0, sort_output))
   413    311.2 MiB      0.0 MiB   	use_cliques = get_param("use_cliques")
   414    311.2 MiB      0.0 MiB   	use_twr = get_param("use_twr")
   415                             
   416    311.2 MiB      0.0 MiB   	print '%s' % use_twr
   417    311.2 MiB      0.0 MiB   	if use_cliques: # AKA "Lazy" witness relation.
   418                             		g = nx.Graph()
   419                             		for l in xrange(number_of_vertices):
   420                             			g.add_node(l)
   421    311.2 MiB      0.0 MiB   	def filter_and_build():
   422                             		g2 = None
   423                             		if reentry_filter:
   424                             			g2 = g.copy()
   425                             			to_remove = Set()
   426                             			for l1 in xrange(number_of_vertices):
   427                             				l2 = l1 + 2
   428                             				while l2 < number_of_vertices and g2.has_edge(l1, l2):
   429                             					to_remove.add(ImmutableSet([l1, l2]))
   430                             					l2 += 1
   431                             			for edge in to_remove:
   432                             				g2.remove_edge(*tuple(edge)) # May cause weird things to happen because removing edges doesn't remove them from the filtration.
   433                             		else:
   434                             			g2 = g
   435                             		for clique in nx.find_cliques(g2):
   436                             			filtration.add(SimplexBirth(clique, q, sort_output))
   437    311.2 MiB      0.0 MiB   	if weak: # Builds filtration based on k nearest neighbors.
   438                             		if max_filtration_param % 1 != 0:
   439                             			raise Exception("Argument 'max_filtration_param' must be an integer if using the weak witness relation.")
   440                             		max_filtration_param = int(max_filtration_param)
   441                             		for k in xrange(int(math.fabs(max_filtration_param))):
   442                             			for witness_index in xrange(number_of_datapoints):
   443                             				if use_cliques:
   444                             					for i in xrange(k):
   445                             						g.add_edge(d[witness_index][i].id_num, d[witness_index][k].id_num)
   446                             				elif store_top_simplices:
   447                             					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(k + 1)], k, sort_output))
   448                             				else:
   449                             					if progress > 0:
   450                             						for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(k)], min(k, dimension_cutoff)):
   451                             							new_subset = ImmutableSet(base + (d[witness_index][k].id_num,))
   452                             							filtration.add(SimplexBirth(new_subset, k, sort_output))
   453                             			if use_cliques:
   454                             				filter_and_build()
   455    311.2 MiB      0.0 MiB   	if use_twr:
   456                             		print 'Using TWR'
   457                             		if max_filtration_param < 0: # Automatically determine max.
   458                             			depth = int(-max_filtration_param)
   459                             			min_distance = None
   460                             			for w in xrange(number_of_datapoints):
   461                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   462                             				if min_distance is None or new_distance < min_distance:
   463                             					min_distance = new_distance
   464                             			max_filtration_param = min_distance
   465                             		print 'The max_filtration_param is %d ' % max_filtration_param
   466                             		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   467                             		print 'The step size is %f ' % step
   468                             		print 'There will be %d steps in the filtration' % num_divisions
   469                             		progress_index = [0]*number_of_datapoints
   470                             		done = False
   471                             
   472                             		good_landmarks = [[] for x in range(number_of_datapoints)]
   473                             
   474                             		epsilons = []
   475                             		for q in xrange(num_divisions):
   476                             			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   477                             			print 'The threshold is currently %f' % threshold
   478                             			epsilons.append(threshold)
   479                             			Pre_landmarks = []
   480                             			for witness_index in xrange(number_of_datapoints):
   481                             				pre_landmarks = []
   482                             				add_simplex = False
   483                             				progress = 0
   484                             				while True:
   485                             					progress = progress_index[witness_index]
   486                             					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   487                             						break
   488                             					if progress == number_of_vertices:
   489                             						done = True
   490                             						break
   491                             
   492                             					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   493                             						pre_landmarks.append(d[witness_index][progress].id_num) # PRE_LANDMARKS CONTAINS ID NUMBER
   494                             						progress_index[witness_index] += 1
   495                             					else:
   496                             
   497                             						pre_landmarks_size = len(pre_landmarks)
   498                             						pre_landmarks_string = str(pre_landmarks) # MAKE LIST TO STRING
   499                             						print 'At threshold value %f, witness %d has %d associated landmarks: %s ' % (threshold, witness_index, pre_landmarks_size, pre_landmarks_string)
   500                             						break
   501                             				Pre_landmarks.append(pre_landmarks)
   502                             				Pre_landmarks_size = len(Pre_landmarks)
   503                             
   504                             
   505                             
   506                             			for witness_index in xrange(number_of_datapoints - downsample_rate):
   507                             				if len(Pre_landmarks[witness_index]) == 1:
   508                             					set_range = 1
   509                             				else:
   510                             					set_range = len(Pre_landmarks[witness_index])
   511                             				for k in range(set_range):
   512                             					current_pre_landmark = Pre_landmarks[witness_index][k]
   513                             					next_pre_landmark = Pre_landmarks[witness_index][k]+1 # CHECKS ONE STEP UP FROM ID NUMBER *** SEE JAMIE'S COMMENTS ON SUCH OR CLARIFY ***
   514                             					#	print 'current pre landmark = %d , next pre landmark = %d' % (current_pre_landmark, next_pre_landmark)
   515                             					check_pre_landmark = str(Pre_landmarks[witness_index + downsample_rate]) # HMMMMM
   516                             					#		print 'We are considering the fate of landmark %d witnessed by witness %d...' % (current_pre_landmark, witness_index)
   517                             					#		print 'Should witness %d not witness landmark %d, it will be GONE!' % (witness_index + downsample_rate, current_pre_landmark + 1,)
   518                             					#		print 'Witness %d has landmark set %s' % (witness_index + downsample_rate, check_pre_landmark)
   519                             					print (Pre_landmarks[witness_index][k]) in Pre_landmarks[witness_index]
   520                             					if (Pre_landmarks[witness_index][k]+ 1) in Pre_landmarks[witness_index + downsample_rate]: # change from 1, downsample_rate to 0 to test!
   521                             						good_landmarks[witness_index].append(Pre_landmarks[witness_index][k])
   522                             				print 'Up to threshold value %f, witness %d has landmark set %s' % (threshold, witness_index, str(good_landmarks[witness_index]))
   523                             				if use_cliques:
   524                             					for i in xrange(len(good_landmarks[witness_index])):
   525                             						for j in xrange(i+1,len(good_landmarks[witness_index])):
   526                             							g.add_edge(good_landmarks[witness_index][i], good_landmarks[witness_index][j])
   527                             				else:
   528                             					if not store_top_simplices and len(good_landmarks[witness_index]) > 0:
   529                             						for base in itertools.combinations(good_landmarks[witness_index], min(len(good_landmarks[witness_index]), dimension_cutoff)):
   530                             							new_subset = ImmutableSet(base + (good_landmarks[witness_index][i],))
   531                             							filtration.add(SimplexBirth(new_subset, q, sort_output))
   532                             					add_simplex = True
   533                             				if (not use_cliques) and store_top_simplices and add_simplex and len(good_landmarks[witness_index])>= 2:
   534                             					filtration.add(SimplexBirth([good_landmarks[witness_index][i] for i in xrange(len(good_landmarks[witness_index]))], q, sort_output))
   535                             				if done:
   536                             					break
   537                             			if use_cliques:
   538                             				filter_and_build()
   539                             			if done:
   540                             				break
   541                             			#	print 'We are done with threshold %f' % threshold
   542                             	else:
   543    311.2 MiB      0.0 MiB   		if max_filtration_param < 0: # Automatically determine max.
   544                             			depth = int(-max_filtration_param)
   545                             			min_distance = None
   546                             			for w in xrange(number_of_datapoints):
   547                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   548                             				if min_distance is None or new_distance < min_distance:
   549                             					min_distance = new_distance
   550                             					if min_distance ==0:
   551                             						print "witness ",w
   552                             			max_filtration_param = min_distance
   553                             
   554    311.2 MiB      0.0 MiB   		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   555    311.2 MiB      0.0 MiB   		progress_index = [0]*number_of_datapoints
   556    311.2 MiB      0.0 MiB   		done = False
   557    311.2 MiB      0.0 MiB   		epsilons = []
   558    311.4 MiB      0.1 MiB   		for q in xrange(num_divisions):
   559    311.4 MiB      0.0 MiB   			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   560    311.4 MiB      0.0 MiB   			print 'The threshold is currently %f' % threshold
   561    311.4 MiB      0.0 MiB   			epsilons.append(threshold)
   562    311.4 MiB      0.0 MiB   			for witness_index in xrange(number_of_datapoints):
   563    311.4 MiB      0.0 MiB   				add_simplex = False
   564    311.4 MiB      0.0 MiB   				progress = 0
   565    311.4 MiB      0.0 MiB   				while True:
   566    311.4 MiB      0.0 MiB   					progress = progress_index[witness_index]
   567    311.4 MiB      0.0 MiB   					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   568                             						break
   569    311.4 MiB      0.0 MiB   					if progress == number_of_vertices:
   570                             						done = True
   571                             						break
   572    311.4 MiB      0.0 MiB   					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   573    311.4 MiB      0.0 MiB   						if use_cliques:
   574                             							for i in xrange(progress):
   575                             								g.add_edge(d[witness_index][i].id_num, d[witness_index][progress].id_num)
   576                             						else:
   577    311.4 MiB      0.0 MiB   							if not store_top_simplices and progress > 0:
   578                             								for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], min(progress, dimension_cutoff)):
   579                             									new_subset = ImmutableSet(base + (d[witness_index][progress].id_num,))
   580                             									filtration.add(SimplexBirth(new_subset, q, sort_output))
   581    311.4 MiB      0.0 MiB   							add_simplex = True
   582    311.4 MiB      0.0 MiB   						progress_index[witness_index] += 1
   583                             					else:
   584    311.4 MiB      0.0 MiB   						break
   585    311.4 MiB      0.0 MiB   				if (not use_cliques) and store_top_simplices and add_simplex and progress >= 2:
   586    311.4 MiB      0.0 MiB   					list_o_landmarks = []
   587    311.4 MiB      0.0 MiB   					for landmark_index in xrange(progress):
   588    311.4 MiB      0.0 MiB   						list_o_landmarks.append(d[witness_index][landmark_index].id_num)
   589                             					#print 'At threshold %f, witness %d has landmark set %s' % (threshold, witness_index, str(list_o_landmarks))
   590    311.4 MiB      0.0 MiB   					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], q, sort_output))
   591    311.4 MiB      0.0 MiB   				if done:
   592                             					break
   593    311.4 MiB      0.0 MiB   			if use_cliques:
   594                             				filter_and_build()
   595    311.4 MiB      0.0 MiB   			if done:
   596                             				break
   597                             
   598                             
   599    311.4 MiB      0.0 MiB   	extra_data = (landmarks, witnesses)
   600    311.4 MiB      0.0 MiB   	if weak:
   601                             		max_epsilon = 0.0
   602                             		for w in xrange(number_of_datapoints):
   603                             			#print("type: %s" % type(d[w][max_filtration_param - 1].distance))
   604                             			#print("value: %f" % d[w][max_filtration_param - 1].distance)
   605                             			if (d[w][max_filtration_param - 1].distance) > max_epsilon:
   606                             				max_epsilon = d[w][max_filtration_param - 1].distance
   607                             		print("Done. Filtration contains %i top simplex birth events, with the largest epsilon equal to %f.\n" % (len(filtration), max_epsilon))
   608                             	else:
   609    311.4 MiB      0.0 MiB   		max_sb_length = 0
   610    311.4 MiB      0.0 MiB   		for sb in filtration:
   611    311.4 MiB      0.0 MiB   			if len(sb.landmark_set) > max_sb_length:
   612    311.4 MiB      0.0 MiB   				max_sb_length = len(sb.landmark_set)
   613    311.4 MiB      0.0 MiB   		print("Done. Filtration contains %i top simplex birth events, with the largest one comprised of %i landmarks.\nMax filtration parameter: %s.\n" % (len(filtration), max_sb_length, max_filtration_param))
   614                             
   615                             	## Write to output file
   616    311.4 MiB      0.0 MiB   	output_file_name = get_param("out")
   617                             
   618    311.4 MiB      0.0 MiB   	if not output_file_name is None:
   619                             		output_file = open(output_file_name, "w")
   620                             		output_file.truncate()
   621                             		program = get_param("program")
   622                             		if dimension_cutoff is None:
   623                             			print("Writing filtration for input into %s..." % program)
   624                             			dimension_cutoff = number_of_vertices
   625                             		else:
   626                             			print("Writing filtration to file %s for input into %s, ignoring simplices above dimension %i..." % (output_file_name,program, dimension_cutoff))
   627                             		num_lines = 0
   628                             		if program == "Perseus":
   629                             			sets_printed_so_far = Set()
   630                             			num_lines = len(filtration) + 1
   631                             			output_file.write("1\n")
   632                             			list_filtration = None
   633                             			if (sort_output):
   634                             				list_filtration = list(filtration)
   635                             				list_filtration.sort()
   636                             			for simplex_birth in (list_filtration if sort_output else filtration):
   637                             				dimension = len(simplex_birth.landmark_set) - 1
   638                             				if dimension > dimension_cutoff:
   639                             					for subtuple in itertools.combinations(simplex_birth.landmark_set, dimension_cutoff + 1):
   640                             						subset = ImmutableSet(subtuple)
   641                             						if not ((subset, simplex_birth.birth_time) in sets_printed_so_far):
   642                             							output_file.write(str(dimension_cutoff) + " ")
   643                             							for landmark in subset:
   644                             								output_file.write(str(landmark + 1) + " ")
   645                             							output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   646                             							sets_printed_so_far.add((subset, simplex_birth.birth_time))
   647                             				else:
   648                             					if not ((simplex_birth.landmark_set, simplex_birth.birth_time) in sets_printed_so_far):
   649                             						output_file.write(str(dimension) + " ")
   650                             						for landmark in (simplex_birth.sll if sort_output else simplex_birth.landmark_set):
   651                             							output_file.write(str(landmark + 1) + " ")
   652                             						output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   653                             						sets_printed_so_far.add((simplex_birth.landmark_set, simplex_birth.birth_time))
   654                             		elif program == "PHAT":
   655                             			line_map = {}
   656                             			for i in xrange(number_of_vertices - 1):
   657                             				output_file.write("0\n")
   658                             				line_map[ImmutableSet([i])] = i
   659                             			output_file.write("0")
   660                             			line_map[ImmutableSet([number_of_vertices - 1])] = number_of_vertices - 1
   661                             			simultaneous_additions = []
   662                             			class Context: # Note: if upgrading to Python 3, one could just use the nonlocal keyword (see below comment).
   663                             				line_number = number_of_vertices
   664                             			list_filtration = list(filtration)
   665                             			list_filtration.sort()
   666                             			last_birth_time = 0
   667                             			def process_and_get_line_number(s):
   668                             				#nonlocal line_number
   669                             				if s in line_map:
   670                             					return line_map[s]
   671                             				else:
   672                             					dimension = len(s) - 1
   673                             					if dimension > dimension_cutoff:
   674                             						for subset in itertools.combinations(s, dimension_cutoff + 1): # Take all subsets of size dimension_cutoff + 1
   675                             							process_and_get_line_number(ImmutableSet(subset))
   676                             					elif dimension > 0:
   677                             						subsets_line_numbers = []
   678                             						for e in s:
   679                             							subsets_line_numbers.append(process_and_get_line_number(ImmutableSet(s - Set([e]))))
   680                             						output_file.write("\n" + str(dimension))
   681                             						for l in subsets_line_numbers:
   682                             							output_file.write(" " + str(l))
   683                             						line_map[s] = Context.line_number
   684                             						Context.line_number += 1
   685                             						return Context.line_number - 1
   686                             					else:
   687                             						raise Exception("Should have already added single point for base case: " + str(s))
   688                             			for simplex_birth in list_filtration:
   689                             				if simplex_birth.birth_time > last_birth_time:
   690                             					simultaneous_additions.append((Context.line_number - 1, last_birth_time + 1)) # Every line up to and including that line number (indexing starts at 0) had that birth time or earlier (indexing starts at 1)
   691                             					last_birth_time = simplex_birth.birth_time
   692                             				process_and_get_line_number(simplex_birth.landmark_set)
   693                             			simultaneous_additions.append((sys.maxsize, last_birth_time))
   694                             			output_file.write("\n\n# Simultaneous additions: Every line up to and including __ (indexing starts at 0) has birth time __ (or earlier).")
   695                             			for addition in simultaneous_additions:
   696                             				output_file.write("\n# %20i %20i" % addition)
   697                             			extra_data = (extra_data[0], extra_data[1], simultaneous_additions)
   698                             			num_lines = Context.line_number
   699                             		else:
   700                             			raise Exception("Only supported programs are 'Perseus' and 'PHAT'")
   701                             		output_file.close()
   702                             		print("Done. File contains %i lines.\n" % num_lines)
   703    311.4 MiB      0.0 MiB   	print("Filtration has been successfully built!\n")
   704    311.4 MiB      0.0 MiB   	return (filtration, extra_data + (max_filtration_param,), epsilons)


Filename: /home/elliott/programming/PHETS/PH/BuildFiltration.py

Line #    Mem usage    Increment   Line Contents
================================================
    37    472.8 MiB      0.0 MiB   @profile(stream=f)
    38                             def build_filtration(input_file_name, parameter_set, silent=False):
    39    472.8 MiB      0.0 MiB   	num_threads = 2
    40                             	global d
    41    440.8 MiB    -32.0 MiB   	d = []
    42                             
    43                             
    44    471.8 MiB     31.0 MiB   	def get_param(key):
    45    471.8 MiB      0.0 MiB   		return parameter_set.get(key)
    46                             
    47    440.8 MiB    -31.0 MiB   	input_file = open(input_file_name)
    48    440.8 MiB      0.0 MiB   	speed_amplify = float(get_param("d_speed_amplify"))
    49    440.8 MiB      0.0 MiB   	orientation_amplify = float(get_param("d_orientation_amplify"))
    50    440.8 MiB      0.0 MiB   	stretch = float(get_param("d_stretch"))
    51    440.8 MiB      0.0 MiB   	ray_distance_amplify = get_param("d_ray_distance_amplify")
    52    440.8 MiB      0.0 MiB   	use_hamiltonian = get_param("d_use_hamiltonian")
    53    440.8 MiB      0.0 MiB   	print "use hamiltonian set to ", use_hamiltonian
    54    440.8 MiB      0.0 MiB   	m2_d = float(get_param("m2_d"))
    55    440.8 MiB      0.0 MiB   	straight_VB = float(get_param("straight_VB"))
    56    440.8 MiB      0.0 MiB   	d_cov = get_param("d_cov")
    57    440.8 MiB      0.0 MiB   	graph_induced = get_param("graph_induced")
    58                             	# always_euclidean = speed_amplify == orientation_amplify == stretch == ray_distance_amplify == 1.0 and use_hamiltonian == d_cov==0.
    59                             	# print "always_euclidean set to ", always_euclidean
    60    440.8 MiB      0.0 MiB   	always_euclidean = get_param('always_euclidean')
    61                             	# use_hamiltonian = -5
    62                             
    63    440.8 MiB      0.0 MiB   	filtration = Set()
    64    440.8 MiB      0.0 MiB   	extra_data = None
    65    440.8 MiB      0.0 MiB   	min_filtration_param = float(get_param("min_filtration_param"))
    66    440.8 MiB      0.0 MiB   	max_filtration_param = float(get_param("max_filtration_param"))
    67    440.8 MiB      0.0 MiB   	print "Max filtration parameter is ", max_filtration_param
    68    440.8 MiB      0.0 MiB   	if max_filtration_param < 0 and min_filtration_param != 0:
    69                             		raise Exception("Argument 'min_filtration_param' is incompatible with automatic max_filtration_param selection.")
    70    440.8 MiB      0.0 MiB   	number_of_vertices = 0
    71    440.8 MiB      0.0 MiB   	start = get_param("start")
    72    440.8 MiB      0.0 MiB   	worm_length = get_param("worm_length")
    73    440.8 MiB      0.0 MiB   	store_top_simplices = get_param("store_top_simplices")
    74    440.8 MiB      0.0 MiB   	sort_output = get_param("sort_output")
    75    440.8 MiB      0.0 MiB   	absolute = get_param("absolute")
    76    440.8 MiB      0.0 MiB   	num_divisions = get_param("num_divisions")
    77    440.8 MiB      0.0 MiB   	simplex_cutoff = get_param("simplex_cutoff")
    78                             
    79                             
    80                             	'''=============== This code written by Sam ======================'''
    81                             
    82                             	## Read data into witness and landmark lists.
    83    440.8 MiB      0.0 MiB   	witnesses = []
    84    440.8 MiB      0.0 MiB   	landmarks = []
    85    440.8 MiB      0.0 MiB   	landmark_indices = []
    86    440.8 MiB      0.0 MiB   	ls = get_param("landmark_selector")
    87    440.8 MiB      0.0 MiB   	downsample_rate = get_param("ds_rate")
    88    440.8 MiB      0.0 MiB   	maxmin = False
    89    440.8 MiB      0.0 MiB   	counter = 0
    90    440.8 MiB      0.0 MiB   	for i in xrange(start):           #  Where to start reading data
    91                             		input_file.readline()
    92                             		counter+=1
    93    440.8 MiB      0.0 MiB   	landmark_indices=[]
    94                             
    95                             
    96    666.8 MiB    226.0 MiB   	for line in input_file.read().split("\n"):
    97    666.8 MiB      0.0 MiB   		if line != "" and counter>=start:
    98    666.8 MiB      0.0 MiB   			string_witness = line.split(" ")
    99    666.8 MiB      0.0 MiB   			witness = []
   100    666.8 MiB      0.0 MiB   			d.append([])
   101    666.8 MiB      0.0 MiB   			for coordinate in string_witness:
   102    666.8 MiB      0.0 MiB   				if coordinate != "":
   103    666.8 MiB      0.0 MiB   					witness.append(float(coordinate))
   104    666.8 MiB      0.0 MiB   			witnesses.append(witness)
   105    666.8 MiB      0.0 MiB   			counter += 1
   106    666.8 MiB      0.0 MiB   			if counter == worm_length:
   107    441.1 MiB   -225.7 MiB   				break
   108                             
   109    441.1 MiB      0.0 MiB   	number_of_datapoints = len(witnesses)
   110    441.1 MiB      0.0 MiB   	number_of_vertices = int(number_of_datapoints/downsample_rate)
   111    441.1 MiB      0.0 MiB   	num_coordinates = len(witnesses[0])
   112    441.1 MiB      0.0 MiB   	stop = start + counter
   113                             
   114    441.1 MiB      0.0 MiB   	if max_filtration_param < 0:
   115                             		if float(number_of_vertices) < abs(max_filtration_param) + 1:
   116                             			print '''ERROR: 'max_filtration_param' ({}) and number of landmarks ({})
   117                             			are incompatible. Try decreasing 'ds_rate' or increasing 'worm_length'.'''.format(max_filtration_param,
   118                             			number_of_vertices)
   119                             			sys.exit()
   120                             
   121    441.1 MiB      0.0 MiB   	num_threads = 2
   122                             	# for more information about these parameters type ./find_landmarks --help in the terminal
   123                             	# the distance calculations are calculated and outputted to a file called find_landmarks.txt
   124    441.1 MiB      0.0 MiB   	print os.getcwd()
   125                             
   126    441.1 MiB      0.0 MiB   	if ls=="EST":
   127                             		if always_euclidean:
   128                             			if graph_induced:
   129                             				find_landmarks_cmd = [
   130                             					"./find_landmarks",
   131                             					"-n {}".format(num_threads),
   132                             					"-l {}".format(number_of_vertices),
   133                             					"-w {}-{}".format(start,stop),
   134                             					"-i{}".format(input_file_name),
   135                             					"-olandmark_outputs.txt",
   136                             					"-m {}".format(int(m2_d)),
   137                             					"-a {}".format(speed_amplify),
   138                             					"-y {}".format(orientation_amplify),
   139                             					"-h {}".format(use_hamiltonian),
   140                             					"-r {}".format(ray_distance_amplify),
   141                             					"-v {}".format(straight_VB),
   142                             					"-s {}".format(stretch),
   143                             					"-e {}".format(downsample_rate),
   144                             					"-x {}".format(d_cov),
   145                             					"-c",
   146                             					"-f {}".format(max_filtration_param)
   147                             				]
   148                             			else:
   149                             				find_landmarks_cmd = [
   150                             					"./find_landmarks",
   151                             					"-n {}".format(num_threads),
   152                             					"-l {}".format(number_of_vertices),
   153                             					"-w {}-{}".format(start,stop),
   154                             					"-i{}".format(input_file_name),
   155                             					"-olandmark_outputs.txt",
   156                             					"-m {}".format(int(m2_d)),
   157                             					"-a {}".format(speed_amplify),
   158                             					"-y {}".format(orientation_amplify),
   159                             					"-h {}".format(use_hamiltonian),
   160                             					"-r {}".format(ray_distance_amplify),
   161                             					"-v {}".format(straight_VB),
   162                             					"-s {}".format(stretch),
   163                             					"-e {}".format(downsample_rate),
   164                             					"-x {}".format(d_cov),
   165                             					"-c"
   166                             				]
   167                             		else:
   168                             			if graph_induced:
   169                             				find_landmarks_cmd = [
   170                             					"./find_landmarks",
   171                             					"-n {}".format(num_threads),
   172                             					"-l {}".format(number_of_vertices),
   173                             					"-w {}-{}".format(start,stop),
   174                             					"-i{}".format(input_file_name),
   175                             					"-olandmark_outputs.txt",
   176                             					"-m {}".format(int(m2_d)),
   177                             					"-a {}".format(speed_amplify),
   178                             					"-y {}".format(orientation_amplify),
   179                             					"-h {}".format(use_hamiltonian),
   180                             					"-r {}".format(ray_distance_amplify),
   181                             					"-v {}".format(straight_VB),
   182                             					"-s {}".format(stretch),
   183                             					"-x {}".format(d_cov),
   184                             					"-e {}".format(downsample_rate),
   185                             					"-f {}".format(max_filtration_param)
   186                             				]
   187                             			else:
   188                             				find_landmarks_cmd = [
   189                             					"./find_landmarks",
   190                             					"-n {}".format(num_threads),
   191                             					"-l {}".format(number_of_vertices),
   192                             					"-w {}-{}".format(start,stop),
   193                             					"-i{}".format(input_file_name),
   194                             					"-olandmark_outputs.txt",
   195                             					"-m {}".format(int(m2_d)),
   196                             					"-a {}".format(speed_amplify),
   197                             					"-y {}".format(orientation_amplify),
   198                             					"-h {}".format(use_hamiltonian),
   199                             					"-r {}".format(ray_distance_amplify),
   200                             					"-v {}".format(straight_VB),
   201                             					"-s {}".format(stretch),
   202                             					"-x {}".format(d_cov),
   203                             					"-e {}".format(downsample_rate)
   204                             				]
   205                             	else:
   206    441.1 MiB      0.0 MiB   		if always_euclidean and m2_d!=0:
   207                             			if graph_induced:
   208                             				find_landmarks_cmd = [
   209                             					"./find_landmarks",
   210                             					"-n {}".format(num_threads),
   211                             					"-l {}".format(number_of_vertices),
   212                             					"-w {}-{}".format(start,stop),
   213                             					"-i{}".format(input_file_name),
   214                             					"-olandmark_outputs.txt",
   215                             					"-m {}".format(int(m2_d)),
   216                             					"-a {}".format(speed_amplify),
   217                             					"-y {}".format(orientation_amplify),
   218                             					"-h {}".format(use_hamiltonian),
   219                             					"-r {}".format(ray_distance_amplify),
   220                             					"-v {}".format(straight_VB),
   221                             					"-x {}".format(d_cov),
   222                             					"-s {}".format(stretch),
   223                             					"-f {}".format(max_filtration_param)
   224                             				]
   225                             			else:
   226                             				find_landmarks_cmd = [
   227                             					"./find_landmarks",
   228                             					"-n {}".format(num_threads),
   229                             					"-l {}".format(number_of_vertices),
   230                             					"-w {}-{}".format(start,stop),
   231                             					"-i{}".format(input_file_name),
   232                             					"-olandmark_outputs.txt",
   233                             					"-m {}".format(int(m2_d)),
   234                             					"-a {}".format(speed_amplify),
   235                             					"-y {}".format(orientation_amplify),
   236                             					"-h {}".format(use_hamiltonian),
   237                             					"-r {}".format(ray_distance_amplify),
   238                             					"-v {}".format(straight_VB),
   239                             					"-x {}".format(d_cov),
   240                             					"-s {}".format(stretch)
   241                             				]
   242    441.1 MiB      0.0 MiB   		elif always_euclidean:
   243                             			if graph_induced:
   244                             				find_landmarks_cmd = [
   245                             					"./find_landmarks",
   246                             					"-n {}".format(num_threads),
   247                             					"-l {}".format(number_of_vertices),
   248                             					"-w {}-{}".format(start,stop),
   249                             					"-i{}".format(input_file_name),
   250                             					"-olandmark_outputs.txt",
   251                             					"-m {}".format(int(m2_d)),
   252                             					"-a {}".format(speed_amplify),
   253                             					"-y {}".format(orientation_amplify),
   254                             					"-h {}".format(use_hamiltonian),
   255                             					"-r {}".format(ray_distance_amplify),
   256                             					"-v {}".format(straight_VB),
   257                             					"-x {}".format(d_cov),
   258                             					"-s {}".format(stretch),
   259                             					"-c",
   260                             					"-f {}".format(max_filtration_param)
   261                             				]
   262                             			else:
   263                             				find_landmarks_cmd = [
   264                             					"./find_landmarks",
   265                             					"-n {}".format(num_threads),
   266                             					"-l {}".format(number_of_vertices),
   267                             					"-w {}-{}".format(start,stop),
   268                             					"-i{}".format(input_file_name),
   269                             					"-olandmark_outputs.txt",
   270                             					"-m {}".format(int(m2_d)),
   271                             					"-a {}".format(speed_amplify),
   272                             					"-y {}".format(orientation_amplify),
   273                             					"-h {}".format(use_hamiltonian),
   274                             					"-r {}".format(ray_distance_amplify),
   275                             					"-v {}".format(straight_VB),
   276                             					"-x {}".format(d_cov),
   277                             					"-s {}".format(stretch),
   278                             					"-c"
   279                             				]
   280                             		else:
   281    441.1 MiB      0.0 MiB   			if graph_induced:
   282                             				find_landmarks_cmd = [
   283                             					"./find_landmarks",
   284                             					"-n {}".format(num_threads),
   285                             					"-l {}".format(number_of_vertices),
   286                             					"-w {}-{}".format(start,stop),
   287                             					"-i{}".format(input_file_name),
   288                             					"-olandmark_outputs.txt",
   289                             					"-m {}".format(int(m2_d)),
   290                             					"-a {}".format(speed_amplify),
   291                             					"-y {}".format(orientation_amplify),
   292                             					"-h {}".format(use_hamiltonian),
   293                             					"-r {}".format(ray_distance_amplify),
   294                             					"-v {}".format(straight_VB),
   295                             					"-x {}".format(d_cov),
   296                             					"-s {}".format(stretch),
   297                             					"-f {}".format(max_filtration_param)
   298                             				]
   299                             			else:
   300                             				find_landmarks_cmd = [
   301    441.1 MiB      0.0 MiB   					"./find_landmarks",
   302    441.1 MiB      0.0 MiB   					"-n {}".format(num_threads),
   303    441.1 MiB      0.0 MiB   					"-l {}".format(number_of_vertices),
   304    441.1 MiB      0.0 MiB   					"-w {}-{}".format(start,stop),
   305    441.1 MiB      0.0 MiB   					"-i{}".format(input_file_name),
   306    441.1 MiB      0.0 MiB   					"-olandmark_outputs.txt",
   307    441.1 MiB      0.0 MiB   					"-m {}".format(int(m2_d)),
   308    441.1 MiB      0.0 MiB   					"-a {}".format(speed_amplify),
   309    441.1 MiB      0.0 MiB   					"-y {}".format(orientation_amplify),
   310    441.1 MiB      0.0 MiB   					"-h {}".format(use_hamiltonian),
   311    441.1 MiB      0.0 MiB   					"-r {}".format(ray_distance_amplify),
   312    441.1 MiB      0.0 MiB   					"-v {}".format(straight_VB),
   313    441.1 MiB      0.0 MiB   					"-x {}".format(d_cov),
   314    441.1 MiB      0.0 MiB   					"-s {}".format(stretch)
   315                             				]
   316                             
   317    441.1 MiB      0.0 MiB   	print find_landmarks_cmd
   318                             
   319    441.1 MiB      0.0 MiB   	if silent:
   320                             		p = subprocess.Popen(find_landmarks_cmd, stdout=subprocess.PIPE)
   321                             		out, err = p.communicate()
   322                             	else:
   323    441.1 MiB      0.0 MiB   		p = subprocess.Popen(find_landmarks_cmd)
   324    441.1 MiB      0.0 MiB   		p.communicate()
   325                             
   326                             	## Build and sort distance matrix.
   327    441.1 MiB      0.0 MiB   	landmarks_file = open("landmark_outputs.txt","rb")
   328                             
   329    441.1 MiB      0.0 MiB   	l = landmarks_file.readlines()
   330    441.1 MiB      0.0 MiB   	sys.stdout.write("Reading in distance calculations...")
   331    441.1 MiB      0.0 MiB   	sys.stdout.flush()
   332    441.1 MiB      0.0 MiB   	landmark_index = 0
   333    471.6 MiB     30.4 MiB   	for line in l:
   334    471.1 MiB     -0.5 MiB   		f = line.strip('\n')
   335    471.1 MiB      0.0 MiB   		if "#" not in f:
   336    471.1 MiB      0.0 MiB   			landmark = int(f.split(":")[0])
   337                             
   338    471.1 MiB      0.0 MiB   			distances = [float(i) for i in f.split(":")[1].split(",")]
   339    471.6 MiB      0.5 MiB   			for witness_index in range(0,len(distances)):
   340                             
   341    471.6 MiB      0.0 MiB   				d[witness_index].append(LandmarkDistance(landmark_index,distances[witness_index]))
   342    471.6 MiB      0.0 MiB   			landmarks.append(witnesses[landmark])
   343    471.6 MiB      0.0 MiB   			landmark_indices.append(landmark)
   344    471.6 MiB      0.0 MiB   			landmark_index+=1
   345                             
   346    471.6 MiB      0.0 MiB   	assert(len(d)>0)
   347    471.6 MiB      0.0 MiB   	sys.stdout.write("done\n")
   348    471.6 MiB      0.0 MiB   	sys.stdout.flush()
   349                             
   350                             
   351                             
   352    471.6 MiB      0.0 MiB   	sys.stdout.write("Sorting distances...")
   353    471.6 MiB      0.0 MiB   	sys.stdout.flush()
   354                             
   355                             	# p=multiprocessing.Pool(processes=4)   # commented out by Elliott 4/25
   356                             
   357    471.6 MiB      0.0 MiB   	inputs=[]
   358    471.8 MiB      0.2 MiB   	for w in range(0,len(witnesses)):
   359    471.8 MiB      0.0 MiB   		inputs.append(w)
   360    471.8 MiB      0.0 MiB   		d[w].sort()
   361                             
   362                             	# p.map(sort,inputs)  # was commented out as of 4/25
   363                             	# p.terminate()       # added by Elliott 4/25
   364                             
   365    471.8 MiB      0.0 MiB   	sys.stdout.write("done\n")
   366    471.8 MiB      0.0 MiB   	sys.stdout.flush()
   367    471.8 MiB      0.0 MiB   	assert len(landmarks) == number_of_vertices
   368                             
   369                             	'''=============== End code written by Sam ======================'''
   370                             
   371                             	'''============= Start code written by Elliott =================='''
   372    471.8 MiB      0.0 MiB   	if graph_induced:
   373                             		# import matplotlib.pyplot as plt
   374                             		import pandas as pd
   375                             
   376                             		g = nx.read_edgelist('edgelist.txt')
   377                             
   378                             		closest_wits = np.loadtxt('closest_wits.txt', dtype='int')	# witness, landmark
   379                             		wit_coords = np.array(witnesses)
   380                             		land_coords = np.array(landmarks)
   381                             
   382                             		# land = np.unique(closest_wits[:,1])
   383                             
   384                             		# closest_wits = pd.DataFrame(closest_wits, columns=('witness', 'landmark'))
   385                             		# print closest_wits
   386                             		# closest_wits = closest_wits.sort_values(by='landmark')
   387                             		# print closest_wits
   388                             		#
   389                             		# closest_wits = closest_wits.values
   390                             		# print closest_wits
   391                             
   392                             
   393                             
   394                             		# fig = plt.figure(figsize=(8, 8))
   395                             		# ax = fig.add_subplot(111)
   396                             		# ax.scatter(wit_coords[:, 0], wit_coords[:, 1], s=.1)
   397                             		# ax.scatter(land_coords[:, 0], land_coords[:, 1])
   398                             		# fig.savefig('veronoi_test.png')
   399                             
   400                             	'''=============== End code written by Elliott =================='''
   401                             
   402                             
   403                             
   404                             
   405    471.8 MiB      0.0 MiB   	print("Building filtration...")
   406                             	## Build filtration
   407    471.8 MiB      0.0 MiB   	weak = get_param("weak")
   408    471.8 MiB      0.0 MiB   	dimension_cutoff = get_param("dimension_cutoff")
   409    471.8 MiB      0.0 MiB   	reentry_filter = get_param("reentry_filter")
   410    471.8 MiB      0.0 MiB   	if get_param("connect_time_1_skeleton") or reentry_filter: # Connect time-1-skeleton
   411                             		for i in xrange(number_of_vertices - 1):
   412                             			filtration.add(SimplexBirth(ImmutableSet([i, i + 1]), 0, sort_output))
   413    471.8 MiB      0.0 MiB   	use_cliques = get_param("use_cliques")
   414    471.8 MiB      0.0 MiB   	use_twr = get_param("use_twr")
   415                             
   416    471.8 MiB      0.0 MiB   	print '%s' % use_twr
   417    471.8 MiB      0.0 MiB   	if use_cliques: # AKA "Lazy" witness relation.
   418                             		g = nx.Graph()
   419                             		for l in xrange(number_of_vertices):
   420                             			g.add_node(l)
   421    471.8 MiB      0.0 MiB   	def filter_and_build():
   422                             		g2 = None
   423                             		if reentry_filter:
   424                             			g2 = g.copy()
   425                             			to_remove = Set()
   426                             			for l1 in xrange(number_of_vertices):
   427                             				l2 = l1 + 2
   428                             				while l2 < number_of_vertices and g2.has_edge(l1, l2):
   429                             					to_remove.add(ImmutableSet([l1, l2]))
   430                             					l2 += 1
   431                             			for edge in to_remove:
   432                             				g2.remove_edge(*tuple(edge)) # May cause weird things to happen because removing edges doesn't remove them from the filtration.
   433                             		else:
   434                             			g2 = g
   435                             		for clique in nx.find_cliques(g2):
   436                             			filtration.add(SimplexBirth(clique, q, sort_output))
   437    471.8 MiB      0.0 MiB   	if weak: # Builds filtration based on k nearest neighbors.
   438                             		if max_filtration_param % 1 != 0:
   439                             			raise Exception("Argument 'max_filtration_param' must be an integer if using the weak witness relation.")
   440                             		max_filtration_param = int(max_filtration_param)
   441                             		for k in xrange(int(math.fabs(max_filtration_param))):
   442                             			for witness_index in xrange(number_of_datapoints):
   443                             				if use_cliques:
   444                             					for i in xrange(k):
   445                             						g.add_edge(d[witness_index][i].id_num, d[witness_index][k].id_num)
   446                             				elif store_top_simplices:
   447                             					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(k + 1)], k, sort_output))
   448                             				else:
   449                             					if progress > 0:
   450                             						for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(k)], min(k, dimension_cutoff)):
   451                             							new_subset = ImmutableSet(base + (d[witness_index][k].id_num,))
   452                             							filtration.add(SimplexBirth(new_subset, k, sort_output))
   453                             			if use_cliques:
   454                             				filter_and_build()
   455    471.8 MiB      0.0 MiB   	if use_twr:
   456                             		print 'Using TWR'
   457                             		if max_filtration_param < 0: # Automatically determine max.
   458                             			depth = int(-max_filtration_param)
   459                             			min_distance = None
   460                             			for w in xrange(number_of_datapoints):
   461                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   462                             				if min_distance is None or new_distance < min_distance:
   463                             					min_distance = new_distance
   464                             			max_filtration_param = min_distance
   465                             		print 'The max_filtration_param is %d ' % max_filtration_param
   466                             		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   467                             		print 'The step size is %f ' % step
   468                             		print 'There will be %d steps in the filtration' % num_divisions
   469                             		progress_index = [0]*number_of_datapoints
   470                             		done = False
   471                             
   472                             		good_landmarks = [[] for x in range(number_of_datapoints)]
   473                             
   474                             		epsilons = []
   475                             		for q in xrange(num_divisions):
   476                             			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   477                             			print 'The threshold is currently %f' % threshold
   478                             			epsilons.append(threshold)
   479                             			Pre_landmarks = []
   480                             			for witness_index in xrange(number_of_datapoints):
   481                             				pre_landmarks = []
   482                             				add_simplex = False
   483                             				progress = 0
   484                             				while True:
   485                             					progress = progress_index[witness_index]
   486                             					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   487                             						break
   488                             					if progress == number_of_vertices:
   489                             						done = True
   490                             						break
   491                             
   492                             					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   493                             						pre_landmarks.append(d[witness_index][progress].id_num) # PRE_LANDMARKS CONTAINS ID NUMBER
   494                             						progress_index[witness_index] += 1
   495                             					else:
   496                             
   497                             						pre_landmarks_size = len(pre_landmarks)
   498                             						pre_landmarks_string = str(pre_landmarks) # MAKE LIST TO STRING
   499                             						print 'At threshold value %f, witness %d has %d associated landmarks: %s ' % (threshold, witness_index, pre_landmarks_size, pre_landmarks_string)
   500                             						break
   501                             				Pre_landmarks.append(pre_landmarks)
   502                             				Pre_landmarks_size = len(Pre_landmarks)
   503                             
   504                             
   505                             
   506                             			for witness_index in xrange(number_of_datapoints - downsample_rate):
   507                             				if len(Pre_landmarks[witness_index]) == 1:
   508                             					set_range = 1
   509                             				else:
   510                             					set_range = len(Pre_landmarks[witness_index])
   511                             				for k in range(set_range):
   512                             					current_pre_landmark = Pre_landmarks[witness_index][k]
   513                             					next_pre_landmark = Pre_landmarks[witness_index][k]+1 # CHECKS ONE STEP UP FROM ID NUMBER *** SEE JAMIE'S COMMENTS ON SUCH OR CLARIFY ***
   514                             					#	print 'current pre landmark = %d , next pre landmark = %d' % (current_pre_landmark, next_pre_landmark)
   515                             					check_pre_landmark = str(Pre_landmarks[witness_index + downsample_rate]) # HMMMMM
   516                             					#		print 'We are considering the fate of landmark %d witnessed by witness %d...' % (current_pre_landmark, witness_index)
   517                             					#		print 'Should witness %d not witness landmark %d, it will be GONE!' % (witness_index + downsample_rate, current_pre_landmark + 1,)
   518                             					#		print 'Witness %d has landmark set %s' % (witness_index + downsample_rate, check_pre_landmark)
   519                             					print (Pre_landmarks[witness_index][k]) in Pre_landmarks[witness_index]
   520                             					if (Pre_landmarks[witness_index][k]+ 1) in Pre_landmarks[witness_index + downsample_rate]: # change from 1, downsample_rate to 0 to test!
   521                             						good_landmarks[witness_index].append(Pre_landmarks[witness_index][k])
   522                             				print 'Up to threshold value %f, witness %d has landmark set %s' % (threshold, witness_index, str(good_landmarks[witness_index]))
   523                             				if use_cliques:
   524                             					for i in xrange(len(good_landmarks[witness_index])):
   525                             						for j in xrange(i+1,len(good_landmarks[witness_index])):
   526                             							g.add_edge(good_landmarks[witness_index][i], good_landmarks[witness_index][j])
   527                             				else:
   528                             					if not store_top_simplices and len(good_landmarks[witness_index]) > 0:
   529                             						for base in itertools.combinations(good_landmarks[witness_index], min(len(good_landmarks[witness_index]), dimension_cutoff)):
   530                             							new_subset = ImmutableSet(base + (good_landmarks[witness_index][i],))
   531                             							filtration.add(SimplexBirth(new_subset, q, sort_output))
   532                             					add_simplex = True
   533                             				if (not use_cliques) and store_top_simplices and add_simplex and len(good_landmarks[witness_index])>= 2:
   534                             					filtration.add(SimplexBirth([good_landmarks[witness_index][i] for i in xrange(len(good_landmarks[witness_index]))], q, sort_output))
   535                             				if done:
   536                             					break
   537                             			if use_cliques:
   538                             				filter_and_build()
   539                             			if done:
   540                             				break
   541                             			#	print 'We are done with threshold %f' % threshold
   542                             	else:
   543    471.8 MiB      0.0 MiB   		if max_filtration_param < 0: # Automatically determine max.
   544                             			depth = int(-max_filtration_param)
   545                             			min_distance = None
   546                             			for w in xrange(number_of_datapoints):
   547                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   548                             				if min_distance is None or new_distance < min_distance:
   549                             					min_distance = new_distance
   550                             					if min_distance ==0:
   551                             						print "witness ",w
   552                             			max_filtration_param = min_distance
   553                             
   554    471.8 MiB      0.0 MiB   		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   555    471.8 MiB      0.0 MiB   		progress_index = [0]*number_of_datapoints
   556    471.8 MiB      0.0 MiB   		done = False
   557    471.8 MiB      0.0 MiB   		epsilons = []
   558    471.8 MiB      0.0 MiB   		for q in xrange(num_divisions):
   559    471.8 MiB      0.0 MiB   			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   560    471.8 MiB      0.0 MiB   			print 'The threshold is currently %f' % threshold
   561    471.8 MiB      0.0 MiB   			epsilons.append(threshold)
   562    471.8 MiB      0.0 MiB   			for witness_index in xrange(number_of_datapoints):
   563    471.8 MiB      0.0 MiB   				add_simplex = False
   564    471.8 MiB      0.0 MiB   				progress = 0
   565    471.8 MiB      0.0 MiB   				while True:
   566    471.8 MiB      0.0 MiB   					progress = progress_index[witness_index]
   567    471.8 MiB      0.0 MiB   					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   568                             						break
   569    471.8 MiB      0.0 MiB   					if progress == number_of_vertices:
   570    471.8 MiB      0.0 MiB   						done = True
   571    471.8 MiB      0.0 MiB   						break
   572    471.8 MiB      0.0 MiB   					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   573    471.8 MiB      0.0 MiB   						if use_cliques:
   574                             							for i in xrange(progress):
   575                             								g.add_edge(d[witness_index][i].id_num, d[witness_index][progress].id_num)
   576                             						else:
   577    471.8 MiB      0.0 MiB   							if not store_top_simplices and progress > 0:
   578                             								for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], min(progress, dimension_cutoff)):
   579                             									new_subset = ImmutableSet(base + (d[witness_index][progress].id_num,))
   580                             									filtration.add(SimplexBirth(new_subset, q, sort_output))
   581    471.8 MiB      0.0 MiB   							add_simplex = True
   582    471.8 MiB      0.0 MiB   						progress_index[witness_index] += 1
   583                             					else:
   584                             						break
   585    471.8 MiB      0.0 MiB   				if (not use_cliques) and store_top_simplices and add_simplex and progress >= 2:
   586    471.8 MiB      0.0 MiB   					list_o_landmarks = []
   587    471.8 MiB      0.0 MiB   					for landmark_index in xrange(progress):
   588    471.8 MiB      0.0 MiB   						list_o_landmarks.append(d[witness_index][landmark_index].id_num)
   589                             					#print 'At threshold %f, witness %d has landmark set %s' % (threshold, witness_index, str(list_o_landmarks))
   590    471.8 MiB      0.0 MiB   					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], q, sort_output))
   591    471.8 MiB      0.0 MiB   				if done:
   592    471.8 MiB      0.0 MiB   					break
   593    471.8 MiB      0.0 MiB   			if use_cliques:
   594                             				filter_and_build()
   595    471.8 MiB      0.0 MiB   			if done:
   596    471.8 MiB      0.0 MiB   				break
   597                             
   598                             
   599    471.8 MiB      0.0 MiB   	extra_data = (landmarks, witnesses)
   600    471.8 MiB      0.0 MiB   	if weak:
   601                             		max_epsilon = 0.0
   602                             		for w in xrange(number_of_datapoints):
   603                             			#print("type: %s" % type(d[w][max_filtration_param - 1].distance))
   604                             			#print("value: %f" % d[w][max_filtration_param - 1].distance)
   605                             			if (d[w][max_filtration_param - 1].distance) > max_epsilon:
   606                             				max_epsilon = d[w][max_filtration_param - 1].distance
   607                             		print("Done. Filtration contains %i top simplex birth events, with the largest epsilon equal to %f.\n" % (len(filtration), max_epsilon))
   608                             	else:
   609    471.8 MiB      0.0 MiB   		max_sb_length = 0
   610    471.8 MiB      0.0 MiB   		for sb in filtration:
   611    471.8 MiB      0.0 MiB   			if len(sb.landmark_set) > max_sb_length:
   612    471.8 MiB      0.0 MiB   				max_sb_length = len(sb.landmark_set)
   613    471.8 MiB      0.0 MiB   		print("Done. Filtration contains %i top simplex birth events, with the largest one comprised of %i landmarks.\nMax filtration parameter: %s.\n" % (len(filtration), max_sb_length, max_filtration_param))
   614                             
   615                             	## Write to output file
   616    471.8 MiB      0.0 MiB   	output_file_name = get_param("out")
   617                             
   618    471.8 MiB      0.0 MiB   	if not output_file_name is None:
   619                             		output_file = open(output_file_name, "w")
   620                             		output_file.truncate()
   621                             		program = get_param("program")
   622                             		if dimension_cutoff is None:
   623                             			print("Writing filtration for input into %s..." % program)
   624                             			dimension_cutoff = number_of_vertices
   625                             		else:
   626                             			print("Writing filtration to file %s for input into %s, ignoring simplices above dimension %i..." % (output_file_name,program, dimension_cutoff))
   627                             		num_lines = 0
   628                             		if program == "Perseus":
   629                             			sets_printed_so_far = Set()
   630                             			num_lines = len(filtration) + 1
   631                             			output_file.write("1\n")
   632                             			list_filtration = None
   633                             			if (sort_output):
   634                             				list_filtration = list(filtration)
   635                             				list_filtration.sort()
   636                             			for simplex_birth in (list_filtration if sort_output else filtration):
   637                             				dimension = len(simplex_birth.landmark_set) - 1
   638                             				if dimension > dimension_cutoff:
   639                             					for subtuple in itertools.combinations(simplex_birth.landmark_set, dimension_cutoff + 1):
   640                             						subset = ImmutableSet(subtuple)
   641                             						if not ((subset, simplex_birth.birth_time) in sets_printed_so_far):
   642                             							output_file.write(str(dimension_cutoff) + " ")
   643                             							for landmark in subset:
   644                             								output_file.write(str(landmark + 1) + " ")
   645                             							output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   646                             							sets_printed_so_far.add((subset, simplex_birth.birth_time))
   647                             				else:
   648                             					if not ((simplex_birth.landmark_set, simplex_birth.birth_time) in sets_printed_so_far):
   649                             						output_file.write(str(dimension) + " ")
   650                             						for landmark in (simplex_birth.sll if sort_output else simplex_birth.landmark_set):
   651                             							output_file.write(str(landmark + 1) + " ")
   652                             						output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   653                             						sets_printed_so_far.add((simplex_birth.landmark_set, simplex_birth.birth_time))
   654                             		elif program == "PHAT":
   655                             			line_map = {}
   656                             			for i in xrange(number_of_vertices - 1):
   657                             				output_file.write("0\n")
   658                             				line_map[ImmutableSet([i])] = i
   659                             			output_file.write("0")
   660                             			line_map[ImmutableSet([number_of_vertices - 1])] = number_of_vertices - 1
   661                             			simultaneous_additions = []
   662                             			class Context: # Note: if upgrading to Python 3, one could just use the nonlocal keyword (see below comment).
   663                             				line_number = number_of_vertices
   664                             			list_filtration = list(filtration)
   665                             			list_filtration.sort()
   666                             			last_birth_time = 0
   667                             			def process_and_get_line_number(s):
   668                             				#nonlocal line_number
   669                             				if s in line_map:
   670                             					return line_map[s]
   671                             				else:
   672                             					dimension = len(s) - 1
   673                             					if dimension > dimension_cutoff:
   674                             						for subset in itertools.combinations(s, dimension_cutoff + 1): # Take all subsets of size dimension_cutoff + 1
   675                             							process_and_get_line_number(ImmutableSet(subset))
   676                             					elif dimension > 0:
   677                             						subsets_line_numbers = []
   678                             						for e in s:
   679                             							subsets_line_numbers.append(process_and_get_line_number(ImmutableSet(s - Set([e]))))
   680                             						output_file.write("\n" + str(dimension))
   681                             						for l in subsets_line_numbers:
   682                             							output_file.write(" " + str(l))
   683                             						line_map[s] = Context.line_number
   684                             						Context.line_number += 1
   685                             						return Context.line_number - 1
   686                             					else:
   687                             						raise Exception("Should have already added single point for base case: " + str(s))
   688                             			for simplex_birth in list_filtration:
   689                             				if simplex_birth.birth_time > last_birth_time:
   690                             					simultaneous_additions.append((Context.line_number - 1, last_birth_time + 1)) # Every line up to and including that line number (indexing starts at 0) had that birth time or earlier (indexing starts at 1)
   691                             					last_birth_time = simplex_birth.birth_time
   692                             				process_and_get_line_number(simplex_birth.landmark_set)
   693                             			simultaneous_additions.append((sys.maxsize, last_birth_time))
   694                             			output_file.write("\n\n# Simultaneous additions: Every line up to and including __ (indexing starts at 0) has birth time __ (or earlier).")
   695                             			for addition in simultaneous_additions:
   696                             				output_file.write("\n# %20i %20i" % addition)
   697                             			extra_data = (extra_data[0], extra_data[1], simultaneous_additions)
   698                             			num_lines = Context.line_number
   699                             		else:
   700                             			raise Exception("Only supported programs are 'Perseus' and 'PHAT'")
   701                             		output_file.close()
   702                             		print("Done. File contains %i lines.\n" % num_lines)
   703    471.8 MiB      0.0 MiB   	print("Filtration has been successfully built!\n")
   704    471.8 MiB      0.0 MiB   	return (filtration, extra_data + (max_filtration_param,), epsilons)


Filename: /home/elliott/programming/PHETS/PH/BuildFiltration.py

Line #    Mem usage    Increment   Line Contents
================================================
    37    508.1 MiB      0.0 MiB   @profile(stream=f)
    38                             def build_filtration(input_file_name, parameter_set, silent=False):
    39    508.1 MiB      0.0 MiB   	num_threads = 2
    40                             	global d
    41    477.6 MiB    -30.5 MiB   	d = []
    42                             
    43                             
    44    505.9 MiB     28.3 MiB   	def get_param(key):
    45    505.9 MiB      0.0 MiB   		return parameter_set.get(key)
    46                             
    47    477.6 MiB    -28.3 MiB   	input_file = open(input_file_name)
    48    477.6 MiB      0.0 MiB   	speed_amplify = float(get_param("d_speed_amplify"))
    49    477.6 MiB      0.0 MiB   	orientation_amplify = float(get_param("d_orientation_amplify"))
    50    477.6 MiB      0.0 MiB   	stretch = float(get_param("d_stretch"))
    51    477.6 MiB      0.0 MiB   	ray_distance_amplify = get_param("d_ray_distance_amplify")
    52    477.6 MiB      0.0 MiB   	use_hamiltonian = get_param("d_use_hamiltonian")
    53    477.6 MiB      0.0 MiB   	print "use hamiltonian set to ", use_hamiltonian
    54    477.6 MiB      0.0 MiB   	m2_d = float(get_param("m2_d"))
    55    477.6 MiB      0.0 MiB   	straight_VB = float(get_param("straight_VB"))
    56    477.6 MiB      0.0 MiB   	d_cov = get_param("d_cov")
    57    477.6 MiB      0.0 MiB   	graph_induced = get_param("graph_induced")
    58                             	# always_euclidean = speed_amplify == orientation_amplify == stretch == ray_distance_amplify == 1.0 and use_hamiltonian == d_cov==0.
    59                             	# print "always_euclidean set to ", always_euclidean
    60    477.6 MiB      0.0 MiB   	always_euclidean = get_param('always_euclidean')
    61                             	# use_hamiltonian = -5
    62                             
    63    477.6 MiB      0.0 MiB   	filtration = Set()
    64    477.6 MiB      0.0 MiB   	extra_data = None
    65    477.6 MiB      0.0 MiB   	min_filtration_param = float(get_param("min_filtration_param"))
    66    477.6 MiB      0.0 MiB   	max_filtration_param = float(get_param("max_filtration_param"))
    67    477.6 MiB      0.0 MiB   	print "Max filtration parameter is ", max_filtration_param
    68    477.6 MiB      0.0 MiB   	if max_filtration_param < 0 and min_filtration_param != 0:
    69                             		raise Exception("Argument 'min_filtration_param' is incompatible with automatic max_filtration_param selection.")
    70    477.6 MiB      0.0 MiB   	number_of_vertices = 0
    71    477.6 MiB      0.0 MiB   	start = get_param("start")
    72    477.6 MiB      0.0 MiB   	worm_length = get_param("worm_length")
    73    477.6 MiB      0.0 MiB   	store_top_simplices = get_param("store_top_simplices")
    74    477.6 MiB      0.0 MiB   	sort_output = get_param("sort_output")
    75    477.6 MiB      0.0 MiB   	absolute = get_param("absolute")
    76    477.6 MiB      0.0 MiB   	num_divisions = get_param("num_divisions")
    77    477.6 MiB      0.0 MiB   	simplex_cutoff = get_param("simplex_cutoff")
    78                             
    79                             
    80                             	'''=============== This code written by Sam ======================'''
    81                             
    82                             	## Read data into witness and landmark lists.
    83    477.6 MiB      0.0 MiB   	witnesses = []
    84    477.6 MiB      0.0 MiB   	landmarks = []
    85    477.6 MiB      0.0 MiB   	landmark_indices = []
    86    477.6 MiB      0.0 MiB   	ls = get_param("landmark_selector")
    87    477.6 MiB      0.0 MiB   	downsample_rate = get_param("ds_rate")
    88    477.6 MiB      0.0 MiB   	maxmin = False
    89    477.6 MiB      0.0 MiB   	counter = 0
    90    477.6 MiB      0.0 MiB   	for i in xrange(start):           #  Where to start reading data
    91                             		input_file.readline()
    92                             		counter+=1
    93    477.6 MiB      0.0 MiB   	landmark_indices=[]
    94                             
    95                             
    96    701.8 MiB    224.1 MiB   	for line in input_file.read().split("\n"):
    97    701.8 MiB      0.0 MiB   		if line != "" and counter>=start:
    98    701.8 MiB      0.0 MiB   			string_witness = line.split(" ")
    99    701.8 MiB      0.0 MiB   			witness = []
   100    701.8 MiB      0.0 MiB   			d.append([])
   101    701.8 MiB      0.0 MiB   			for coordinate in string_witness:
   102    701.8 MiB      0.0 MiB   				if coordinate != "":
   103    701.8 MiB      0.0 MiB   					witness.append(float(coordinate))
   104    701.8 MiB      0.0 MiB   			witnesses.append(witness)
   105    701.8 MiB      0.0 MiB   			counter += 1
   106    701.8 MiB      0.0 MiB   			if counter == worm_length:
   107    477.7 MiB   -224.1 MiB   				break
   108                             
   109    477.7 MiB      0.0 MiB   	number_of_datapoints = len(witnesses)
   110    477.7 MiB      0.0 MiB   	number_of_vertices = int(number_of_datapoints/downsample_rate)
   111    477.7 MiB      0.0 MiB   	num_coordinates = len(witnesses[0])
   112    477.7 MiB      0.0 MiB   	stop = start + counter
   113                             
   114    477.7 MiB      0.0 MiB   	if max_filtration_param < 0:
   115                             		if float(number_of_vertices) < abs(max_filtration_param) + 1:
   116                             			print '''ERROR: 'max_filtration_param' ({}) and number of landmarks ({})
   117                             			are incompatible. Try decreasing 'ds_rate' or increasing 'worm_length'.'''.format(max_filtration_param,
   118                             			number_of_vertices)
   119                             			sys.exit()
   120                             
   121    477.7 MiB      0.0 MiB   	num_threads = 2
   122                             	# for more information about these parameters type ./find_landmarks --help in the terminal
   123                             	# the distance calculations are calculated and outputted to a file called find_landmarks.txt
   124    477.7 MiB      0.0 MiB   	print os.getcwd()
   125                             
   126    477.7 MiB      0.0 MiB   	if ls=="EST":
   127                             		if always_euclidean:
   128                             			if graph_induced:
   129                             				find_landmarks_cmd = [
   130                             					"./find_landmarks",
   131                             					"-n {}".format(num_threads),
   132                             					"-l {}".format(number_of_vertices),
   133                             					"-w {}-{}".format(start,stop),
   134                             					"-i{}".format(input_file_name),
   135                             					"-olandmark_outputs.txt",
   136                             					"-m {}".format(int(m2_d)),
   137                             					"-a {}".format(speed_amplify),
   138                             					"-y {}".format(orientation_amplify),
   139                             					"-h {}".format(use_hamiltonian),
   140                             					"-r {}".format(ray_distance_amplify),
   141                             					"-v {}".format(straight_VB),
   142                             					"-s {}".format(stretch),
   143                             					"-e {}".format(downsample_rate),
   144                             					"-x {}".format(d_cov),
   145                             					"-c",
   146                             					"-f {}".format(max_filtration_param)
   147                             				]
   148                             			else:
   149                             				find_landmarks_cmd = [
   150                             					"./find_landmarks",
   151                             					"-n {}".format(num_threads),
   152                             					"-l {}".format(number_of_vertices),
   153                             					"-w {}-{}".format(start,stop),
   154                             					"-i{}".format(input_file_name),
   155                             					"-olandmark_outputs.txt",
   156                             					"-m {}".format(int(m2_d)),
   157                             					"-a {}".format(speed_amplify),
   158                             					"-y {}".format(orientation_amplify),
   159                             					"-h {}".format(use_hamiltonian),
   160                             					"-r {}".format(ray_distance_amplify),
   161                             					"-v {}".format(straight_VB),
   162                             					"-s {}".format(stretch),
   163                             					"-e {}".format(downsample_rate),
   164                             					"-x {}".format(d_cov),
   165                             					"-c"
   166                             				]
   167                             		else:
   168                             			if graph_induced:
   169                             				find_landmarks_cmd = [
   170                             					"./find_landmarks",
   171                             					"-n {}".format(num_threads),
   172                             					"-l {}".format(number_of_vertices),
   173                             					"-w {}-{}".format(start,stop),
   174                             					"-i{}".format(input_file_name),
   175                             					"-olandmark_outputs.txt",
   176                             					"-m {}".format(int(m2_d)),
   177                             					"-a {}".format(speed_amplify),
   178                             					"-y {}".format(orientation_amplify),
   179                             					"-h {}".format(use_hamiltonian),
   180                             					"-r {}".format(ray_distance_amplify),
   181                             					"-v {}".format(straight_VB),
   182                             					"-s {}".format(stretch),
   183                             					"-x {}".format(d_cov),
   184                             					"-e {}".format(downsample_rate),
   185                             					"-f {}".format(max_filtration_param)
   186                             				]
   187                             			else:
   188                             				find_landmarks_cmd = [
   189                             					"./find_landmarks",
   190                             					"-n {}".format(num_threads),
   191                             					"-l {}".format(number_of_vertices),
   192                             					"-w {}-{}".format(start,stop),
   193                             					"-i{}".format(input_file_name),
   194                             					"-olandmark_outputs.txt",
   195                             					"-m {}".format(int(m2_d)),
   196                             					"-a {}".format(speed_amplify),
   197                             					"-y {}".format(orientation_amplify),
   198                             					"-h {}".format(use_hamiltonian),
   199                             					"-r {}".format(ray_distance_amplify),
   200                             					"-v {}".format(straight_VB),
   201                             					"-s {}".format(stretch),
   202                             					"-x {}".format(d_cov),
   203                             					"-e {}".format(downsample_rate)
   204                             				]
   205                             	else:
   206    477.7 MiB      0.0 MiB   		if always_euclidean and m2_d!=0:
   207                             			if graph_induced:
   208                             				find_landmarks_cmd = [
   209                             					"./find_landmarks",
   210                             					"-n {}".format(num_threads),
   211                             					"-l {}".format(number_of_vertices),
   212                             					"-w {}-{}".format(start,stop),
   213                             					"-i{}".format(input_file_name),
   214                             					"-olandmark_outputs.txt",
   215                             					"-m {}".format(int(m2_d)),
   216                             					"-a {}".format(speed_amplify),
   217                             					"-y {}".format(orientation_amplify),
   218                             					"-h {}".format(use_hamiltonian),
   219                             					"-r {}".format(ray_distance_amplify),
   220                             					"-v {}".format(straight_VB),
   221                             					"-x {}".format(d_cov),
   222                             					"-s {}".format(stretch),
   223                             					"-f {}".format(max_filtration_param)
   224                             				]
   225                             			else:
   226                             				find_landmarks_cmd = [
   227                             					"./find_landmarks",
   228                             					"-n {}".format(num_threads),
   229                             					"-l {}".format(number_of_vertices),
   230                             					"-w {}-{}".format(start,stop),
   231                             					"-i{}".format(input_file_name),
   232                             					"-olandmark_outputs.txt",
   233                             					"-m {}".format(int(m2_d)),
   234                             					"-a {}".format(speed_amplify),
   235                             					"-y {}".format(orientation_amplify),
   236                             					"-h {}".format(use_hamiltonian),
   237                             					"-r {}".format(ray_distance_amplify),
   238                             					"-v {}".format(straight_VB),
   239                             					"-x {}".format(d_cov),
   240                             					"-s {}".format(stretch)
   241                             				]
   242    477.7 MiB      0.0 MiB   		elif always_euclidean:
   243                             			if graph_induced:
   244                             				find_landmarks_cmd = [
   245                             					"./find_landmarks",
   246                             					"-n {}".format(num_threads),
   247                             					"-l {}".format(number_of_vertices),
   248                             					"-w {}-{}".format(start,stop),
   249                             					"-i{}".format(input_file_name),
   250                             					"-olandmark_outputs.txt",
   251                             					"-m {}".format(int(m2_d)),
   252                             					"-a {}".format(speed_amplify),
   253                             					"-y {}".format(orientation_amplify),
   254                             					"-h {}".format(use_hamiltonian),
   255                             					"-r {}".format(ray_distance_amplify),
   256                             					"-v {}".format(straight_VB),
   257                             					"-x {}".format(d_cov),
   258                             					"-s {}".format(stretch),
   259                             					"-c",
   260                             					"-f {}".format(max_filtration_param)
   261                             				]
   262                             			else:
   263                             				find_landmarks_cmd = [
   264                             					"./find_landmarks",
   265                             					"-n {}".format(num_threads),
   266                             					"-l {}".format(number_of_vertices),
   267                             					"-w {}-{}".format(start,stop),
   268                             					"-i{}".format(input_file_name),
   269                             					"-olandmark_outputs.txt",
   270                             					"-m {}".format(int(m2_d)),
   271                             					"-a {}".format(speed_amplify),
   272                             					"-y {}".format(orientation_amplify),
   273                             					"-h {}".format(use_hamiltonian),
   274                             					"-r {}".format(ray_distance_amplify),
   275                             					"-v {}".format(straight_VB),
   276                             					"-x {}".format(d_cov),
   277                             					"-s {}".format(stretch),
   278                             					"-c"
   279                             				]
   280                             		else:
   281    477.7 MiB      0.0 MiB   			if graph_induced:
   282                             				find_landmarks_cmd = [
   283                             					"./find_landmarks",
   284                             					"-n {}".format(num_threads),
   285                             					"-l {}".format(number_of_vertices),
   286                             					"-w {}-{}".format(start,stop),
   287                             					"-i{}".format(input_file_name),
   288                             					"-olandmark_outputs.txt",
   289                             					"-m {}".format(int(m2_d)),
   290                             					"-a {}".format(speed_amplify),
   291                             					"-y {}".format(orientation_amplify),
   292                             					"-h {}".format(use_hamiltonian),
   293                             					"-r {}".format(ray_distance_amplify),
   294                             					"-v {}".format(straight_VB),
   295                             					"-x {}".format(d_cov),
   296                             					"-s {}".format(stretch),
   297                             					"-f {}".format(max_filtration_param)
   298                             				]
   299                             			else:
   300                             				find_landmarks_cmd = [
   301    477.7 MiB      0.0 MiB   					"./find_landmarks",
   302    477.7 MiB      0.0 MiB   					"-n {}".format(num_threads),
   303    477.7 MiB      0.0 MiB   					"-l {}".format(number_of_vertices),
   304    477.7 MiB      0.0 MiB   					"-w {}-{}".format(start,stop),
   305    477.7 MiB      0.0 MiB   					"-i{}".format(input_file_name),
   306    477.7 MiB      0.0 MiB   					"-olandmark_outputs.txt",
   307    477.7 MiB      0.0 MiB   					"-m {}".format(int(m2_d)),
   308    477.7 MiB      0.0 MiB   					"-a {}".format(speed_amplify),
   309    477.7 MiB      0.0 MiB   					"-y {}".format(orientation_amplify),
   310    477.7 MiB      0.0 MiB   					"-h {}".format(use_hamiltonian),
   311    477.7 MiB      0.0 MiB   					"-r {}".format(ray_distance_amplify),
   312    477.7 MiB      0.0 MiB   					"-v {}".format(straight_VB),
   313    477.7 MiB      0.0 MiB   					"-x {}".format(d_cov),
   314    477.7 MiB      0.0 MiB   					"-s {}".format(stretch)
   315                             				]
   316                             
   317    477.7 MiB      0.0 MiB   	print find_landmarks_cmd
   318                             
   319    477.7 MiB      0.0 MiB   	if silent:
   320                             		p = subprocess.Popen(find_landmarks_cmd, stdout=subprocess.PIPE)
   321                             		out, err = p.communicate()
   322                             	else:
   323    477.7 MiB      0.0 MiB   		p = subprocess.Popen(find_landmarks_cmd)
   324    477.7 MiB      0.0 MiB   		p.communicate()
   325                             
   326                             	## Build and sort distance matrix.
   327    477.7 MiB      0.0 MiB   	landmarks_file = open("landmark_outputs.txt","rb")
   328                             
   329    477.7 MiB      0.0 MiB   	l = landmarks_file.readlines()
   330    477.7 MiB      0.0 MiB   	sys.stdout.write("Reading in distance calculations...")
   331    477.7 MiB      0.0 MiB   	sys.stdout.flush()
   332    477.7 MiB      0.0 MiB   	landmark_index = 0
   333    505.9 MiB     28.2 MiB   	for line in l:
   334    505.2 MiB     -0.8 MiB   		f = line.strip('\n')
   335    505.2 MiB      0.0 MiB   		if "#" not in f:
   336    505.2 MiB      0.0 MiB   			landmark = int(f.split(":")[0])
   337                             
   338    505.4 MiB      0.2 MiB   			distances = [float(i) for i in f.split(":")[1].split(",")]
   339    505.9 MiB      0.5 MiB   			for witness_index in range(0,len(distances)):
   340                             
   341    505.9 MiB      0.0 MiB   				d[witness_index].append(LandmarkDistance(landmark_index,distances[witness_index]))
   342    505.9 MiB      0.0 MiB   			landmarks.append(witnesses[landmark])
   343    505.9 MiB      0.0 MiB   			landmark_indices.append(landmark)
   344    505.9 MiB      0.0 MiB   			landmark_index+=1
   345                             
   346    505.9 MiB      0.0 MiB   	assert(len(d)>0)
   347    505.9 MiB      0.0 MiB   	sys.stdout.write("done\n")
   348    505.9 MiB      0.0 MiB   	sys.stdout.flush()
   349                             
   350                             
   351                             
   352    505.9 MiB      0.0 MiB   	sys.stdout.write("Sorting distances...")
   353    505.9 MiB      0.0 MiB   	sys.stdout.flush()
   354                             
   355                             	# p=multiprocessing.Pool(processes=4)   # commented out by Elliott 4/25
   356                             
   357    505.9 MiB      0.0 MiB   	inputs=[]
   358    505.9 MiB      0.0 MiB   	for w in range(0,len(witnesses)):
   359    505.9 MiB      0.0 MiB   		inputs.append(w)
   360    505.9 MiB      0.0 MiB   		d[w].sort()
   361                             
   362                             	# p.map(sort,inputs)  # was commented out as of 4/25
   363                             	# p.terminate()       # added by Elliott 4/25
   364                             
   365    505.9 MiB      0.0 MiB   	sys.stdout.write("done\n")
   366    505.9 MiB      0.0 MiB   	sys.stdout.flush()
   367    505.9 MiB      0.0 MiB   	assert len(landmarks) == number_of_vertices
   368                             
   369                             	'''=============== End code written by Sam ======================'''
   370                             
   371                             	'''============= Start code written by Elliott =================='''
   372    505.9 MiB      0.0 MiB   	if graph_induced:
   373                             		# import matplotlib.pyplot as plt
   374                             		import pandas as pd
   375                             
   376                             		g = nx.read_edgelist('edgelist.txt')
   377                             
   378                             		closest_wits = np.loadtxt('closest_wits.txt', dtype='int')	# witness, landmark
   379                             		wit_coords = np.array(witnesses)
   380                             		land_coords = np.array(landmarks)
   381                             
   382                             		# land = np.unique(closest_wits[:,1])
   383                             
   384                             		# closest_wits = pd.DataFrame(closest_wits, columns=('witness', 'landmark'))
   385                             		# print closest_wits
   386                             		# closest_wits = closest_wits.sort_values(by='landmark')
   387                             		# print closest_wits
   388                             		#
   389                             		# closest_wits = closest_wits.values
   390                             		# print closest_wits
   391                             
   392                             
   393                             
   394                             		# fig = plt.figure(figsize=(8, 8))
   395                             		# ax = fig.add_subplot(111)
   396                             		# ax.scatter(wit_coords[:, 0], wit_coords[:, 1], s=.1)
   397                             		# ax.scatter(land_coords[:, 0], land_coords[:, 1])
   398                             		# fig.savefig('veronoi_test.png')
   399                             
   400                             	'''=============== End code written by Elliott =================='''
   401                             
   402                             
   403                             
   404                             
   405    505.9 MiB      0.0 MiB   	print("Building filtration...")
   406                             	## Build filtration
   407    505.9 MiB      0.0 MiB   	weak = get_param("weak")
   408    505.9 MiB      0.0 MiB   	dimension_cutoff = get_param("dimension_cutoff")
   409    505.9 MiB      0.0 MiB   	reentry_filter = get_param("reentry_filter")
   410    505.9 MiB      0.0 MiB   	if get_param("connect_time_1_skeleton") or reentry_filter: # Connect time-1-skeleton
   411                             		for i in xrange(number_of_vertices - 1):
   412                             			filtration.add(SimplexBirth(ImmutableSet([i, i + 1]), 0, sort_output))
   413    505.9 MiB      0.0 MiB   	use_cliques = get_param("use_cliques")
   414    505.9 MiB      0.0 MiB   	use_twr = get_param("use_twr")
   415                             
   416    505.9 MiB      0.0 MiB   	print '%s' % use_twr
   417    505.9 MiB      0.0 MiB   	if use_cliques: # AKA "Lazy" witness relation.
   418                             		g = nx.Graph()
   419                             		for l in xrange(number_of_vertices):
   420                             			g.add_node(l)
   421    505.9 MiB      0.0 MiB   	def filter_and_build():
   422                             		g2 = None
   423                             		if reentry_filter:
   424                             			g2 = g.copy()
   425                             			to_remove = Set()
   426                             			for l1 in xrange(number_of_vertices):
   427                             				l2 = l1 + 2
   428                             				while l2 < number_of_vertices and g2.has_edge(l1, l2):
   429                             					to_remove.add(ImmutableSet([l1, l2]))
   430                             					l2 += 1
   431                             			for edge in to_remove:
   432                             				g2.remove_edge(*tuple(edge)) # May cause weird things to happen because removing edges doesn't remove them from the filtration.
   433                             		else:
   434                             			g2 = g
   435                             		for clique in nx.find_cliques(g2):
   436                             			filtration.add(SimplexBirth(clique, q, sort_output))
   437    505.9 MiB      0.0 MiB   	if weak: # Builds filtration based on k nearest neighbors.
   438                             		if max_filtration_param % 1 != 0:
   439                             			raise Exception("Argument 'max_filtration_param' must be an integer if using the weak witness relation.")
   440                             		max_filtration_param = int(max_filtration_param)
   441                             		for k in xrange(int(math.fabs(max_filtration_param))):
   442                             			for witness_index in xrange(number_of_datapoints):
   443                             				if use_cliques:
   444                             					for i in xrange(k):
   445                             						g.add_edge(d[witness_index][i].id_num, d[witness_index][k].id_num)
   446                             				elif store_top_simplices:
   447                             					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(k + 1)], k, sort_output))
   448                             				else:
   449                             					if progress > 0:
   450                             						for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(k)], min(k, dimension_cutoff)):
   451                             							new_subset = ImmutableSet(base + (d[witness_index][k].id_num,))
   452                             							filtration.add(SimplexBirth(new_subset, k, sort_output))
   453                             			if use_cliques:
   454                             				filter_and_build()
   455    505.9 MiB      0.0 MiB   	if use_twr:
   456                             		print 'Using TWR'
   457                             		if max_filtration_param < 0: # Automatically determine max.
   458                             			depth = int(-max_filtration_param)
   459                             			min_distance = None
   460                             			for w in xrange(number_of_datapoints):
   461                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   462                             				if min_distance is None or new_distance < min_distance:
   463                             					min_distance = new_distance
   464                             			max_filtration_param = min_distance
   465                             		print 'The max_filtration_param is %d ' % max_filtration_param
   466                             		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   467                             		print 'The step size is %f ' % step
   468                             		print 'There will be %d steps in the filtration' % num_divisions
   469                             		progress_index = [0]*number_of_datapoints
   470                             		done = False
   471                             
   472                             		good_landmarks = [[] for x in range(number_of_datapoints)]
   473                             
   474                             		epsilons = []
   475                             		for q in xrange(num_divisions):
   476                             			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   477                             			print 'The threshold is currently %f' % threshold
   478                             			epsilons.append(threshold)
   479                             			Pre_landmarks = []
   480                             			for witness_index in xrange(number_of_datapoints):
   481                             				pre_landmarks = []
   482                             				add_simplex = False
   483                             				progress = 0
   484                             				while True:
   485                             					progress = progress_index[witness_index]
   486                             					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   487                             						break
   488                             					if progress == number_of_vertices:
   489                             						done = True
   490                             						break
   491                             
   492                             					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   493                             						pre_landmarks.append(d[witness_index][progress].id_num) # PRE_LANDMARKS CONTAINS ID NUMBER
   494                             						progress_index[witness_index] += 1
   495                             					else:
   496                             
   497                             						pre_landmarks_size = len(pre_landmarks)
   498                             						pre_landmarks_string = str(pre_landmarks) # MAKE LIST TO STRING
   499                             						print 'At threshold value %f, witness %d has %d associated landmarks: %s ' % (threshold, witness_index, pre_landmarks_size, pre_landmarks_string)
   500                             						break
   501                             				Pre_landmarks.append(pre_landmarks)
   502                             				Pre_landmarks_size = len(Pre_landmarks)
   503                             
   504                             
   505                             
   506                             			for witness_index in xrange(number_of_datapoints - downsample_rate):
   507                             				if len(Pre_landmarks[witness_index]) == 1:
   508                             					set_range = 1
   509                             				else:
   510                             					set_range = len(Pre_landmarks[witness_index])
   511                             				for k in range(set_range):
   512                             					current_pre_landmark = Pre_landmarks[witness_index][k]
   513                             					next_pre_landmark = Pre_landmarks[witness_index][k]+1 # CHECKS ONE STEP UP FROM ID NUMBER *** SEE JAMIE'S COMMENTS ON SUCH OR CLARIFY ***
   514                             					#	print 'current pre landmark = %d , next pre landmark = %d' % (current_pre_landmark, next_pre_landmark)
   515                             					check_pre_landmark = str(Pre_landmarks[witness_index + downsample_rate]) # HMMMMM
   516                             					#		print 'We are considering the fate of landmark %d witnessed by witness %d...' % (current_pre_landmark, witness_index)
   517                             					#		print 'Should witness %d not witness landmark %d, it will be GONE!' % (witness_index + downsample_rate, current_pre_landmark + 1,)
   518                             					#		print 'Witness %d has landmark set %s' % (witness_index + downsample_rate, check_pre_landmark)
   519                             					print (Pre_landmarks[witness_index][k]) in Pre_landmarks[witness_index]
   520                             					if (Pre_landmarks[witness_index][k]+ 1) in Pre_landmarks[witness_index + downsample_rate]: # change from 1, downsample_rate to 0 to test!
   521                             						good_landmarks[witness_index].append(Pre_landmarks[witness_index][k])
   522                             				print 'Up to threshold value %f, witness %d has landmark set %s' % (threshold, witness_index, str(good_landmarks[witness_index]))
   523                             				if use_cliques:
   524                             					for i in xrange(len(good_landmarks[witness_index])):
   525                             						for j in xrange(i+1,len(good_landmarks[witness_index])):
   526                             							g.add_edge(good_landmarks[witness_index][i], good_landmarks[witness_index][j])
   527                             				else:
   528                             					if not store_top_simplices and len(good_landmarks[witness_index]) > 0:
   529                             						for base in itertools.combinations(good_landmarks[witness_index], min(len(good_landmarks[witness_index]), dimension_cutoff)):
   530                             							new_subset = ImmutableSet(base + (good_landmarks[witness_index][i],))
   531                             							filtration.add(SimplexBirth(new_subset, q, sort_output))
   532                             					add_simplex = True
   533                             				if (not use_cliques) and store_top_simplices and add_simplex and len(good_landmarks[witness_index])>= 2:
   534                             					filtration.add(SimplexBirth([good_landmarks[witness_index][i] for i in xrange(len(good_landmarks[witness_index]))], q, sort_output))
   535                             				if done:
   536                             					break
   537                             			if use_cliques:
   538                             				filter_and_build()
   539                             			if done:
   540                             				break
   541                             			#	print 'We are done with threshold %f' % threshold
   542                             	else:
   543    505.9 MiB      0.0 MiB   		if max_filtration_param < 0: # Automatically determine max.
   544                             			depth = int(-max_filtration_param)
   545                             			min_distance = None
   546                             			for w in xrange(number_of_datapoints):
   547                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   548                             				if min_distance is None or new_distance < min_distance:
   549                             					min_distance = new_distance
   550                             					if min_distance ==0:
   551                             						print "witness ",w
   552                             			max_filtration_param = min_distance
   553                             
   554    505.9 MiB      0.0 MiB   		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   555    505.9 MiB      0.0 MiB   		progress_index = [0]*number_of_datapoints
   556    505.9 MiB      0.0 MiB   		done = False
   557    505.9 MiB      0.0 MiB   		epsilons = []
   558    505.9 MiB      0.0 MiB   		for q in xrange(num_divisions):
   559    505.9 MiB      0.0 MiB   			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   560    505.9 MiB      0.0 MiB   			print 'The threshold is currently %f' % threshold
   561    505.9 MiB      0.0 MiB   			epsilons.append(threshold)
   562    505.9 MiB      0.0 MiB   			for witness_index in xrange(number_of_datapoints):
   563    505.9 MiB      0.0 MiB   				add_simplex = False
   564    505.9 MiB      0.0 MiB   				progress = 0
   565    505.9 MiB      0.0 MiB   				while True:
   566    505.9 MiB      0.0 MiB   					progress = progress_index[witness_index]
   567    505.9 MiB      0.0 MiB   					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   568                             						break
   569    505.9 MiB      0.0 MiB   					if progress == number_of_vertices:
   570    505.9 MiB      0.0 MiB   						done = True
   571    505.9 MiB      0.0 MiB   						break
   572    505.9 MiB      0.0 MiB   					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   573    505.9 MiB      0.0 MiB   						if use_cliques:
   574                             							for i in xrange(progress):
   575                             								g.add_edge(d[witness_index][i].id_num, d[witness_index][progress].id_num)
   576                             						else:
   577    505.9 MiB      0.0 MiB   							if not store_top_simplices and progress > 0:
   578                             								for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], min(progress, dimension_cutoff)):
   579                             									new_subset = ImmutableSet(base + (d[witness_index][progress].id_num,))
   580                             									filtration.add(SimplexBirth(new_subset, q, sort_output))
   581    505.9 MiB      0.0 MiB   							add_simplex = True
   582    505.9 MiB      0.0 MiB   						progress_index[witness_index] += 1
   583                             					else:
   584                             						break
   585    505.9 MiB      0.0 MiB   				if (not use_cliques) and store_top_simplices and add_simplex and progress >= 2:
   586    505.9 MiB      0.0 MiB   					list_o_landmarks = []
   587    505.9 MiB      0.0 MiB   					for landmark_index in xrange(progress):
   588    505.9 MiB      0.0 MiB   						list_o_landmarks.append(d[witness_index][landmark_index].id_num)
   589                             					#print 'At threshold %f, witness %d has landmark set %s' % (threshold, witness_index, str(list_o_landmarks))
   590    505.9 MiB      0.0 MiB   					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], q, sort_output))
   591    505.9 MiB      0.0 MiB   				if done:
   592    505.9 MiB      0.0 MiB   					break
   593    505.9 MiB      0.0 MiB   			if use_cliques:
   594                             				filter_and_build()
   595    505.9 MiB      0.0 MiB   			if done:
   596    505.9 MiB      0.0 MiB   				break
   597                             
   598                             
   599    505.9 MiB      0.0 MiB   	extra_data = (landmarks, witnesses)
   600    505.9 MiB      0.0 MiB   	if weak:
   601                             		max_epsilon = 0.0
   602                             		for w in xrange(number_of_datapoints):
   603                             			#print("type: %s" % type(d[w][max_filtration_param - 1].distance))
   604                             			#print("value: %f" % d[w][max_filtration_param - 1].distance)
   605                             			if (d[w][max_filtration_param - 1].distance) > max_epsilon:
   606                             				max_epsilon = d[w][max_filtration_param - 1].distance
   607                             		print("Done. Filtration contains %i top simplex birth events, with the largest epsilon equal to %f.\n" % (len(filtration), max_epsilon))
   608                             	else:
   609    505.9 MiB      0.0 MiB   		max_sb_length = 0
   610    505.9 MiB      0.0 MiB   		for sb in filtration:
   611    505.9 MiB      0.0 MiB   			if len(sb.landmark_set) > max_sb_length:
   612    505.9 MiB      0.0 MiB   				max_sb_length = len(sb.landmark_set)
   613    505.9 MiB      0.0 MiB   		print("Done. Filtration contains %i top simplex birth events, with the largest one comprised of %i landmarks.\nMax filtration parameter: %s.\n" % (len(filtration), max_sb_length, max_filtration_param))
   614                             
   615                             	## Write to output file
   616    505.9 MiB      0.0 MiB   	output_file_name = get_param("out")
   617                             
   618    505.9 MiB      0.0 MiB   	if not output_file_name is None:
   619                             		output_file = open(output_file_name, "w")
   620                             		output_file.truncate()
   621                             		program = get_param("program")
   622                             		if dimension_cutoff is None:
   623                             			print("Writing filtration for input into %s..." % program)
   624                             			dimension_cutoff = number_of_vertices
   625                             		else:
   626                             			print("Writing filtration to file %s for input into %s, ignoring simplices above dimension %i..." % (output_file_name,program, dimension_cutoff))
   627                             		num_lines = 0
   628                             		if program == "Perseus":
   629                             			sets_printed_so_far = Set()
   630                             			num_lines = len(filtration) + 1
   631                             			output_file.write("1\n")
   632                             			list_filtration = None
   633                             			if (sort_output):
   634                             				list_filtration = list(filtration)
   635                             				list_filtration.sort()
   636                             			for simplex_birth in (list_filtration if sort_output else filtration):
   637                             				dimension = len(simplex_birth.landmark_set) - 1
   638                             				if dimension > dimension_cutoff:
   639                             					for subtuple in itertools.combinations(simplex_birth.landmark_set, dimension_cutoff + 1):
   640                             						subset = ImmutableSet(subtuple)
   641                             						if not ((subset, simplex_birth.birth_time) in sets_printed_so_far):
   642                             							output_file.write(str(dimension_cutoff) + " ")
   643                             							for landmark in subset:
   644                             								output_file.write(str(landmark + 1) + " ")
   645                             							output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   646                             							sets_printed_so_far.add((subset, simplex_birth.birth_time))
   647                             				else:
   648                             					if not ((simplex_birth.landmark_set, simplex_birth.birth_time) in sets_printed_so_far):
   649                             						output_file.write(str(dimension) + " ")
   650                             						for landmark in (simplex_birth.sll if sort_output else simplex_birth.landmark_set):
   651                             							output_file.write(str(landmark + 1) + " ")
   652                             						output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   653                             						sets_printed_so_far.add((simplex_birth.landmark_set, simplex_birth.birth_time))
   654                             		elif program == "PHAT":
   655                             			line_map = {}
   656                             			for i in xrange(number_of_vertices - 1):
   657                             				output_file.write("0\n")
   658                             				line_map[ImmutableSet([i])] = i
   659                             			output_file.write("0")
   660                             			line_map[ImmutableSet([number_of_vertices - 1])] = number_of_vertices - 1
   661                             			simultaneous_additions = []
   662                             			class Context: # Note: if upgrading to Python 3, one could just use the nonlocal keyword (see below comment).
   663                             				line_number = number_of_vertices
   664                             			list_filtration = list(filtration)
   665                             			list_filtration.sort()
   666                             			last_birth_time = 0
   667                             			def process_and_get_line_number(s):
   668                             				#nonlocal line_number
   669                             				if s in line_map:
   670                             					return line_map[s]
   671                             				else:
   672                             					dimension = len(s) - 1
   673                             					if dimension > dimension_cutoff:
   674                             						for subset in itertools.combinations(s, dimension_cutoff + 1): # Take all subsets of size dimension_cutoff + 1
   675                             							process_and_get_line_number(ImmutableSet(subset))
   676                             					elif dimension > 0:
   677                             						subsets_line_numbers = []
   678                             						for e in s:
   679                             							subsets_line_numbers.append(process_and_get_line_number(ImmutableSet(s - Set([e]))))
   680                             						output_file.write("\n" + str(dimension))
   681                             						for l in subsets_line_numbers:
   682                             							output_file.write(" " + str(l))
   683                             						line_map[s] = Context.line_number
   684                             						Context.line_number += 1
   685                             						return Context.line_number - 1
   686                             					else:
   687                             						raise Exception("Should have already added single point for base case: " + str(s))
   688                             			for simplex_birth in list_filtration:
   689                             				if simplex_birth.birth_time > last_birth_time:
   690                             					simultaneous_additions.append((Context.line_number - 1, last_birth_time + 1)) # Every line up to and including that line number (indexing starts at 0) had that birth time or earlier (indexing starts at 1)
   691                             					last_birth_time = simplex_birth.birth_time
   692                             				process_and_get_line_number(simplex_birth.landmark_set)
   693                             			simultaneous_additions.append((sys.maxsize, last_birth_time))
   694                             			output_file.write("\n\n# Simultaneous additions: Every line up to and including __ (indexing starts at 0) has birth time __ (or earlier).")
   695                             			for addition in simultaneous_additions:
   696                             				output_file.write("\n# %20i %20i" % addition)
   697                             			extra_data = (extra_data[0], extra_data[1], simultaneous_additions)
   698                             			num_lines = Context.line_number
   699                             		else:
   700                             			raise Exception("Only supported programs are 'Perseus' and 'PHAT'")
   701                             		output_file.close()
   702                             		print("Done. File contains %i lines.\n" % num_lines)
   703    505.9 MiB      0.0 MiB   	print("Filtration has been successfully built!\n")
   704    505.9 MiB      0.0 MiB   	return (filtration, extra_data + (max_filtration_param,), epsilons)


Filename: /home/elliott/programming/PHETS/PH/BuildFiltration.py

Line #    Mem usage    Increment   Line Contents
================================================
    37    542.1 MiB      0.0 MiB   @profile(stream=f)
    38                             def build_filtration(input_file_name, parameter_set, silent=False):
    39    542.1 MiB      0.0 MiB   	num_threads = 2
    40                             	global d
    41    514.1 MiB    -28.0 MiB   	d = []
    42                             
    43                             
    44    540.0 MiB     25.9 MiB   	def get_param(key):
    45    540.0 MiB      0.0 MiB   		return parameter_set.get(key)
    46                             
    47    514.1 MiB    -25.9 MiB   	input_file = open(input_file_name)
    48    514.1 MiB      0.0 MiB   	speed_amplify = float(get_param("d_speed_amplify"))
    49    514.1 MiB      0.0 MiB   	orientation_amplify = float(get_param("d_orientation_amplify"))
    50    514.1 MiB      0.0 MiB   	stretch = float(get_param("d_stretch"))
    51    514.1 MiB      0.0 MiB   	ray_distance_amplify = get_param("d_ray_distance_amplify")
    52    514.1 MiB      0.0 MiB   	use_hamiltonian = get_param("d_use_hamiltonian")
    53    514.1 MiB      0.0 MiB   	print "use hamiltonian set to ", use_hamiltonian
    54    514.1 MiB      0.0 MiB   	m2_d = float(get_param("m2_d"))
    55    514.1 MiB      0.0 MiB   	straight_VB = float(get_param("straight_VB"))
    56    514.1 MiB      0.0 MiB   	d_cov = get_param("d_cov")
    57    514.1 MiB      0.0 MiB   	graph_induced = get_param("graph_induced")
    58                             	# always_euclidean = speed_amplify == orientation_amplify == stretch == ray_distance_amplify == 1.0 and use_hamiltonian == d_cov==0.
    59                             	# print "always_euclidean set to ", always_euclidean
    60    514.1 MiB      0.0 MiB   	always_euclidean = get_param('always_euclidean')
    61                             	# use_hamiltonian = -5
    62                             
    63    514.1 MiB      0.0 MiB   	filtration = Set()
    64    514.1 MiB      0.0 MiB   	extra_data = None
    65    514.1 MiB      0.0 MiB   	min_filtration_param = float(get_param("min_filtration_param"))
    66    514.1 MiB      0.0 MiB   	max_filtration_param = float(get_param("max_filtration_param"))
    67    514.1 MiB      0.0 MiB   	print "Max filtration parameter is ", max_filtration_param
    68    514.1 MiB      0.0 MiB   	if max_filtration_param < 0 and min_filtration_param != 0:
    69                             		raise Exception("Argument 'min_filtration_param' is incompatible with automatic max_filtration_param selection.")
    70    514.1 MiB      0.0 MiB   	number_of_vertices = 0
    71    514.1 MiB      0.0 MiB   	start = get_param("start")
    72    514.1 MiB      0.0 MiB   	worm_length = get_param("worm_length")
    73    514.1 MiB      0.0 MiB   	store_top_simplices = get_param("store_top_simplices")
    74    514.1 MiB      0.0 MiB   	sort_output = get_param("sort_output")
    75    514.1 MiB      0.0 MiB   	absolute = get_param("absolute")
    76    514.1 MiB      0.0 MiB   	num_divisions = get_param("num_divisions")
    77    514.1 MiB      0.0 MiB   	simplex_cutoff = get_param("simplex_cutoff")
    78                             
    79                             
    80                             	'''=============== This code written by Sam ======================'''
    81                             
    82                             	## Read data into witness and landmark lists.
    83    514.1 MiB      0.0 MiB   	witnesses = []
    84    514.1 MiB      0.0 MiB   	landmarks = []
    85    514.1 MiB      0.0 MiB   	landmark_indices = []
    86    514.1 MiB      0.0 MiB   	ls = get_param("landmark_selector")
    87    514.1 MiB      0.0 MiB   	downsample_rate = get_param("ds_rate")
    88    514.1 MiB      0.0 MiB   	maxmin = False
    89    514.1 MiB      0.0 MiB   	counter = 0
    90    514.1 MiB      0.0 MiB   	for i in xrange(start):           #  Where to start reading data
    91                             		input_file.readline()
    92                             		counter+=1
    93    514.1 MiB      0.0 MiB   	landmark_indices=[]
    94                             
    95                             
    96    736.3 MiB    222.2 MiB   	for line in input_file.read().split("\n"):
    97    736.3 MiB      0.0 MiB   		if line != "" and counter>=start:
    98    736.3 MiB      0.0 MiB   			string_witness = line.split(" ")
    99    736.3 MiB      0.0 MiB   			witness = []
   100    736.3 MiB      0.0 MiB   			d.append([])
   101    736.3 MiB      0.0 MiB   			for coordinate in string_witness:
   102    736.3 MiB      0.0 MiB   				if coordinate != "":
   103    736.3 MiB      0.0 MiB   					witness.append(float(coordinate))
   104    736.3 MiB      0.0 MiB   			witnesses.append(witness)
   105    736.3 MiB      0.0 MiB   			counter += 1
   106    736.3 MiB      0.0 MiB   			if counter == worm_length:
   107    514.3 MiB   -222.0 MiB   				break
   108                             
   109    514.3 MiB      0.0 MiB   	number_of_datapoints = len(witnesses)
   110    514.3 MiB      0.0 MiB   	number_of_vertices = int(number_of_datapoints/downsample_rate)
   111    514.3 MiB      0.0 MiB   	num_coordinates = len(witnesses[0])
   112    514.3 MiB      0.0 MiB   	stop = start + counter
   113                             
   114    514.3 MiB      0.0 MiB   	if max_filtration_param < 0:
   115                             		if float(number_of_vertices) < abs(max_filtration_param) + 1:
   116                             			print '''ERROR: 'max_filtration_param' ({}) and number of landmarks ({})
   117                             			are incompatible. Try decreasing 'ds_rate' or increasing 'worm_length'.'''.format(max_filtration_param,
   118                             			number_of_vertices)
   119                             			sys.exit()
   120                             
   121    514.3 MiB      0.0 MiB   	num_threads = 2
   122                             	# for more information about these parameters type ./find_landmarks --help in the terminal
   123                             	# the distance calculations are calculated and outputted to a file called find_landmarks.txt
   124    514.3 MiB      0.0 MiB   	print os.getcwd()
   125                             
   126    514.3 MiB      0.0 MiB   	if ls=="EST":
   127                             		if always_euclidean:
   128                             			if graph_induced:
   129                             				find_landmarks_cmd = [
   130                             					"./find_landmarks",
   131                             					"-n {}".format(num_threads),
   132                             					"-l {}".format(number_of_vertices),
   133                             					"-w {}-{}".format(start,stop),
   134                             					"-i{}".format(input_file_name),
   135                             					"-olandmark_outputs.txt",
   136                             					"-m {}".format(int(m2_d)),
   137                             					"-a {}".format(speed_amplify),
   138                             					"-y {}".format(orientation_amplify),
   139                             					"-h {}".format(use_hamiltonian),
   140                             					"-r {}".format(ray_distance_amplify),
   141                             					"-v {}".format(straight_VB),
   142                             					"-s {}".format(stretch),
   143                             					"-e {}".format(downsample_rate),
   144                             					"-x {}".format(d_cov),
   145                             					"-c",
   146                             					"-f {}".format(max_filtration_param)
   147                             				]
   148                             			else:
   149                             				find_landmarks_cmd = [
   150                             					"./find_landmarks",
   151                             					"-n {}".format(num_threads),
   152                             					"-l {}".format(number_of_vertices),
   153                             					"-w {}-{}".format(start,stop),
   154                             					"-i{}".format(input_file_name),
   155                             					"-olandmark_outputs.txt",
   156                             					"-m {}".format(int(m2_d)),
   157                             					"-a {}".format(speed_amplify),
   158                             					"-y {}".format(orientation_amplify),
   159                             					"-h {}".format(use_hamiltonian),
   160                             					"-r {}".format(ray_distance_amplify),
   161                             					"-v {}".format(straight_VB),
   162                             					"-s {}".format(stretch),
   163                             					"-e {}".format(downsample_rate),
   164                             					"-x {}".format(d_cov),
   165                             					"-c"
   166                             				]
   167                             		else:
   168                             			if graph_induced:
   169                             				find_landmarks_cmd = [
   170                             					"./find_landmarks",
   171                             					"-n {}".format(num_threads),
   172                             					"-l {}".format(number_of_vertices),
   173                             					"-w {}-{}".format(start,stop),
   174                             					"-i{}".format(input_file_name),
   175                             					"-olandmark_outputs.txt",
   176                             					"-m {}".format(int(m2_d)),
   177                             					"-a {}".format(speed_amplify),
   178                             					"-y {}".format(orientation_amplify),
   179                             					"-h {}".format(use_hamiltonian),
   180                             					"-r {}".format(ray_distance_amplify),
   181                             					"-v {}".format(straight_VB),
   182                             					"-s {}".format(stretch),
   183                             					"-x {}".format(d_cov),
   184                             					"-e {}".format(downsample_rate),
   185                             					"-f {}".format(max_filtration_param)
   186                             				]
   187                             			else:
   188                             				find_landmarks_cmd = [
   189                             					"./find_landmarks",
   190                             					"-n {}".format(num_threads),
   191                             					"-l {}".format(number_of_vertices),
   192                             					"-w {}-{}".format(start,stop),
   193                             					"-i{}".format(input_file_name),
   194                             					"-olandmark_outputs.txt",
   195                             					"-m {}".format(int(m2_d)),
   196                             					"-a {}".format(speed_amplify),
   197                             					"-y {}".format(orientation_amplify),
   198                             					"-h {}".format(use_hamiltonian),
   199                             					"-r {}".format(ray_distance_amplify),
   200                             					"-v {}".format(straight_VB),
   201                             					"-s {}".format(stretch),
   202                             					"-x {}".format(d_cov),
   203                             					"-e {}".format(downsample_rate)
   204                             				]
   205                             	else:
   206    514.3 MiB      0.0 MiB   		if always_euclidean and m2_d!=0:
   207                             			if graph_induced:
   208                             				find_landmarks_cmd = [
   209                             					"./find_landmarks",
   210                             					"-n {}".format(num_threads),
   211                             					"-l {}".format(number_of_vertices),
   212                             					"-w {}-{}".format(start,stop),
   213                             					"-i{}".format(input_file_name),
   214                             					"-olandmark_outputs.txt",
   215                             					"-m {}".format(int(m2_d)),
   216                             					"-a {}".format(speed_amplify),
   217                             					"-y {}".format(orientation_amplify),
   218                             					"-h {}".format(use_hamiltonian),
   219                             					"-r {}".format(ray_distance_amplify),
   220                             					"-v {}".format(straight_VB),
   221                             					"-x {}".format(d_cov),
   222                             					"-s {}".format(stretch),
   223                             					"-f {}".format(max_filtration_param)
   224                             				]
   225                             			else:
   226                             				find_landmarks_cmd = [
   227                             					"./find_landmarks",
   228                             					"-n {}".format(num_threads),
   229                             					"-l {}".format(number_of_vertices),
   230                             					"-w {}-{}".format(start,stop),
   231                             					"-i{}".format(input_file_name),
   232                             					"-olandmark_outputs.txt",
   233                             					"-m {}".format(int(m2_d)),
   234                             					"-a {}".format(speed_amplify),
   235                             					"-y {}".format(orientation_amplify),
   236                             					"-h {}".format(use_hamiltonian),
   237                             					"-r {}".format(ray_distance_amplify),
   238                             					"-v {}".format(straight_VB),
   239                             					"-x {}".format(d_cov),
   240                             					"-s {}".format(stretch)
   241                             				]
   242    514.3 MiB      0.0 MiB   		elif always_euclidean:
   243                             			if graph_induced:
   244                             				find_landmarks_cmd = [
   245                             					"./find_landmarks",
   246                             					"-n {}".format(num_threads),
   247                             					"-l {}".format(number_of_vertices),
   248                             					"-w {}-{}".format(start,stop),
   249                             					"-i{}".format(input_file_name),
   250                             					"-olandmark_outputs.txt",
   251                             					"-m {}".format(int(m2_d)),
   252                             					"-a {}".format(speed_amplify),
   253                             					"-y {}".format(orientation_amplify),
   254                             					"-h {}".format(use_hamiltonian),
   255                             					"-r {}".format(ray_distance_amplify),
   256                             					"-v {}".format(straight_VB),
   257                             					"-x {}".format(d_cov),
   258                             					"-s {}".format(stretch),
   259                             					"-c",
   260                             					"-f {}".format(max_filtration_param)
   261                             				]
   262                             			else:
   263                             				find_landmarks_cmd = [
   264                             					"./find_landmarks",
   265                             					"-n {}".format(num_threads),
   266                             					"-l {}".format(number_of_vertices),
   267                             					"-w {}-{}".format(start,stop),
   268                             					"-i{}".format(input_file_name),
   269                             					"-olandmark_outputs.txt",
   270                             					"-m {}".format(int(m2_d)),
   271                             					"-a {}".format(speed_amplify),
   272                             					"-y {}".format(orientation_amplify),
   273                             					"-h {}".format(use_hamiltonian),
   274                             					"-r {}".format(ray_distance_amplify),
   275                             					"-v {}".format(straight_VB),
   276                             					"-x {}".format(d_cov),
   277                             					"-s {}".format(stretch),
   278                             					"-c"
   279                             				]
   280                             		else:
   281    514.3 MiB      0.0 MiB   			if graph_induced:
   282                             				find_landmarks_cmd = [
   283                             					"./find_landmarks",
   284                             					"-n {}".format(num_threads),
   285                             					"-l {}".format(number_of_vertices),
   286                             					"-w {}-{}".format(start,stop),
   287                             					"-i{}".format(input_file_name),
   288                             					"-olandmark_outputs.txt",
   289                             					"-m {}".format(int(m2_d)),
   290                             					"-a {}".format(speed_amplify),
   291                             					"-y {}".format(orientation_amplify),
   292                             					"-h {}".format(use_hamiltonian),
   293                             					"-r {}".format(ray_distance_amplify),
   294                             					"-v {}".format(straight_VB),
   295                             					"-x {}".format(d_cov),
   296                             					"-s {}".format(stretch),
   297                             					"-f {}".format(max_filtration_param)
   298                             				]
   299                             			else:
   300                             				find_landmarks_cmd = [
   301    514.3 MiB      0.0 MiB   					"./find_landmarks",
   302    514.3 MiB      0.0 MiB   					"-n {}".format(num_threads),
   303    514.3 MiB      0.0 MiB   					"-l {}".format(number_of_vertices),
   304    514.3 MiB      0.0 MiB   					"-w {}-{}".format(start,stop),
   305    514.3 MiB      0.0 MiB   					"-i{}".format(input_file_name),
   306    514.3 MiB      0.0 MiB   					"-olandmark_outputs.txt",
   307    514.3 MiB      0.0 MiB   					"-m {}".format(int(m2_d)),
   308    514.3 MiB      0.0 MiB   					"-a {}".format(speed_amplify),
   309    514.3 MiB      0.0 MiB   					"-y {}".format(orientation_amplify),
   310    514.3 MiB      0.0 MiB   					"-h {}".format(use_hamiltonian),
   311    514.3 MiB      0.0 MiB   					"-r {}".format(ray_distance_amplify),
   312    514.3 MiB      0.0 MiB   					"-v {}".format(straight_VB),
   313    514.3 MiB      0.0 MiB   					"-x {}".format(d_cov),
   314    514.3 MiB      0.0 MiB   					"-s {}".format(stretch)
   315                             				]
   316                             
   317    514.3 MiB      0.0 MiB   	print find_landmarks_cmd
   318                             
   319    514.3 MiB      0.0 MiB   	if silent:
   320                             		p = subprocess.Popen(find_landmarks_cmd, stdout=subprocess.PIPE)
   321                             		out, err = p.communicate()
   322                             	else:
   323    514.3 MiB      0.0 MiB   		p = subprocess.Popen(find_landmarks_cmd)
   324    514.3 MiB      0.0 MiB   		p.communicate()
   325                             
   326                             	## Build and sort distance matrix.
   327    514.3 MiB      0.0 MiB   	landmarks_file = open("landmark_outputs.txt","rb")
   328                             
   329    514.3 MiB      0.0 MiB   	l = landmarks_file.readlines()
   330    514.3 MiB      0.0 MiB   	sys.stdout.write("Reading in distance calculations...")
   331    514.3 MiB      0.0 MiB   	sys.stdout.flush()
   332    514.3 MiB      0.0 MiB   	landmark_index = 0
   333    540.0 MiB     25.8 MiB   	for line in l:
   334    539.3 MiB     -0.8 MiB   		f = line.strip('\n')
   335    539.3 MiB      0.0 MiB   		if "#" not in f:
   336    539.3 MiB      0.0 MiB   			landmark = int(f.split(":")[0])
   337                             
   338    539.3 MiB      0.0 MiB   			distances = [float(i) for i in f.split(":")[1].split(",")]
   339    540.0 MiB      0.8 MiB   			for witness_index in range(0,len(distances)):
   340                             
   341    540.0 MiB      0.0 MiB   				d[witness_index].append(LandmarkDistance(landmark_index,distances[witness_index]))
   342    540.0 MiB      0.0 MiB   			landmarks.append(witnesses[landmark])
   343    540.0 MiB      0.0 MiB   			landmark_indices.append(landmark)
   344    540.0 MiB      0.0 MiB   			landmark_index+=1
   345                             
   346    540.0 MiB      0.0 MiB   	assert(len(d)>0)
   347    540.0 MiB      0.0 MiB   	sys.stdout.write("done\n")
   348    540.0 MiB      0.0 MiB   	sys.stdout.flush()
   349                             
   350                             
   351                             
   352    540.0 MiB      0.0 MiB   	sys.stdout.write("Sorting distances...")
   353    540.0 MiB      0.0 MiB   	sys.stdout.flush()
   354                             
   355                             	# p=multiprocessing.Pool(processes=4)   # commented out by Elliott 4/25
   356                             
   357    540.0 MiB      0.0 MiB   	inputs=[]
   358    540.0 MiB      0.0 MiB   	for w in range(0,len(witnesses)):
   359    540.0 MiB      0.0 MiB   		inputs.append(w)
   360    540.0 MiB      0.0 MiB   		d[w].sort()
   361                             
   362                             	# p.map(sort,inputs)  # was commented out as of 4/25
   363                             	# p.terminate()       # added by Elliott 4/25
   364                             
   365    540.0 MiB      0.0 MiB   	sys.stdout.write("done\n")
   366    540.0 MiB      0.0 MiB   	sys.stdout.flush()
   367    540.0 MiB      0.0 MiB   	assert len(landmarks) == number_of_vertices
   368                             
   369                             	'''=============== End code written by Sam ======================'''
   370                             
   371                             	'''============= Start code written by Elliott =================='''
   372    540.0 MiB      0.0 MiB   	if graph_induced:
   373                             		# import matplotlib.pyplot as plt
   374                             		import pandas as pd
   375                             
   376                             		g = nx.read_edgelist('edgelist.txt')
   377                             
   378                             		closest_wits = np.loadtxt('closest_wits.txt', dtype='int')	# witness, landmark
   379                             		wit_coords = np.array(witnesses)
   380                             		land_coords = np.array(landmarks)
   381                             
   382                             		# land = np.unique(closest_wits[:,1])
   383                             
   384                             		# closest_wits = pd.DataFrame(closest_wits, columns=('witness', 'landmark'))
   385                             		# print closest_wits
   386                             		# closest_wits = closest_wits.sort_values(by='landmark')
   387                             		# print closest_wits
   388                             		#
   389                             		# closest_wits = closest_wits.values
   390                             		# print closest_wits
   391                             
   392                             
   393                             
   394                             		# fig = plt.figure(figsize=(8, 8))
   395                             		# ax = fig.add_subplot(111)
   396                             		# ax.scatter(wit_coords[:, 0], wit_coords[:, 1], s=.1)
   397                             		# ax.scatter(land_coords[:, 0], land_coords[:, 1])
   398                             		# fig.savefig('veronoi_test.png')
   399                             
   400                             	'''=============== End code written by Elliott =================='''
   401                             
   402                             
   403                             
   404                             
   405    540.0 MiB      0.0 MiB   	print("Building filtration...")
   406                             	## Build filtration
   407    540.0 MiB      0.0 MiB   	weak = get_param("weak")
   408    540.0 MiB      0.0 MiB   	dimension_cutoff = get_param("dimension_cutoff")
   409    540.0 MiB      0.0 MiB   	reentry_filter = get_param("reentry_filter")
   410    540.0 MiB      0.0 MiB   	if get_param("connect_time_1_skeleton") or reentry_filter: # Connect time-1-skeleton
   411                             		for i in xrange(number_of_vertices - 1):
   412                             			filtration.add(SimplexBirth(ImmutableSet([i, i + 1]), 0, sort_output))
   413    540.0 MiB      0.0 MiB   	use_cliques = get_param("use_cliques")
   414    540.0 MiB      0.0 MiB   	use_twr = get_param("use_twr")
   415                             
   416    540.0 MiB      0.0 MiB   	print '%s' % use_twr
   417    540.0 MiB      0.0 MiB   	if use_cliques: # AKA "Lazy" witness relation.
   418                             		g = nx.Graph()
   419                             		for l in xrange(number_of_vertices):
   420                             			g.add_node(l)
   421    540.0 MiB      0.0 MiB   	def filter_and_build():
   422                             		g2 = None
   423                             		if reentry_filter:
   424                             			g2 = g.copy()
   425                             			to_remove = Set()
   426                             			for l1 in xrange(number_of_vertices):
   427                             				l2 = l1 + 2
   428                             				while l2 < number_of_vertices and g2.has_edge(l1, l2):
   429                             					to_remove.add(ImmutableSet([l1, l2]))
   430                             					l2 += 1
   431                             			for edge in to_remove:
   432                             				g2.remove_edge(*tuple(edge)) # May cause weird things to happen because removing edges doesn't remove them from the filtration.
   433                             		else:
   434                             			g2 = g
   435                             		for clique in nx.find_cliques(g2):
   436                             			filtration.add(SimplexBirth(clique, q, sort_output))
   437    540.0 MiB      0.0 MiB   	if weak: # Builds filtration based on k nearest neighbors.
   438                             		if max_filtration_param % 1 != 0:
   439                             			raise Exception("Argument 'max_filtration_param' must be an integer if using the weak witness relation.")
   440                             		max_filtration_param = int(max_filtration_param)
   441                             		for k in xrange(int(math.fabs(max_filtration_param))):
   442                             			for witness_index in xrange(number_of_datapoints):
   443                             				if use_cliques:
   444                             					for i in xrange(k):
   445                             						g.add_edge(d[witness_index][i].id_num, d[witness_index][k].id_num)
   446                             				elif store_top_simplices:
   447                             					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(k + 1)], k, sort_output))
   448                             				else:
   449                             					if progress > 0:
   450                             						for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(k)], min(k, dimension_cutoff)):
   451                             							new_subset = ImmutableSet(base + (d[witness_index][k].id_num,))
   452                             							filtration.add(SimplexBirth(new_subset, k, sort_output))
   453                             			if use_cliques:
   454                             				filter_and_build()
   455    540.0 MiB      0.0 MiB   	if use_twr:
   456                             		print 'Using TWR'
   457                             		if max_filtration_param < 0: # Automatically determine max.
   458                             			depth = int(-max_filtration_param)
   459                             			min_distance = None
   460                             			for w in xrange(number_of_datapoints):
   461                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   462                             				if min_distance is None or new_distance < min_distance:
   463                             					min_distance = new_distance
   464                             			max_filtration_param = min_distance
   465                             		print 'The max_filtration_param is %d ' % max_filtration_param
   466                             		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   467                             		print 'The step size is %f ' % step
   468                             		print 'There will be %d steps in the filtration' % num_divisions
   469                             		progress_index = [0]*number_of_datapoints
   470                             		done = False
   471                             
   472                             		good_landmarks = [[] for x in range(number_of_datapoints)]
   473                             
   474                             		epsilons = []
   475                             		for q in xrange(num_divisions):
   476                             			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   477                             			print 'The threshold is currently %f' % threshold
   478                             			epsilons.append(threshold)
   479                             			Pre_landmarks = []
   480                             			for witness_index in xrange(number_of_datapoints):
   481                             				pre_landmarks = []
   482                             				add_simplex = False
   483                             				progress = 0
   484                             				while True:
   485                             					progress = progress_index[witness_index]
   486                             					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   487                             						break
   488                             					if progress == number_of_vertices:
   489                             						done = True
   490                             						break
   491                             
   492                             					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   493                             						pre_landmarks.append(d[witness_index][progress].id_num) # PRE_LANDMARKS CONTAINS ID NUMBER
   494                             						progress_index[witness_index] += 1
   495                             					else:
   496                             
   497                             						pre_landmarks_size = len(pre_landmarks)
   498                             						pre_landmarks_string = str(pre_landmarks) # MAKE LIST TO STRING
   499                             						print 'At threshold value %f, witness %d has %d associated landmarks: %s ' % (threshold, witness_index, pre_landmarks_size, pre_landmarks_string)
   500                             						break
   501                             				Pre_landmarks.append(pre_landmarks)
   502                             				Pre_landmarks_size = len(Pre_landmarks)
   503                             
   504                             
   505                             
   506                             			for witness_index in xrange(number_of_datapoints - downsample_rate):
   507                             				if len(Pre_landmarks[witness_index]) == 1:
   508                             					set_range = 1
   509                             				else:
   510                             					set_range = len(Pre_landmarks[witness_index])
   511                             				for k in range(set_range):
   512                             					current_pre_landmark = Pre_landmarks[witness_index][k]
   513                             					next_pre_landmark = Pre_landmarks[witness_index][k]+1 # CHECKS ONE STEP UP FROM ID NUMBER *** SEE JAMIE'S COMMENTS ON SUCH OR CLARIFY ***
   514                             					#	print 'current pre landmark = %d , next pre landmark = %d' % (current_pre_landmark, next_pre_landmark)
   515                             					check_pre_landmark = str(Pre_landmarks[witness_index + downsample_rate]) # HMMMMM
   516                             					#		print 'We are considering the fate of landmark %d witnessed by witness %d...' % (current_pre_landmark, witness_index)
   517                             					#		print 'Should witness %d not witness landmark %d, it will be GONE!' % (witness_index + downsample_rate, current_pre_landmark + 1,)
   518                             					#		print 'Witness %d has landmark set %s' % (witness_index + downsample_rate, check_pre_landmark)
   519                             					print (Pre_landmarks[witness_index][k]) in Pre_landmarks[witness_index]
   520                             					if (Pre_landmarks[witness_index][k]+ 1) in Pre_landmarks[witness_index + downsample_rate]: # change from 1, downsample_rate to 0 to test!
   521                             						good_landmarks[witness_index].append(Pre_landmarks[witness_index][k])
   522                             				print 'Up to threshold value %f, witness %d has landmark set %s' % (threshold, witness_index, str(good_landmarks[witness_index]))
   523                             				if use_cliques:
   524                             					for i in xrange(len(good_landmarks[witness_index])):
   525                             						for j in xrange(i+1,len(good_landmarks[witness_index])):
   526                             							g.add_edge(good_landmarks[witness_index][i], good_landmarks[witness_index][j])
   527                             				else:
   528                             					if not store_top_simplices and len(good_landmarks[witness_index]) > 0:
   529                             						for base in itertools.combinations(good_landmarks[witness_index], min(len(good_landmarks[witness_index]), dimension_cutoff)):
   530                             							new_subset = ImmutableSet(base + (good_landmarks[witness_index][i],))
   531                             							filtration.add(SimplexBirth(new_subset, q, sort_output))
   532                             					add_simplex = True
   533                             				if (not use_cliques) and store_top_simplices and add_simplex and len(good_landmarks[witness_index])>= 2:
   534                             					filtration.add(SimplexBirth([good_landmarks[witness_index][i] for i in xrange(len(good_landmarks[witness_index]))], q, sort_output))
   535                             				if done:
   536                             					break
   537                             			if use_cliques:
   538                             				filter_and_build()
   539                             			if done:
   540                             				break
   541                             			#	print 'We are done with threshold %f' % threshold
   542                             	else:
   543    540.0 MiB      0.0 MiB   		if max_filtration_param < 0: # Automatically determine max.
   544                             			depth = int(-max_filtration_param)
   545                             			min_distance = None
   546                             			for w in xrange(number_of_datapoints):
   547                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   548                             				if min_distance is None or new_distance < min_distance:
   549                             					min_distance = new_distance
   550                             					if min_distance ==0:
   551                             						print "witness ",w
   552                             			max_filtration_param = min_distance
   553                             
   554    540.0 MiB      0.0 MiB   		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   555    540.0 MiB      0.0 MiB   		progress_index = [0]*number_of_datapoints
   556    540.0 MiB      0.0 MiB   		done = False
   557    540.0 MiB      0.0 MiB   		epsilons = []
   558    540.0 MiB      0.0 MiB   		for q in xrange(num_divisions):
   559    540.0 MiB      0.0 MiB   			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   560    540.0 MiB      0.0 MiB   			print 'The threshold is currently %f' % threshold
   561    540.0 MiB      0.0 MiB   			epsilons.append(threshold)
   562    540.0 MiB      0.0 MiB   			for witness_index in xrange(number_of_datapoints):
   563    540.0 MiB      0.0 MiB   				add_simplex = False
   564    540.0 MiB      0.0 MiB   				progress = 0
   565    540.0 MiB      0.0 MiB   				while True:
   566    540.0 MiB      0.0 MiB   					progress = progress_index[witness_index]
   567    540.0 MiB      0.0 MiB   					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   568                             						break
   569    540.0 MiB      0.0 MiB   					if progress == number_of_vertices:
   570                             						done = True
   571                             						break
   572    540.0 MiB      0.0 MiB   					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   573    540.0 MiB      0.0 MiB   						if use_cliques:
   574                             							for i in xrange(progress):
   575                             								g.add_edge(d[witness_index][i].id_num, d[witness_index][progress].id_num)
   576                             						else:
   577    540.0 MiB      0.0 MiB   							if not store_top_simplices and progress > 0:
   578                             								for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], min(progress, dimension_cutoff)):
   579                             									new_subset = ImmutableSet(base + (d[witness_index][progress].id_num,))
   580                             									filtration.add(SimplexBirth(new_subset, q, sort_output))
   581    540.0 MiB      0.0 MiB   							add_simplex = True
   582    540.0 MiB      0.0 MiB   						progress_index[witness_index] += 1
   583                             					else:
   584    540.0 MiB      0.0 MiB   						break
   585    540.0 MiB      0.0 MiB   				if (not use_cliques) and store_top_simplices and add_simplex and progress >= 2:
   586    540.0 MiB      0.0 MiB   					list_o_landmarks = []
   587    540.0 MiB      0.0 MiB   					for landmark_index in xrange(progress):
   588    540.0 MiB      0.0 MiB   						list_o_landmarks.append(d[witness_index][landmark_index].id_num)
   589                             					#print 'At threshold %f, witness %d has landmark set %s' % (threshold, witness_index, str(list_o_landmarks))
   590    540.0 MiB      0.0 MiB   					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], q, sort_output))
   591    540.0 MiB      0.0 MiB   				if done:
   592                             					break
   593    540.0 MiB      0.0 MiB   			if use_cliques:
   594                             				filter_and_build()
   595    540.0 MiB      0.0 MiB   			if done:
   596                             				break
   597                             
   598                             
   599    540.0 MiB      0.0 MiB   	extra_data = (landmarks, witnesses)
   600    540.0 MiB      0.0 MiB   	if weak:
   601                             		max_epsilon = 0.0
   602                             		for w in xrange(number_of_datapoints):
   603                             			#print("type: %s" % type(d[w][max_filtration_param - 1].distance))
   604                             			#print("value: %f" % d[w][max_filtration_param - 1].distance)
   605                             			if (d[w][max_filtration_param - 1].distance) > max_epsilon:
   606                             				max_epsilon = d[w][max_filtration_param - 1].distance
   607                             		print("Done. Filtration contains %i top simplex birth events, with the largest epsilon equal to %f.\n" % (len(filtration), max_epsilon))
   608                             	else:
   609    540.0 MiB      0.0 MiB   		max_sb_length = 0
   610    540.0 MiB      0.0 MiB   		for sb in filtration:
   611    540.0 MiB      0.0 MiB   			if len(sb.landmark_set) > max_sb_length:
   612    540.0 MiB      0.0 MiB   				max_sb_length = len(sb.landmark_set)
   613    540.0 MiB      0.0 MiB   		print("Done. Filtration contains %i top simplex birth events, with the largest one comprised of %i landmarks.\nMax filtration parameter: %s.\n" % (len(filtration), max_sb_length, max_filtration_param))
   614                             
   615                             	## Write to output file
   616    540.0 MiB      0.0 MiB   	output_file_name = get_param("out")
   617                             
   618    540.0 MiB      0.0 MiB   	if not output_file_name is None:
   619                             		output_file = open(output_file_name, "w")
   620                             		output_file.truncate()
   621                             		program = get_param("program")
   622                             		if dimension_cutoff is None:
   623                             			print("Writing filtration for input into %s..." % program)
   624                             			dimension_cutoff = number_of_vertices
   625                             		else:
   626                             			print("Writing filtration to file %s for input into %s, ignoring simplices above dimension %i..." % (output_file_name,program, dimension_cutoff))
   627                             		num_lines = 0
   628                             		if program == "Perseus":
   629                             			sets_printed_so_far = Set()
   630                             			num_lines = len(filtration) + 1
   631                             			output_file.write("1\n")
   632                             			list_filtration = None
   633                             			if (sort_output):
   634                             				list_filtration = list(filtration)
   635                             				list_filtration.sort()
   636                             			for simplex_birth in (list_filtration if sort_output else filtration):
   637                             				dimension = len(simplex_birth.landmark_set) - 1
   638                             				if dimension > dimension_cutoff:
   639                             					for subtuple in itertools.combinations(simplex_birth.landmark_set, dimension_cutoff + 1):
   640                             						subset = ImmutableSet(subtuple)
   641                             						if not ((subset, simplex_birth.birth_time) in sets_printed_so_far):
   642                             							output_file.write(str(dimension_cutoff) + " ")
   643                             							for landmark in subset:
   644                             								output_file.write(str(landmark + 1) + " ")
   645                             							output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   646                             							sets_printed_so_far.add((subset, simplex_birth.birth_time))
   647                             				else:
   648                             					if not ((simplex_birth.landmark_set, simplex_birth.birth_time) in sets_printed_so_far):
   649                             						output_file.write(str(dimension) + " ")
   650                             						for landmark in (simplex_birth.sll if sort_output else simplex_birth.landmark_set):
   651                             							output_file.write(str(landmark + 1) + " ")
   652                             						output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   653                             						sets_printed_so_far.add((simplex_birth.landmark_set, simplex_birth.birth_time))
   654                             		elif program == "PHAT":
   655                             			line_map = {}
   656                             			for i in xrange(number_of_vertices - 1):
   657                             				output_file.write("0\n")
   658                             				line_map[ImmutableSet([i])] = i
   659                             			output_file.write("0")
   660                             			line_map[ImmutableSet([number_of_vertices - 1])] = number_of_vertices - 1
   661                             			simultaneous_additions = []
   662                             			class Context: # Note: if upgrading to Python 3, one could just use the nonlocal keyword (see below comment).
   663                             				line_number = number_of_vertices
   664                             			list_filtration = list(filtration)
   665                             			list_filtration.sort()
   666                             			last_birth_time = 0
   667                             			def process_and_get_line_number(s):
   668                             				#nonlocal line_number
   669                             				if s in line_map:
   670                             					return line_map[s]
   671                             				else:
   672                             					dimension = len(s) - 1
   673                             					if dimension > dimension_cutoff:
   674                             						for subset in itertools.combinations(s, dimension_cutoff + 1): # Take all subsets of size dimension_cutoff + 1
   675                             							process_and_get_line_number(ImmutableSet(subset))
   676                             					elif dimension > 0:
   677                             						subsets_line_numbers = []
   678                             						for e in s:
   679                             							subsets_line_numbers.append(process_and_get_line_number(ImmutableSet(s - Set([e]))))
   680                             						output_file.write("\n" + str(dimension))
   681                             						for l in subsets_line_numbers:
   682                             							output_file.write(" " + str(l))
   683                             						line_map[s] = Context.line_number
   684                             						Context.line_number += 1
   685                             						return Context.line_number - 1
   686                             					else:
   687                             						raise Exception("Should have already added single point for base case: " + str(s))
   688                             			for simplex_birth in list_filtration:
   689                             				if simplex_birth.birth_time > last_birth_time:
   690                             					simultaneous_additions.append((Context.line_number - 1, last_birth_time + 1)) # Every line up to and including that line number (indexing starts at 0) had that birth time or earlier (indexing starts at 1)
   691                             					last_birth_time = simplex_birth.birth_time
   692                             				process_and_get_line_number(simplex_birth.landmark_set)
   693                             			simultaneous_additions.append((sys.maxsize, last_birth_time))
   694                             			output_file.write("\n\n# Simultaneous additions: Every line up to and including __ (indexing starts at 0) has birth time __ (or earlier).")
   695                             			for addition in simultaneous_additions:
   696                             				output_file.write("\n# %20i %20i" % addition)
   697                             			extra_data = (extra_data[0], extra_data[1], simultaneous_additions)
   698                             			num_lines = Context.line_number
   699                             		else:
   700                             			raise Exception("Only supported programs are 'Perseus' and 'PHAT'")
   701                             		output_file.close()
   702                             		print("Done. File contains %i lines.\n" % num_lines)
   703    540.0 MiB      0.0 MiB   	print("Filtration has been successfully built!\n")
   704    540.0 MiB      0.0 MiB   	return (filtration, extra_data + (max_filtration_param,), epsilons)


Filename: /home/elliott/programming/PHETS/PH/BuildFiltration.py

Line #    Mem usage    Increment   Line Contents
================================================
    37    624.8 MiB      0.0 MiB   @profile(stream=f)
    38                             def build_filtration(input_file_name, parameter_set, silent=False):
    39    624.8 MiB      0.0 MiB   	num_threads = 2
    40                             	global d
    41    599.5 MiB    -25.2 MiB   	d = []
    42                             
    43                             
    44    623.4 MiB     23.9 MiB   	def get_param(key):
    45    623.4 MiB      0.0 MiB   		return parameter_set.get(key)
    46                             
    47    599.5 MiB    -23.9 MiB   	input_file = open(input_file_name)
    48    599.5 MiB      0.0 MiB   	speed_amplify = float(get_param("d_speed_amplify"))
    49    599.5 MiB      0.0 MiB   	orientation_amplify = float(get_param("d_orientation_amplify"))
    50    599.5 MiB      0.0 MiB   	stretch = float(get_param("d_stretch"))
    51    599.5 MiB      0.0 MiB   	ray_distance_amplify = get_param("d_ray_distance_amplify")
    52    599.5 MiB      0.0 MiB   	use_hamiltonian = get_param("d_use_hamiltonian")
    53    599.5 MiB      0.0 MiB   	print "use hamiltonian set to ", use_hamiltonian
    54    599.5 MiB      0.0 MiB   	m2_d = float(get_param("m2_d"))
    55    599.5 MiB      0.0 MiB   	straight_VB = float(get_param("straight_VB"))
    56    599.5 MiB      0.0 MiB   	d_cov = get_param("d_cov")
    57    599.5 MiB      0.0 MiB   	graph_induced = get_param("graph_induced")
    58                             	# always_euclidean = speed_amplify == orientation_amplify == stretch == ray_distance_amplify == 1.0 and use_hamiltonian == d_cov==0.
    59                             	# print "always_euclidean set to ", always_euclidean
    60    599.5 MiB      0.0 MiB   	always_euclidean = get_param('always_euclidean')
    61                             	# use_hamiltonian = -5
    62                             
    63    599.5 MiB      0.0 MiB   	filtration = Set()
    64    599.5 MiB      0.0 MiB   	extra_data = None
    65    599.5 MiB      0.0 MiB   	min_filtration_param = float(get_param("min_filtration_param"))
    66    599.5 MiB      0.0 MiB   	max_filtration_param = float(get_param("max_filtration_param"))
    67    599.5 MiB      0.0 MiB   	print "Max filtration parameter is ", max_filtration_param
    68    599.5 MiB      0.0 MiB   	if max_filtration_param < 0 and min_filtration_param != 0:
    69                             		raise Exception("Argument 'min_filtration_param' is incompatible with automatic max_filtration_param selection.")
    70    599.5 MiB      0.0 MiB   	number_of_vertices = 0
    71    599.5 MiB      0.0 MiB   	start = get_param("start")
    72    599.5 MiB      0.0 MiB   	worm_length = get_param("worm_length")
    73    599.5 MiB      0.0 MiB   	store_top_simplices = get_param("store_top_simplices")
    74    599.5 MiB      0.0 MiB   	sort_output = get_param("sort_output")
    75    599.5 MiB      0.0 MiB   	absolute = get_param("absolute")
    76    599.5 MiB      0.0 MiB   	num_divisions = get_param("num_divisions")
    77    599.5 MiB      0.0 MiB   	simplex_cutoff = get_param("simplex_cutoff")
    78                             
    79                             
    80                             	'''=============== This code written by Sam ======================'''
    81                             
    82                             	## Read data into witness and landmark lists.
    83    599.5 MiB      0.0 MiB   	witnesses = []
    84    599.5 MiB      0.0 MiB   	landmarks = []
    85    599.5 MiB      0.0 MiB   	landmark_indices = []
    86    599.5 MiB      0.0 MiB   	ls = get_param("landmark_selector")
    87    599.5 MiB      0.0 MiB   	downsample_rate = get_param("ds_rate")
    88    599.5 MiB      0.0 MiB   	maxmin = False
    89    599.5 MiB      0.0 MiB   	counter = 0
    90    599.5 MiB      0.0 MiB   	for i in xrange(start):           #  Where to start reading data
    91                             		input_file.readline()
    92                             		counter+=1
    93    599.5 MiB      0.0 MiB   	landmark_indices=[]
    94                             
    95                             
    96    819.9 MiB    220.4 MiB   	for line in input_file.read().split("\n"):
    97    819.9 MiB      0.0 MiB   		if line != "" and counter>=start:
    98    819.9 MiB      0.0 MiB   			string_witness = line.split(" ")
    99    819.9 MiB      0.0 MiB   			witness = []
   100    819.9 MiB      0.0 MiB   			d.append([])
   101    819.9 MiB      0.0 MiB   			for coordinate in string_witness:
   102    819.9 MiB      0.0 MiB   				if coordinate != "":
   103    819.9 MiB      0.0 MiB   					witness.append(float(coordinate))
   104    819.9 MiB      0.0 MiB   			witnesses.append(witness)
   105    819.9 MiB      0.0 MiB   			counter += 1
   106    819.9 MiB      0.0 MiB   			if counter == worm_length:
   107    599.7 MiB   -220.1 MiB   				break
   108                             
   109    599.7 MiB      0.0 MiB   	number_of_datapoints = len(witnesses)
   110    599.7 MiB      0.0 MiB   	number_of_vertices = int(number_of_datapoints/downsample_rate)
   111    599.7 MiB      0.0 MiB   	num_coordinates = len(witnesses[0])
   112    599.7 MiB      0.0 MiB   	stop = start + counter
   113                             
   114    599.7 MiB      0.0 MiB   	if max_filtration_param < 0:
   115                             		if float(number_of_vertices) < abs(max_filtration_param) + 1:
   116                             			print '''ERROR: 'max_filtration_param' ({}) and number of landmarks ({})
   117                             			are incompatible. Try decreasing 'ds_rate' or increasing 'worm_length'.'''.format(max_filtration_param,
   118                             			number_of_vertices)
   119                             			sys.exit()
   120                             
   121    599.7 MiB      0.0 MiB   	num_threads = 2
   122                             	# for more information about these parameters type ./find_landmarks --help in the terminal
   123                             	# the distance calculations are calculated and outputted to a file called find_landmarks.txt
   124    599.7 MiB      0.0 MiB   	print os.getcwd()
   125                             
   126    599.7 MiB      0.0 MiB   	if ls=="EST":
   127                             		if always_euclidean:
   128                             			if graph_induced:
   129                             				find_landmarks_cmd = [
   130                             					"./find_landmarks",
   131                             					"-n {}".format(num_threads),
   132                             					"-l {}".format(number_of_vertices),
   133                             					"-w {}-{}".format(start,stop),
   134                             					"-i{}".format(input_file_name),
   135                             					"-olandmark_outputs.txt",
   136                             					"-m {}".format(int(m2_d)),
   137                             					"-a {}".format(speed_amplify),
   138                             					"-y {}".format(orientation_amplify),
   139                             					"-h {}".format(use_hamiltonian),
   140                             					"-r {}".format(ray_distance_amplify),
   141                             					"-v {}".format(straight_VB),
   142                             					"-s {}".format(stretch),
   143                             					"-e {}".format(downsample_rate),
   144                             					"-x {}".format(d_cov),
   145                             					"-c",
   146                             					"-f {}".format(max_filtration_param)
   147                             				]
   148                             			else:
   149                             				find_landmarks_cmd = [
   150                             					"./find_landmarks",
   151                             					"-n {}".format(num_threads),
   152                             					"-l {}".format(number_of_vertices),
   153                             					"-w {}-{}".format(start,stop),
   154                             					"-i{}".format(input_file_name),
   155                             					"-olandmark_outputs.txt",
   156                             					"-m {}".format(int(m2_d)),
   157                             					"-a {}".format(speed_amplify),
   158                             					"-y {}".format(orientation_amplify),
   159                             					"-h {}".format(use_hamiltonian),
   160                             					"-r {}".format(ray_distance_amplify),
   161                             					"-v {}".format(straight_VB),
   162                             					"-s {}".format(stretch),
   163                             					"-e {}".format(downsample_rate),
   164                             					"-x {}".format(d_cov),
   165                             					"-c"
   166                             				]
   167                             		else:
   168                             			if graph_induced:
   169                             				find_landmarks_cmd = [
   170                             					"./find_landmarks",
   171                             					"-n {}".format(num_threads),
   172                             					"-l {}".format(number_of_vertices),
   173                             					"-w {}-{}".format(start,stop),
   174                             					"-i{}".format(input_file_name),
   175                             					"-olandmark_outputs.txt",
   176                             					"-m {}".format(int(m2_d)),
   177                             					"-a {}".format(speed_amplify),
   178                             					"-y {}".format(orientation_amplify),
   179                             					"-h {}".format(use_hamiltonian),
   180                             					"-r {}".format(ray_distance_amplify),
   181                             					"-v {}".format(straight_VB),
   182                             					"-s {}".format(stretch),
   183                             					"-x {}".format(d_cov),
   184                             					"-e {}".format(downsample_rate),
   185                             					"-f {}".format(max_filtration_param)
   186                             				]
   187                             			else:
   188                             				find_landmarks_cmd = [
   189                             					"./find_landmarks",
   190                             					"-n {}".format(num_threads),
   191                             					"-l {}".format(number_of_vertices),
   192                             					"-w {}-{}".format(start,stop),
   193                             					"-i{}".format(input_file_name),
   194                             					"-olandmark_outputs.txt",
   195                             					"-m {}".format(int(m2_d)),
   196                             					"-a {}".format(speed_amplify),
   197                             					"-y {}".format(orientation_amplify),
   198                             					"-h {}".format(use_hamiltonian),
   199                             					"-r {}".format(ray_distance_amplify),
   200                             					"-v {}".format(straight_VB),
   201                             					"-s {}".format(stretch),
   202                             					"-x {}".format(d_cov),
   203                             					"-e {}".format(downsample_rate)
   204                             				]
   205                             	else:
   206    599.7 MiB      0.0 MiB   		if always_euclidean and m2_d!=0:
   207                             			if graph_induced:
   208                             				find_landmarks_cmd = [
   209                             					"./find_landmarks",
   210                             					"-n {}".format(num_threads),
   211                             					"-l {}".format(number_of_vertices),
   212                             					"-w {}-{}".format(start,stop),
   213                             					"-i{}".format(input_file_name),
   214                             					"-olandmark_outputs.txt",
   215                             					"-m {}".format(int(m2_d)),
   216                             					"-a {}".format(speed_amplify),
   217                             					"-y {}".format(orientation_amplify),
   218                             					"-h {}".format(use_hamiltonian),
   219                             					"-r {}".format(ray_distance_amplify),
   220                             					"-v {}".format(straight_VB),
   221                             					"-x {}".format(d_cov),
   222                             					"-s {}".format(stretch),
   223                             					"-f {}".format(max_filtration_param)
   224                             				]
   225                             			else:
   226                             				find_landmarks_cmd = [
   227                             					"./find_landmarks",
   228                             					"-n {}".format(num_threads),
   229                             					"-l {}".format(number_of_vertices),
   230                             					"-w {}-{}".format(start,stop),
   231                             					"-i{}".format(input_file_name),
   232                             					"-olandmark_outputs.txt",
   233                             					"-m {}".format(int(m2_d)),
   234                             					"-a {}".format(speed_amplify),
   235                             					"-y {}".format(orientation_amplify),
   236                             					"-h {}".format(use_hamiltonian),
   237                             					"-r {}".format(ray_distance_amplify),
   238                             					"-v {}".format(straight_VB),
   239                             					"-x {}".format(d_cov),
   240                             					"-s {}".format(stretch)
   241                             				]
   242    599.7 MiB      0.0 MiB   		elif always_euclidean:
   243                             			if graph_induced:
   244                             				find_landmarks_cmd = [
   245                             					"./find_landmarks",
   246                             					"-n {}".format(num_threads),
   247                             					"-l {}".format(number_of_vertices),
   248                             					"-w {}-{}".format(start,stop),
   249                             					"-i{}".format(input_file_name),
   250                             					"-olandmark_outputs.txt",
   251                             					"-m {}".format(int(m2_d)),
   252                             					"-a {}".format(speed_amplify),
   253                             					"-y {}".format(orientation_amplify),
   254                             					"-h {}".format(use_hamiltonian),
   255                             					"-r {}".format(ray_distance_amplify),
   256                             					"-v {}".format(straight_VB),
   257                             					"-x {}".format(d_cov),
   258                             					"-s {}".format(stretch),
   259                             					"-c",
   260                             					"-f {}".format(max_filtration_param)
   261                             				]
   262                             			else:
   263                             				find_landmarks_cmd = [
   264                             					"./find_landmarks",
   265                             					"-n {}".format(num_threads),
   266                             					"-l {}".format(number_of_vertices),
   267                             					"-w {}-{}".format(start,stop),
   268                             					"-i{}".format(input_file_name),
   269                             					"-olandmark_outputs.txt",
   270                             					"-m {}".format(int(m2_d)),
   271                             					"-a {}".format(speed_amplify),
   272                             					"-y {}".format(orientation_amplify),
   273                             					"-h {}".format(use_hamiltonian),
   274                             					"-r {}".format(ray_distance_amplify),
   275                             					"-v {}".format(straight_VB),
   276                             					"-x {}".format(d_cov),
   277                             					"-s {}".format(stretch),
   278                             					"-c"
   279                             				]
   280                             		else:
   281    599.7 MiB      0.0 MiB   			if graph_induced:
   282                             				find_landmarks_cmd = [
   283                             					"./find_landmarks",
   284                             					"-n {}".format(num_threads),
   285                             					"-l {}".format(number_of_vertices),
   286                             					"-w {}-{}".format(start,stop),
   287                             					"-i{}".format(input_file_name),
   288                             					"-olandmark_outputs.txt",
   289                             					"-m {}".format(int(m2_d)),
   290                             					"-a {}".format(speed_amplify),
   291                             					"-y {}".format(orientation_amplify),
   292                             					"-h {}".format(use_hamiltonian),
   293                             					"-r {}".format(ray_distance_amplify),
   294                             					"-v {}".format(straight_VB),
   295                             					"-x {}".format(d_cov),
   296                             					"-s {}".format(stretch),
   297                             					"-f {}".format(max_filtration_param)
   298                             				]
   299                             			else:
   300                             				find_landmarks_cmd = [
   301    599.7 MiB      0.0 MiB   					"./find_landmarks",
   302    599.7 MiB      0.0 MiB   					"-n {}".format(num_threads),
   303    599.7 MiB      0.0 MiB   					"-l {}".format(number_of_vertices),
   304    599.7 MiB      0.0 MiB   					"-w {}-{}".format(start,stop),
   305    599.7 MiB      0.0 MiB   					"-i{}".format(input_file_name),
   306    599.7 MiB      0.0 MiB   					"-olandmark_outputs.txt",
   307    599.7 MiB      0.0 MiB   					"-m {}".format(int(m2_d)),
   308    599.7 MiB      0.0 MiB   					"-a {}".format(speed_amplify),
   309    599.7 MiB      0.0 MiB   					"-y {}".format(orientation_amplify),
   310    599.7 MiB      0.0 MiB   					"-h {}".format(use_hamiltonian),
   311    599.7 MiB      0.0 MiB   					"-r {}".format(ray_distance_amplify),
   312    599.7 MiB      0.0 MiB   					"-v {}".format(straight_VB),
   313    599.7 MiB      0.0 MiB   					"-x {}".format(d_cov),
   314    599.7 MiB      0.0 MiB   					"-s {}".format(stretch)
   315                             				]
   316                             
   317    599.7 MiB      0.0 MiB   	print find_landmarks_cmd
   318                             
   319    599.7 MiB      0.0 MiB   	if silent:
   320                             		p = subprocess.Popen(find_landmarks_cmd, stdout=subprocess.PIPE)
   321                             		out, err = p.communicate()
   322                             	else:
   323    599.7 MiB      0.0 MiB   		p = subprocess.Popen(find_landmarks_cmd)
   324    599.7 MiB      0.0 MiB   		p.communicate()
   325                             
   326                             	## Build and sort distance matrix.
   327    599.7 MiB      0.0 MiB   	landmarks_file = open("landmark_outputs.txt","rb")
   328                             
   329    599.7 MiB      0.0 MiB   	l = landmarks_file.readlines()
   330    599.7 MiB      0.0 MiB   	sys.stdout.write("Reading in distance calculations...")
   331    599.7 MiB      0.0 MiB   	sys.stdout.flush()
   332    599.7 MiB      0.0 MiB   	landmark_index = 0
   333    623.2 MiB     23.5 MiB   	for line in l:
   334    622.7 MiB     -0.5 MiB   		f = line.strip('\n')
   335    622.7 MiB      0.0 MiB   		if "#" not in f:
   336    622.7 MiB      0.0 MiB   			landmark = int(f.split(":")[0])
   337                             
   338    622.7 MiB      0.0 MiB   			distances = [float(i) for i in f.split(":")[1].split(",")]
   339    623.2 MiB      0.5 MiB   			for witness_index in range(0,len(distances)):
   340                             
   341    623.2 MiB      0.0 MiB   				d[witness_index].append(LandmarkDistance(landmark_index,distances[witness_index]))
   342    623.2 MiB      0.0 MiB   			landmarks.append(witnesses[landmark])
   343    623.2 MiB      0.0 MiB   			landmark_indices.append(landmark)
   344    623.2 MiB      0.0 MiB   			landmark_index+=1
   345                             
   346    623.2 MiB      0.0 MiB   	assert(len(d)>0)
   347    623.2 MiB      0.0 MiB   	sys.stdout.write("done\n")
   348    623.2 MiB      0.0 MiB   	sys.stdout.flush()
   349                             
   350                             
   351                             
   352    623.2 MiB      0.0 MiB   	sys.stdout.write("Sorting distances...")
   353    623.2 MiB      0.0 MiB   	sys.stdout.flush()
   354                             
   355                             	# p=multiprocessing.Pool(processes=4)   # commented out by Elliott 4/25
   356                             
   357    623.2 MiB      0.0 MiB   	inputs=[]
   358    623.2 MiB      0.0 MiB   	for w in range(0,len(witnesses)):
   359    623.2 MiB      0.0 MiB   		inputs.append(w)
   360    623.2 MiB      0.0 MiB   		d[w].sort()
   361                             
   362                             	# p.map(sort,inputs)  # was commented out as of 4/25
   363                             	# p.terminate()       # added by Elliott 4/25
   364                             
   365    623.2 MiB      0.0 MiB   	sys.stdout.write("done\n")
   366    623.2 MiB      0.0 MiB   	sys.stdout.flush()
   367    623.2 MiB      0.0 MiB   	assert len(landmarks) == number_of_vertices
   368                             
   369                             	'''=============== End code written by Sam ======================'''
   370                             
   371                             	'''============= Start code written by Elliott =================='''
   372    623.2 MiB      0.0 MiB   	if graph_induced:
   373                             		# import matplotlib.pyplot as plt
   374                             		import pandas as pd
   375                             
   376                             		g = nx.read_edgelist('edgelist.txt')
   377                             
   378                             		closest_wits = np.loadtxt('closest_wits.txt', dtype='int')	# witness, landmark
   379                             		wit_coords = np.array(witnesses)
   380                             		land_coords = np.array(landmarks)
   381                             
   382                             		# land = np.unique(closest_wits[:,1])
   383                             
   384                             		# closest_wits = pd.DataFrame(closest_wits, columns=('witness', 'landmark'))
   385                             		# print closest_wits
   386                             		# closest_wits = closest_wits.sort_values(by='landmark')
   387                             		# print closest_wits
   388                             		#
   389                             		# closest_wits = closest_wits.values
   390                             		# print closest_wits
   391                             
   392                             
   393                             
   394                             		# fig = plt.figure(figsize=(8, 8))
   395                             		# ax = fig.add_subplot(111)
   396                             		# ax.scatter(wit_coords[:, 0], wit_coords[:, 1], s=.1)
   397                             		# ax.scatter(land_coords[:, 0], land_coords[:, 1])
   398                             		# fig.savefig('veronoi_test.png')
   399                             
   400                             	'''=============== End code written by Elliott =================='''
   401                             
   402                             
   403                             
   404                             
   405    623.2 MiB      0.0 MiB   	print("Building filtration...")
   406                             	## Build filtration
   407    623.2 MiB      0.0 MiB   	weak = get_param("weak")
   408    623.2 MiB      0.0 MiB   	dimension_cutoff = get_param("dimension_cutoff")
   409    623.2 MiB      0.0 MiB   	reentry_filter = get_param("reentry_filter")
   410    623.2 MiB      0.0 MiB   	if get_param("connect_time_1_skeleton") or reentry_filter: # Connect time-1-skeleton
   411                             		for i in xrange(number_of_vertices - 1):
   412                             			filtration.add(SimplexBirth(ImmutableSet([i, i + 1]), 0, sort_output))
   413    623.2 MiB      0.0 MiB   	use_cliques = get_param("use_cliques")
   414    623.2 MiB      0.0 MiB   	use_twr = get_param("use_twr")
   415                             
   416    623.2 MiB      0.0 MiB   	print '%s' % use_twr
   417    623.2 MiB      0.0 MiB   	if use_cliques: # AKA "Lazy" witness relation.
   418                             		g = nx.Graph()
   419                             		for l in xrange(number_of_vertices):
   420                             			g.add_node(l)
   421    623.2 MiB      0.0 MiB   	def filter_and_build():
   422                             		g2 = None
   423                             		if reentry_filter:
   424                             			g2 = g.copy()
   425                             			to_remove = Set()
   426                             			for l1 in xrange(number_of_vertices):
   427                             				l2 = l1 + 2
   428                             				while l2 < number_of_vertices and g2.has_edge(l1, l2):
   429                             					to_remove.add(ImmutableSet([l1, l2]))
   430                             					l2 += 1
   431                             			for edge in to_remove:
   432                             				g2.remove_edge(*tuple(edge)) # May cause weird things to happen because removing edges doesn't remove them from the filtration.
   433                             		else:
   434                             			g2 = g
   435                             		for clique in nx.find_cliques(g2):
   436                             			filtration.add(SimplexBirth(clique, q, sort_output))
   437    623.2 MiB      0.0 MiB   	if weak: # Builds filtration based on k nearest neighbors.
   438                             		if max_filtration_param % 1 != 0:
   439                             			raise Exception("Argument 'max_filtration_param' must be an integer if using the weak witness relation.")
   440                             		max_filtration_param = int(max_filtration_param)
   441                             		for k in xrange(int(math.fabs(max_filtration_param))):
   442                             			for witness_index in xrange(number_of_datapoints):
   443                             				if use_cliques:
   444                             					for i in xrange(k):
   445                             						g.add_edge(d[witness_index][i].id_num, d[witness_index][k].id_num)
   446                             				elif store_top_simplices:
   447                             					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(k + 1)], k, sort_output))
   448                             				else:
   449                             					if progress > 0:
   450                             						for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(k)], min(k, dimension_cutoff)):
   451                             							new_subset = ImmutableSet(base + (d[witness_index][k].id_num,))
   452                             							filtration.add(SimplexBirth(new_subset, k, sort_output))
   453                             			if use_cliques:
   454                             				filter_and_build()
   455    623.2 MiB      0.0 MiB   	if use_twr:
   456                             		print 'Using TWR'
   457                             		if max_filtration_param < 0: # Automatically determine max.
   458                             			depth = int(-max_filtration_param)
   459                             			min_distance = None
   460                             			for w in xrange(number_of_datapoints):
   461                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   462                             				if min_distance is None or new_distance < min_distance:
   463                             					min_distance = new_distance
   464                             			max_filtration_param = min_distance
   465                             		print 'The max_filtration_param is %d ' % max_filtration_param
   466                             		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   467                             		print 'The step size is %f ' % step
   468                             		print 'There will be %d steps in the filtration' % num_divisions
   469                             		progress_index = [0]*number_of_datapoints
   470                             		done = False
   471                             
   472                             		good_landmarks = [[] for x in range(number_of_datapoints)]
   473                             
   474                             		epsilons = []
   475                             		for q in xrange(num_divisions):
   476                             			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   477                             			print 'The threshold is currently %f' % threshold
   478                             			epsilons.append(threshold)
   479                             			Pre_landmarks = []
   480                             			for witness_index in xrange(number_of_datapoints):
   481                             				pre_landmarks = []
   482                             				add_simplex = False
   483                             				progress = 0
   484                             				while True:
   485                             					progress = progress_index[witness_index]
   486                             					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   487                             						break
   488                             					if progress == number_of_vertices:
   489                             						done = True
   490                             						break
   491                             
   492                             					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   493                             						pre_landmarks.append(d[witness_index][progress].id_num) # PRE_LANDMARKS CONTAINS ID NUMBER
   494                             						progress_index[witness_index] += 1
   495                             					else:
   496                             
   497                             						pre_landmarks_size = len(pre_landmarks)
   498                             						pre_landmarks_string = str(pre_landmarks) # MAKE LIST TO STRING
   499                             						print 'At threshold value %f, witness %d has %d associated landmarks: %s ' % (threshold, witness_index, pre_landmarks_size, pre_landmarks_string)
   500                             						break
   501                             				Pre_landmarks.append(pre_landmarks)
   502                             				Pre_landmarks_size = len(Pre_landmarks)
   503                             
   504                             
   505                             
   506                             			for witness_index in xrange(number_of_datapoints - downsample_rate):
   507                             				if len(Pre_landmarks[witness_index]) == 1:
   508                             					set_range = 1
   509                             				else:
   510                             					set_range = len(Pre_landmarks[witness_index])
   511                             				for k in range(set_range):
   512                             					current_pre_landmark = Pre_landmarks[witness_index][k]
   513                             					next_pre_landmark = Pre_landmarks[witness_index][k]+1 # CHECKS ONE STEP UP FROM ID NUMBER *** SEE JAMIE'S COMMENTS ON SUCH OR CLARIFY ***
   514                             					#	print 'current pre landmark = %d , next pre landmark = %d' % (current_pre_landmark, next_pre_landmark)
   515                             					check_pre_landmark = str(Pre_landmarks[witness_index + downsample_rate]) # HMMMMM
   516                             					#		print 'We are considering the fate of landmark %d witnessed by witness %d...' % (current_pre_landmark, witness_index)
   517                             					#		print 'Should witness %d not witness landmark %d, it will be GONE!' % (witness_index + downsample_rate, current_pre_landmark + 1,)
   518                             					#		print 'Witness %d has landmark set %s' % (witness_index + downsample_rate, check_pre_landmark)
   519                             					print (Pre_landmarks[witness_index][k]) in Pre_landmarks[witness_index]
   520                             					if (Pre_landmarks[witness_index][k]+ 1) in Pre_landmarks[witness_index + downsample_rate]: # change from 1, downsample_rate to 0 to test!
   521                             						good_landmarks[witness_index].append(Pre_landmarks[witness_index][k])
   522                             				print 'Up to threshold value %f, witness %d has landmark set %s' % (threshold, witness_index, str(good_landmarks[witness_index]))
   523                             				if use_cliques:
   524                             					for i in xrange(len(good_landmarks[witness_index])):
   525                             						for j in xrange(i+1,len(good_landmarks[witness_index])):
   526                             							g.add_edge(good_landmarks[witness_index][i], good_landmarks[witness_index][j])
   527                             				else:
   528                             					if not store_top_simplices and len(good_landmarks[witness_index]) > 0:
   529                             						for base in itertools.combinations(good_landmarks[witness_index], min(len(good_landmarks[witness_index]), dimension_cutoff)):
   530                             							new_subset = ImmutableSet(base + (good_landmarks[witness_index][i],))
   531                             							filtration.add(SimplexBirth(new_subset, q, sort_output))
   532                             					add_simplex = True
   533                             				if (not use_cliques) and store_top_simplices and add_simplex and len(good_landmarks[witness_index])>= 2:
   534                             					filtration.add(SimplexBirth([good_landmarks[witness_index][i] for i in xrange(len(good_landmarks[witness_index]))], q, sort_output))
   535                             				if done:
   536                             					break
   537                             			if use_cliques:
   538                             				filter_and_build()
   539                             			if done:
   540                             				break
   541                             			#	print 'We are done with threshold %f' % threshold
   542                             	else:
   543    623.2 MiB      0.0 MiB   		if max_filtration_param < 0: # Automatically determine max.
   544                             			depth = int(-max_filtration_param)
   545                             			min_distance = None
   546                             			for w in xrange(number_of_datapoints):
   547                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   548                             				if min_distance is None or new_distance < min_distance:
   549                             					min_distance = new_distance
   550                             					if min_distance ==0:
   551                             						print "witness ",w
   552                             			max_filtration_param = min_distance
   553                             
   554    623.2 MiB      0.0 MiB   		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   555    623.2 MiB      0.0 MiB   		progress_index = [0]*number_of_datapoints
   556    623.2 MiB      0.0 MiB   		done = False
   557    623.2 MiB      0.0 MiB   		epsilons = []
   558    623.4 MiB      0.2 MiB   		for q in xrange(num_divisions):
   559    623.4 MiB      0.0 MiB   			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   560    623.4 MiB      0.0 MiB   			print 'The threshold is currently %f' % threshold
   561    623.4 MiB      0.0 MiB   			epsilons.append(threshold)
   562    623.4 MiB      0.0 MiB   			for witness_index in xrange(number_of_datapoints):
   563    623.4 MiB      0.0 MiB   				add_simplex = False
   564    623.4 MiB      0.0 MiB   				progress = 0
   565    623.4 MiB      0.0 MiB   				while True:
   566    623.4 MiB      0.0 MiB   					progress = progress_index[witness_index]
   567    623.4 MiB      0.0 MiB   					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   568                             						break
   569    623.4 MiB      0.0 MiB   					if progress == number_of_vertices:
   570                             						done = True
   571                             						break
   572    623.4 MiB      0.0 MiB   					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   573    623.4 MiB      0.0 MiB   						if use_cliques:
   574                             							for i in xrange(progress):
   575                             								g.add_edge(d[witness_index][i].id_num, d[witness_index][progress].id_num)
   576                             						else:
   577    623.4 MiB      0.0 MiB   							if not store_top_simplices and progress > 0:
   578                             								for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], min(progress, dimension_cutoff)):
   579                             									new_subset = ImmutableSet(base + (d[witness_index][progress].id_num,))
   580                             									filtration.add(SimplexBirth(new_subset, q, sort_output))
   581    623.4 MiB      0.0 MiB   							add_simplex = True
   582    623.4 MiB      0.0 MiB   						progress_index[witness_index] += 1
   583                             					else:
   584    623.4 MiB      0.0 MiB   						break
   585    623.4 MiB      0.0 MiB   				if (not use_cliques) and store_top_simplices and add_simplex and progress >= 2:
   586    623.4 MiB      0.0 MiB   					list_o_landmarks = []
   587    623.4 MiB      0.0 MiB   					for landmark_index in xrange(progress):
   588    623.4 MiB      0.0 MiB   						list_o_landmarks.append(d[witness_index][landmark_index].id_num)
   589                             					#print 'At threshold %f, witness %d has landmark set %s' % (threshold, witness_index, str(list_o_landmarks))
   590    623.4 MiB      0.0 MiB   					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], q, sort_output))
   591    623.4 MiB      0.0 MiB   				if done:
   592                             					break
   593    623.4 MiB      0.0 MiB   			if use_cliques:
   594                             				filter_and_build()
   595    623.4 MiB      0.0 MiB   			if done:
   596                             				break
   597                             
   598                             
   599    623.4 MiB      0.0 MiB   	extra_data = (landmarks, witnesses)
   600    623.4 MiB      0.0 MiB   	if weak:
   601                             		max_epsilon = 0.0
   602                             		for w in xrange(number_of_datapoints):
   603                             			#print("type: %s" % type(d[w][max_filtration_param - 1].distance))
   604                             			#print("value: %f" % d[w][max_filtration_param - 1].distance)
   605                             			if (d[w][max_filtration_param - 1].distance) > max_epsilon:
   606                             				max_epsilon = d[w][max_filtration_param - 1].distance
   607                             		print("Done. Filtration contains %i top simplex birth events, with the largest epsilon equal to %f.\n" % (len(filtration), max_epsilon))
   608                             	else:
   609    623.4 MiB      0.0 MiB   		max_sb_length = 0
   610    623.4 MiB      0.0 MiB   		for sb in filtration:
   611    623.4 MiB      0.0 MiB   			if len(sb.landmark_set) > max_sb_length:
   612    623.4 MiB      0.0 MiB   				max_sb_length = len(sb.landmark_set)
   613    623.4 MiB      0.0 MiB   		print("Done. Filtration contains %i top simplex birth events, with the largest one comprised of %i landmarks.\nMax filtration parameter: %s.\n" % (len(filtration), max_sb_length, max_filtration_param))
   614                             
   615                             	## Write to output file
   616    623.4 MiB      0.0 MiB   	output_file_name = get_param("out")
   617                             
   618    623.4 MiB      0.0 MiB   	if not output_file_name is None:
   619                             		output_file = open(output_file_name, "w")
   620                             		output_file.truncate()
   621                             		program = get_param("program")
   622                             		if dimension_cutoff is None:
   623                             			print("Writing filtration for input into %s..." % program)
   624                             			dimension_cutoff = number_of_vertices
   625                             		else:
   626                             			print("Writing filtration to file %s for input into %s, ignoring simplices above dimension %i..." % (output_file_name,program, dimension_cutoff))
   627                             		num_lines = 0
   628                             		if program == "Perseus":
   629                             			sets_printed_so_far = Set()
   630                             			num_lines = len(filtration) + 1
   631                             			output_file.write("1\n")
   632                             			list_filtration = None
   633                             			if (sort_output):
   634                             				list_filtration = list(filtration)
   635                             				list_filtration.sort()
   636                             			for simplex_birth in (list_filtration if sort_output else filtration):
   637                             				dimension = len(simplex_birth.landmark_set) - 1
   638                             				if dimension > dimension_cutoff:
   639                             					for subtuple in itertools.combinations(simplex_birth.landmark_set, dimension_cutoff + 1):
   640                             						subset = ImmutableSet(subtuple)
   641                             						if not ((subset, simplex_birth.birth_time) in sets_printed_so_far):
   642                             							output_file.write(str(dimension_cutoff) + " ")
   643                             							for landmark in subset:
   644                             								output_file.write(str(landmark + 1) + " ")
   645                             							output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   646                             							sets_printed_so_far.add((subset, simplex_birth.birth_time))
   647                             				else:
   648                             					if not ((simplex_birth.landmark_set, simplex_birth.birth_time) in sets_printed_so_far):
   649                             						output_file.write(str(dimension) + " ")
   650                             						for landmark in (simplex_birth.sll if sort_output else simplex_birth.landmark_set):
   651                             							output_file.write(str(landmark + 1) + " ")
   652                             						output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   653                             						sets_printed_so_far.add((simplex_birth.landmark_set, simplex_birth.birth_time))
   654                             		elif program == "PHAT":
   655                             			line_map = {}
   656                             			for i in xrange(number_of_vertices - 1):
   657                             				output_file.write("0\n")
   658                             				line_map[ImmutableSet([i])] = i
   659                             			output_file.write("0")
   660                             			line_map[ImmutableSet([number_of_vertices - 1])] = number_of_vertices - 1
   661                             			simultaneous_additions = []
   662                             			class Context: # Note: if upgrading to Python 3, one could just use the nonlocal keyword (see below comment).
   663                             				line_number = number_of_vertices
   664                             			list_filtration = list(filtration)
   665                             			list_filtration.sort()
   666                             			last_birth_time = 0
   667                             			def process_and_get_line_number(s):
   668                             				#nonlocal line_number
   669                             				if s in line_map:
   670                             					return line_map[s]
   671                             				else:
   672                             					dimension = len(s) - 1
   673                             					if dimension > dimension_cutoff:
   674                             						for subset in itertools.combinations(s, dimension_cutoff + 1): # Take all subsets of size dimension_cutoff + 1
   675                             							process_and_get_line_number(ImmutableSet(subset))
   676                             					elif dimension > 0:
   677                             						subsets_line_numbers = []
   678                             						for e in s:
   679                             							subsets_line_numbers.append(process_and_get_line_number(ImmutableSet(s - Set([e]))))
   680                             						output_file.write("\n" + str(dimension))
   681                             						for l in subsets_line_numbers:
   682                             							output_file.write(" " + str(l))
   683                             						line_map[s] = Context.line_number
   684                             						Context.line_number += 1
   685                             						return Context.line_number - 1
   686                             					else:
   687                             						raise Exception("Should have already added single point for base case: " + str(s))
   688                             			for simplex_birth in list_filtration:
   689                             				if simplex_birth.birth_time > last_birth_time:
   690                             					simultaneous_additions.append((Context.line_number - 1, last_birth_time + 1)) # Every line up to and including that line number (indexing starts at 0) had that birth time or earlier (indexing starts at 1)
   691                             					last_birth_time = simplex_birth.birth_time
   692                             				process_and_get_line_number(simplex_birth.landmark_set)
   693                             			simultaneous_additions.append((sys.maxsize, last_birth_time))
   694                             			output_file.write("\n\n# Simultaneous additions: Every line up to and including __ (indexing starts at 0) has birth time __ (or earlier).")
   695                             			for addition in simultaneous_additions:
   696                             				output_file.write("\n# %20i %20i" % addition)
   697                             			extra_data = (extra_data[0], extra_data[1], simultaneous_additions)
   698                             			num_lines = Context.line_number
   699                             		else:
   700                             			raise Exception("Only supported programs are 'Perseus' and 'PHAT'")
   701                             		output_file.close()
   702                             		print("Done. File contains %i lines.\n" % num_lines)
   703    623.4 MiB      0.0 MiB   	print("Filtration has been successfully built!\n")
   704    623.4 MiB      0.0 MiB   	return (filtration, extra_data + (max_filtration_param,), epsilons)


Filename: /home/elliott/programming/PHETS/PH/BuildFiltration.py

Line #    Mem usage    Increment   Line Contents
================================================
    37    709.1 MiB      0.0 MiB   @profile(stream=f)
    38                             def build_filtration(input_file_name, parameter_set, silent=False):
    39    709.1 MiB      0.0 MiB   	num_threads = 2
    40                             	global d
    41    685.6 MiB    -23.5 MiB   	d = []
    42                             
    43                             
    44    708.3 MiB     22.7 MiB   	def get_param(key):
    45    708.3 MiB      0.0 MiB   		return parameter_set.get(key)
    46                             
    47    685.6 MiB    -22.7 MiB   	input_file = open(input_file_name)
    48    685.6 MiB      0.0 MiB   	speed_amplify = float(get_param("d_speed_amplify"))
    49    685.6 MiB      0.0 MiB   	orientation_amplify = float(get_param("d_orientation_amplify"))
    50    685.6 MiB      0.0 MiB   	stretch = float(get_param("d_stretch"))
    51    685.6 MiB      0.0 MiB   	ray_distance_amplify = get_param("d_ray_distance_amplify")
    52    685.6 MiB      0.0 MiB   	use_hamiltonian = get_param("d_use_hamiltonian")
    53    685.6 MiB      0.0 MiB   	print "use hamiltonian set to ", use_hamiltonian
    54    685.6 MiB      0.0 MiB   	m2_d = float(get_param("m2_d"))
    55    685.6 MiB      0.0 MiB   	straight_VB = float(get_param("straight_VB"))
    56    685.6 MiB      0.0 MiB   	d_cov = get_param("d_cov")
    57    685.6 MiB      0.0 MiB   	graph_induced = get_param("graph_induced")
    58                             	# always_euclidean = speed_amplify == orientation_amplify == stretch == ray_distance_amplify == 1.0 and use_hamiltonian == d_cov==0.
    59                             	# print "always_euclidean set to ", always_euclidean
    60    685.6 MiB      0.0 MiB   	always_euclidean = get_param('always_euclidean')
    61                             	# use_hamiltonian = -5
    62                             
    63    685.6 MiB      0.0 MiB   	filtration = Set()
    64    685.6 MiB      0.0 MiB   	extra_data = None
    65    685.6 MiB      0.0 MiB   	min_filtration_param = float(get_param("min_filtration_param"))
    66    685.6 MiB      0.0 MiB   	max_filtration_param = float(get_param("max_filtration_param"))
    67    685.6 MiB      0.0 MiB   	print "Max filtration parameter is ", max_filtration_param
    68    685.6 MiB      0.0 MiB   	if max_filtration_param < 0 and min_filtration_param != 0:
    69                             		raise Exception("Argument 'min_filtration_param' is incompatible with automatic max_filtration_param selection.")
    70    685.6 MiB      0.0 MiB   	number_of_vertices = 0
    71    685.6 MiB      0.0 MiB   	start = get_param("start")
    72    685.6 MiB      0.0 MiB   	worm_length = get_param("worm_length")
    73    685.6 MiB      0.0 MiB   	store_top_simplices = get_param("store_top_simplices")
    74    685.6 MiB      0.0 MiB   	sort_output = get_param("sort_output")
    75    685.6 MiB      0.0 MiB   	absolute = get_param("absolute")
    76    685.6 MiB      0.0 MiB   	num_divisions = get_param("num_divisions")
    77    685.6 MiB      0.0 MiB   	simplex_cutoff = get_param("simplex_cutoff")
    78                             
    79                             
    80                             	'''=============== This code written by Sam ======================'''
    81                             
    82                             	## Read data into witness and landmark lists.
    83    685.6 MiB      0.0 MiB   	witnesses = []
    84    685.6 MiB      0.0 MiB   	landmarks = []
    85    685.6 MiB      0.0 MiB   	landmark_indices = []
    86    685.6 MiB      0.0 MiB   	ls = get_param("landmark_selector")
    87    685.6 MiB      0.0 MiB   	downsample_rate = get_param("ds_rate")
    88    685.6 MiB      0.0 MiB   	maxmin = False
    89    685.6 MiB      0.0 MiB   	counter = 0
    90    685.6 MiB      0.0 MiB   	for i in xrange(start):           #  Where to start reading data
    91                             		input_file.readline()
    92                             		counter+=1
    93    685.6 MiB      0.0 MiB   	landmark_indices=[]
    94                             
    95                             
    96    904.9 MiB    219.3 MiB   	for line in input_file.read().split("\n"):
    97    904.9 MiB      0.0 MiB   		if line != "" and counter>=start:
    98    904.9 MiB      0.0 MiB   			string_witness = line.split(" ")
    99    904.9 MiB      0.0 MiB   			witness = []
   100    904.9 MiB      0.0 MiB   			d.append([])
   101    904.9 MiB      0.0 MiB   			for coordinate in string_witness:
   102    904.9 MiB      0.0 MiB   				if coordinate != "":
   103    904.9 MiB      0.0 MiB   					witness.append(float(coordinate))
   104    904.9 MiB      0.0 MiB   			witnesses.append(witness)
   105    904.9 MiB      0.0 MiB   			counter += 1
   106    904.9 MiB      0.0 MiB   			if counter == worm_length:
   107    685.8 MiB   -219.1 MiB   				break
   108                             
   109    685.8 MiB      0.0 MiB   	number_of_datapoints = len(witnesses)
   110    685.8 MiB      0.0 MiB   	number_of_vertices = int(number_of_datapoints/downsample_rate)
   111    685.8 MiB      0.0 MiB   	num_coordinates = len(witnesses[0])
   112    685.8 MiB      0.0 MiB   	stop = start + counter
   113                             
   114    685.8 MiB      0.0 MiB   	if max_filtration_param < 0:
   115                             		if float(number_of_vertices) < abs(max_filtration_param) + 1:
   116                             			print '''ERROR: 'max_filtration_param' ({}) and number of landmarks ({})
   117                             			are incompatible. Try decreasing 'ds_rate' or increasing 'worm_length'.'''.format(max_filtration_param,
   118                             			number_of_vertices)
   119                             			sys.exit()
   120                             
   121    685.8 MiB      0.0 MiB   	num_threads = 2
   122                             	# for more information about these parameters type ./find_landmarks --help in the terminal
   123                             	# the distance calculations are calculated and outputted to a file called find_landmarks.txt
   124    685.8 MiB      0.0 MiB   	print os.getcwd()
   125                             
   126    685.8 MiB      0.0 MiB   	if ls=="EST":
   127                             		if always_euclidean:
   128                             			if graph_induced:
   129                             				find_landmarks_cmd = [
   130                             					"./find_landmarks",
   131                             					"-n {}".format(num_threads),
   132                             					"-l {}".format(number_of_vertices),
   133                             					"-w {}-{}".format(start,stop),
   134                             					"-i{}".format(input_file_name),
   135                             					"-olandmark_outputs.txt",
   136                             					"-m {}".format(int(m2_d)),
   137                             					"-a {}".format(speed_amplify),
   138                             					"-y {}".format(orientation_amplify),
   139                             					"-h {}".format(use_hamiltonian),
   140                             					"-r {}".format(ray_distance_amplify),
   141                             					"-v {}".format(straight_VB),
   142                             					"-s {}".format(stretch),
   143                             					"-e {}".format(downsample_rate),
   144                             					"-x {}".format(d_cov),
   145                             					"-c",
   146                             					"-f {}".format(max_filtration_param)
   147                             				]
   148                             			else:
   149                             				find_landmarks_cmd = [
   150                             					"./find_landmarks",
   151                             					"-n {}".format(num_threads),
   152                             					"-l {}".format(number_of_vertices),
   153                             					"-w {}-{}".format(start,stop),
   154                             					"-i{}".format(input_file_name),
   155                             					"-olandmark_outputs.txt",
   156                             					"-m {}".format(int(m2_d)),
   157                             					"-a {}".format(speed_amplify),
   158                             					"-y {}".format(orientation_amplify),
   159                             					"-h {}".format(use_hamiltonian),
   160                             					"-r {}".format(ray_distance_amplify),
   161                             					"-v {}".format(straight_VB),
   162                             					"-s {}".format(stretch),
   163                             					"-e {}".format(downsample_rate),
   164                             					"-x {}".format(d_cov),
   165                             					"-c"
   166                             				]
   167                             		else:
   168                             			if graph_induced:
   169                             				find_landmarks_cmd = [
   170                             					"./find_landmarks",
   171                             					"-n {}".format(num_threads),
   172                             					"-l {}".format(number_of_vertices),
   173                             					"-w {}-{}".format(start,stop),
   174                             					"-i{}".format(input_file_name),
   175                             					"-olandmark_outputs.txt",
   176                             					"-m {}".format(int(m2_d)),
   177                             					"-a {}".format(speed_amplify),
   178                             					"-y {}".format(orientation_amplify),
   179                             					"-h {}".format(use_hamiltonian),
   180                             					"-r {}".format(ray_distance_amplify),
   181                             					"-v {}".format(straight_VB),
   182                             					"-s {}".format(stretch),
   183                             					"-x {}".format(d_cov),
   184                             					"-e {}".format(downsample_rate),
   185                             					"-f {}".format(max_filtration_param)
   186                             				]
   187                             			else:
   188                             				find_landmarks_cmd = [
   189                             					"./find_landmarks",
   190                             					"-n {}".format(num_threads),
   191                             					"-l {}".format(number_of_vertices),
   192                             					"-w {}-{}".format(start,stop),
   193                             					"-i{}".format(input_file_name),
   194                             					"-olandmark_outputs.txt",
   195                             					"-m {}".format(int(m2_d)),
   196                             					"-a {}".format(speed_amplify),
   197                             					"-y {}".format(orientation_amplify),
   198                             					"-h {}".format(use_hamiltonian),
   199                             					"-r {}".format(ray_distance_amplify),
   200                             					"-v {}".format(straight_VB),
   201                             					"-s {}".format(stretch),
   202                             					"-x {}".format(d_cov),
   203                             					"-e {}".format(downsample_rate)
   204                             				]
   205                             	else:
   206    685.8 MiB      0.0 MiB   		if always_euclidean and m2_d!=0:
   207                             			if graph_induced:
   208                             				find_landmarks_cmd = [
   209                             					"./find_landmarks",
   210                             					"-n {}".format(num_threads),
   211                             					"-l {}".format(number_of_vertices),
   212                             					"-w {}-{}".format(start,stop),
   213                             					"-i{}".format(input_file_name),
   214                             					"-olandmark_outputs.txt",
   215                             					"-m {}".format(int(m2_d)),
   216                             					"-a {}".format(speed_amplify),
   217                             					"-y {}".format(orientation_amplify),
   218                             					"-h {}".format(use_hamiltonian),
   219                             					"-r {}".format(ray_distance_amplify),
   220                             					"-v {}".format(straight_VB),
   221                             					"-x {}".format(d_cov),
   222                             					"-s {}".format(stretch),
   223                             					"-f {}".format(max_filtration_param)
   224                             				]
   225                             			else:
   226                             				find_landmarks_cmd = [
   227                             					"./find_landmarks",
   228                             					"-n {}".format(num_threads),
   229                             					"-l {}".format(number_of_vertices),
   230                             					"-w {}-{}".format(start,stop),
   231                             					"-i{}".format(input_file_name),
   232                             					"-olandmark_outputs.txt",
   233                             					"-m {}".format(int(m2_d)),
   234                             					"-a {}".format(speed_amplify),
   235                             					"-y {}".format(orientation_amplify),
   236                             					"-h {}".format(use_hamiltonian),
   237                             					"-r {}".format(ray_distance_amplify),
   238                             					"-v {}".format(straight_VB),
   239                             					"-x {}".format(d_cov),
   240                             					"-s {}".format(stretch)
   241                             				]
   242    685.8 MiB      0.0 MiB   		elif always_euclidean:
   243                             			if graph_induced:
   244                             				find_landmarks_cmd = [
   245                             					"./find_landmarks",
   246                             					"-n {}".format(num_threads),
   247                             					"-l {}".format(number_of_vertices),
   248                             					"-w {}-{}".format(start,stop),
   249                             					"-i{}".format(input_file_name),
   250                             					"-olandmark_outputs.txt",
   251                             					"-m {}".format(int(m2_d)),
   252                             					"-a {}".format(speed_amplify),
   253                             					"-y {}".format(orientation_amplify),
   254                             					"-h {}".format(use_hamiltonian),
   255                             					"-r {}".format(ray_distance_amplify),
   256                             					"-v {}".format(straight_VB),
   257                             					"-x {}".format(d_cov),
   258                             					"-s {}".format(stretch),
   259                             					"-c",
   260                             					"-f {}".format(max_filtration_param)
   261                             				]
   262                             			else:
   263                             				find_landmarks_cmd = [
   264                             					"./find_landmarks",
   265                             					"-n {}".format(num_threads),
   266                             					"-l {}".format(number_of_vertices),
   267                             					"-w {}-{}".format(start,stop),
   268                             					"-i{}".format(input_file_name),
   269                             					"-olandmark_outputs.txt",
   270                             					"-m {}".format(int(m2_d)),
   271                             					"-a {}".format(speed_amplify),
   272                             					"-y {}".format(orientation_amplify),
   273                             					"-h {}".format(use_hamiltonian),
   274                             					"-r {}".format(ray_distance_amplify),
   275                             					"-v {}".format(straight_VB),
   276                             					"-x {}".format(d_cov),
   277                             					"-s {}".format(stretch),
   278                             					"-c"
   279                             				]
   280                             		else:
   281    685.8 MiB      0.0 MiB   			if graph_induced:
   282                             				find_landmarks_cmd = [
   283                             					"./find_landmarks",
   284                             					"-n {}".format(num_threads),
   285                             					"-l {}".format(number_of_vertices),
   286                             					"-w {}-{}".format(start,stop),
   287                             					"-i{}".format(input_file_name),
   288                             					"-olandmark_outputs.txt",
   289                             					"-m {}".format(int(m2_d)),
   290                             					"-a {}".format(speed_amplify),
   291                             					"-y {}".format(orientation_amplify),
   292                             					"-h {}".format(use_hamiltonian),
   293                             					"-r {}".format(ray_distance_amplify),
   294                             					"-v {}".format(straight_VB),
   295                             					"-x {}".format(d_cov),
   296                             					"-s {}".format(stretch),
   297                             					"-f {}".format(max_filtration_param)
   298                             				]
   299                             			else:
   300                             				find_landmarks_cmd = [
   301    685.8 MiB      0.0 MiB   					"./find_landmarks",
   302    685.8 MiB      0.0 MiB   					"-n {}".format(num_threads),
   303    685.8 MiB      0.0 MiB   					"-l {}".format(number_of_vertices),
   304    685.8 MiB      0.0 MiB   					"-w {}-{}".format(start,stop),
   305    685.8 MiB      0.0 MiB   					"-i{}".format(input_file_name),
   306    685.8 MiB      0.0 MiB   					"-olandmark_outputs.txt",
   307    685.8 MiB      0.0 MiB   					"-m {}".format(int(m2_d)),
   308    685.8 MiB      0.0 MiB   					"-a {}".format(speed_amplify),
   309    685.8 MiB      0.0 MiB   					"-y {}".format(orientation_amplify),
   310    685.8 MiB      0.0 MiB   					"-h {}".format(use_hamiltonian),
   311    685.8 MiB      0.0 MiB   					"-r {}".format(ray_distance_amplify),
   312    685.8 MiB      0.0 MiB   					"-v {}".format(straight_VB),
   313    685.8 MiB      0.0 MiB   					"-x {}".format(d_cov),
   314    685.8 MiB      0.0 MiB   					"-s {}".format(stretch)
   315                             				]
   316                             
   317    685.8 MiB      0.0 MiB   	print find_landmarks_cmd
   318                             
   319    685.8 MiB      0.0 MiB   	if silent:
   320                             		p = subprocess.Popen(find_landmarks_cmd, stdout=subprocess.PIPE)
   321                             		out, err = p.communicate()
   322                             	else:
   323    685.8 MiB      0.0 MiB   		p = subprocess.Popen(find_landmarks_cmd)
   324    685.8 MiB      0.0 MiB   		p.communicate()
   325                             
   326                             	## Build and sort distance matrix.
   327    685.8 MiB      0.0 MiB   	landmarks_file = open("landmark_outputs.txt","rb")
   328                             
   329    685.8 MiB      0.0 MiB   	l = landmarks_file.readlines()
   330    685.8 MiB      0.0 MiB   	sys.stdout.write("Reading in distance calculations...")
   331    685.8 MiB      0.0 MiB   	sys.stdout.flush()
   332    685.8 MiB      0.0 MiB   	landmark_index = 0
   333    708.3 MiB     22.5 MiB   	for line in l:
   334    707.5 MiB     -0.8 MiB   		f = line.strip('\n')
   335    707.5 MiB      0.0 MiB   		if "#" not in f:
   336    707.5 MiB      0.0 MiB   			landmark = int(f.split(":")[0])
   337                             
   338    707.5 MiB      0.0 MiB   			distances = [float(i) for i in f.split(":")[1].split(",")]
   339    708.3 MiB      0.8 MiB   			for witness_index in range(0,len(distances)):
   340                             
   341    708.3 MiB      0.0 MiB   				d[witness_index].append(LandmarkDistance(landmark_index,distances[witness_index]))
   342    708.3 MiB      0.0 MiB   			landmarks.append(witnesses[landmark])
   343    708.3 MiB      0.0 MiB   			landmark_indices.append(landmark)
   344    708.3 MiB      0.0 MiB   			landmark_index+=1
   345                             
   346    708.3 MiB      0.0 MiB   	assert(len(d)>0)
   347    708.3 MiB      0.0 MiB   	sys.stdout.write("done\n")
   348    708.3 MiB      0.0 MiB   	sys.stdout.flush()
   349                             
   350                             
   351                             
   352    708.3 MiB      0.0 MiB   	sys.stdout.write("Sorting distances...")
   353    708.3 MiB      0.0 MiB   	sys.stdout.flush()
   354                             
   355                             	# p=multiprocessing.Pool(processes=4)   # commented out by Elliott 4/25
   356                             
   357    708.3 MiB      0.0 MiB   	inputs=[]
   358    708.3 MiB      0.0 MiB   	for w in range(0,len(witnesses)):
   359    708.3 MiB      0.0 MiB   		inputs.append(w)
   360    708.3 MiB      0.0 MiB   		d[w].sort()
   361                             
   362                             	# p.map(sort,inputs)  # was commented out as of 4/25
   363                             	# p.terminate()       # added by Elliott 4/25
   364                             
   365    708.3 MiB      0.0 MiB   	sys.stdout.write("done\n")
   366    708.3 MiB      0.0 MiB   	sys.stdout.flush()
   367    708.3 MiB      0.0 MiB   	assert len(landmarks) == number_of_vertices
   368                             
   369                             	'''=============== End code written by Sam ======================'''
   370                             
   371                             	'''============= Start code written by Elliott =================='''
   372    708.3 MiB      0.0 MiB   	if graph_induced:
   373                             		# import matplotlib.pyplot as plt
   374                             		import pandas as pd
   375                             
   376                             		g = nx.read_edgelist('edgelist.txt')
   377                             
   378                             		closest_wits = np.loadtxt('closest_wits.txt', dtype='int')	# witness, landmark
   379                             		wit_coords = np.array(witnesses)
   380                             		land_coords = np.array(landmarks)
   381                             
   382                             		# land = np.unique(closest_wits[:,1])
   383                             
   384                             		# closest_wits = pd.DataFrame(closest_wits, columns=('witness', 'landmark'))
   385                             		# print closest_wits
   386                             		# closest_wits = closest_wits.sort_values(by='landmark')
   387                             		# print closest_wits
   388                             		#
   389                             		# closest_wits = closest_wits.values
   390                             		# print closest_wits
   391                             
   392                             
   393                             
   394                             		# fig = plt.figure(figsize=(8, 8))
   395                             		# ax = fig.add_subplot(111)
   396                             		# ax.scatter(wit_coords[:, 0], wit_coords[:, 1], s=.1)
   397                             		# ax.scatter(land_coords[:, 0], land_coords[:, 1])
   398                             		# fig.savefig('veronoi_test.png')
   399                             
   400                             	'''=============== End code written by Elliott =================='''
   401                             
   402                             
   403                             
   404                             
   405    708.3 MiB      0.0 MiB   	print("Building filtration...")
   406                             	## Build filtration
   407    708.3 MiB      0.0 MiB   	weak = get_param("weak")
   408    708.3 MiB      0.0 MiB   	dimension_cutoff = get_param("dimension_cutoff")
   409    708.3 MiB      0.0 MiB   	reentry_filter = get_param("reentry_filter")
   410    708.3 MiB      0.0 MiB   	if get_param("connect_time_1_skeleton") or reentry_filter: # Connect time-1-skeleton
   411                             		for i in xrange(number_of_vertices - 1):
   412                             			filtration.add(SimplexBirth(ImmutableSet([i, i + 1]), 0, sort_output))
   413    708.3 MiB      0.0 MiB   	use_cliques = get_param("use_cliques")
   414    708.3 MiB      0.0 MiB   	use_twr = get_param("use_twr")
   415                             
   416    708.3 MiB      0.0 MiB   	print '%s' % use_twr
   417    708.3 MiB      0.0 MiB   	if use_cliques: # AKA "Lazy" witness relation.
   418                             		g = nx.Graph()
   419                             		for l in xrange(number_of_vertices):
   420                             			g.add_node(l)
   421    708.3 MiB      0.0 MiB   	def filter_and_build():
   422                             		g2 = None
   423                             		if reentry_filter:
   424                             			g2 = g.copy()
   425                             			to_remove = Set()
   426                             			for l1 in xrange(number_of_vertices):
   427                             				l2 = l1 + 2
   428                             				while l2 < number_of_vertices and g2.has_edge(l1, l2):
   429                             					to_remove.add(ImmutableSet([l1, l2]))
   430                             					l2 += 1
   431                             			for edge in to_remove:
   432                             				g2.remove_edge(*tuple(edge)) # May cause weird things to happen because removing edges doesn't remove them from the filtration.
   433                             		else:
   434                             			g2 = g
   435                             		for clique in nx.find_cliques(g2):
   436                             			filtration.add(SimplexBirth(clique, q, sort_output))
   437    708.3 MiB      0.0 MiB   	if weak: # Builds filtration based on k nearest neighbors.
   438                             		if max_filtration_param % 1 != 0:
   439                             			raise Exception("Argument 'max_filtration_param' must be an integer if using the weak witness relation.")
   440                             		max_filtration_param = int(max_filtration_param)
   441                             		for k in xrange(int(math.fabs(max_filtration_param))):
   442                             			for witness_index in xrange(number_of_datapoints):
   443                             				if use_cliques:
   444                             					for i in xrange(k):
   445                             						g.add_edge(d[witness_index][i].id_num, d[witness_index][k].id_num)
   446                             				elif store_top_simplices:
   447                             					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(k + 1)], k, sort_output))
   448                             				else:
   449                             					if progress > 0:
   450                             						for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(k)], min(k, dimension_cutoff)):
   451                             							new_subset = ImmutableSet(base + (d[witness_index][k].id_num,))
   452                             							filtration.add(SimplexBirth(new_subset, k, sort_output))
   453                             			if use_cliques:
   454                             				filter_and_build()
   455    708.3 MiB      0.0 MiB   	if use_twr:
   456                             		print 'Using TWR'
   457                             		if max_filtration_param < 0: # Automatically determine max.
   458                             			depth = int(-max_filtration_param)
   459                             			min_distance = None
   460                             			for w in xrange(number_of_datapoints):
   461                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   462                             				if min_distance is None or new_distance < min_distance:
   463                             					min_distance = new_distance
   464                             			max_filtration_param = min_distance
   465                             		print 'The max_filtration_param is %d ' % max_filtration_param
   466                             		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   467                             		print 'The step size is %f ' % step
   468                             		print 'There will be %d steps in the filtration' % num_divisions
   469                             		progress_index = [0]*number_of_datapoints
   470                             		done = False
   471                             
   472                             		good_landmarks = [[] for x in range(number_of_datapoints)]
   473                             
   474                             		epsilons = []
   475                             		for q in xrange(num_divisions):
   476                             			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   477                             			print 'The threshold is currently %f' % threshold
   478                             			epsilons.append(threshold)
   479                             			Pre_landmarks = []
   480                             			for witness_index in xrange(number_of_datapoints):
   481                             				pre_landmarks = []
   482                             				add_simplex = False
   483                             				progress = 0
   484                             				while True:
   485                             					progress = progress_index[witness_index]
   486                             					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   487                             						break
   488                             					if progress == number_of_vertices:
   489                             						done = True
   490                             						break
   491                             
   492                             					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   493                             						pre_landmarks.append(d[witness_index][progress].id_num) # PRE_LANDMARKS CONTAINS ID NUMBER
   494                             						progress_index[witness_index] += 1
   495                             					else:
   496                             
   497                             						pre_landmarks_size = len(pre_landmarks)
   498                             						pre_landmarks_string = str(pre_landmarks) # MAKE LIST TO STRING
   499                             						print 'At threshold value %f, witness %d has %d associated landmarks: %s ' % (threshold, witness_index, pre_landmarks_size, pre_landmarks_string)
   500                             						break
   501                             				Pre_landmarks.append(pre_landmarks)
   502                             				Pre_landmarks_size = len(Pre_landmarks)
   503                             
   504                             
   505                             
   506                             			for witness_index in xrange(number_of_datapoints - downsample_rate):
   507                             				if len(Pre_landmarks[witness_index]) == 1:
   508                             					set_range = 1
   509                             				else:
   510                             					set_range = len(Pre_landmarks[witness_index])
   511                             				for k in range(set_range):
   512                             					current_pre_landmark = Pre_landmarks[witness_index][k]
   513                             					next_pre_landmark = Pre_landmarks[witness_index][k]+1 # CHECKS ONE STEP UP FROM ID NUMBER *** SEE JAMIE'S COMMENTS ON SUCH OR CLARIFY ***
   514                             					#	print 'current pre landmark = %d , next pre landmark = %d' % (current_pre_landmark, next_pre_landmark)
   515                             					check_pre_landmark = str(Pre_landmarks[witness_index + downsample_rate]) # HMMMMM
   516                             					#		print 'We are considering the fate of landmark %d witnessed by witness %d...' % (current_pre_landmark, witness_index)
   517                             					#		print 'Should witness %d not witness landmark %d, it will be GONE!' % (witness_index + downsample_rate, current_pre_landmark + 1,)
   518                             					#		print 'Witness %d has landmark set %s' % (witness_index + downsample_rate, check_pre_landmark)
   519                             					print (Pre_landmarks[witness_index][k]) in Pre_landmarks[witness_index]
   520                             					if (Pre_landmarks[witness_index][k]+ 1) in Pre_landmarks[witness_index + downsample_rate]: # change from 1, downsample_rate to 0 to test!
   521                             						good_landmarks[witness_index].append(Pre_landmarks[witness_index][k])
   522                             				print 'Up to threshold value %f, witness %d has landmark set %s' % (threshold, witness_index, str(good_landmarks[witness_index]))
   523                             				if use_cliques:
   524                             					for i in xrange(len(good_landmarks[witness_index])):
   525                             						for j in xrange(i+1,len(good_landmarks[witness_index])):
   526                             							g.add_edge(good_landmarks[witness_index][i], good_landmarks[witness_index][j])
   527                             				else:
   528                             					if not store_top_simplices and len(good_landmarks[witness_index]) > 0:
   529                             						for base in itertools.combinations(good_landmarks[witness_index], min(len(good_landmarks[witness_index]), dimension_cutoff)):
   530                             							new_subset = ImmutableSet(base + (good_landmarks[witness_index][i],))
   531                             							filtration.add(SimplexBirth(new_subset, q, sort_output))
   532                             					add_simplex = True
   533                             				if (not use_cliques) and store_top_simplices and add_simplex and len(good_landmarks[witness_index])>= 2:
   534                             					filtration.add(SimplexBirth([good_landmarks[witness_index][i] for i in xrange(len(good_landmarks[witness_index]))], q, sort_output))
   535                             				if done:
   536                             					break
   537                             			if use_cliques:
   538                             				filter_and_build()
   539                             			if done:
   540                             				break
   541                             			#	print 'We are done with threshold %f' % threshold
   542                             	else:
   543    708.3 MiB      0.0 MiB   		if max_filtration_param < 0: # Automatically determine max.
   544                             			depth = int(-max_filtration_param)
   545                             			min_distance = None
   546                             			for w in xrange(number_of_datapoints):
   547                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   548                             				if min_distance is None or new_distance < min_distance:
   549                             					min_distance = new_distance
   550                             					if min_distance ==0:
   551                             						print "witness ",w
   552                             			max_filtration_param = min_distance
   553                             
   554    708.3 MiB      0.0 MiB   		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   555    708.3 MiB      0.0 MiB   		progress_index = [0]*number_of_datapoints
   556    708.3 MiB      0.0 MiB   		done = False
   557    708.3 MiB      0.0 MiB   		epsilons = []
   558    708.3 MiB      0.0 MiB   		for q in xrange(num_divisions):
   559    708.3 MiB      0.0 MiB   			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   560    708.3 MiB      0.0 MiB   			print 'The threshold is currently %f' % threshold
   561    708.3 MiB      0.0 MiB   			epsilons.append(threshold)
   562    708.3 MiB      0.0 MiB   			for witness_index in xrange(number_of_datapoints):
   563    708.3 MiB      0.0 MiB   				add_simplex = False
   564    708.3 MiB      0.0 MiB   				progress = 0
   565    708.3 MiB      0.0 MiB   				while True:
   566    708.3 MiB      0.0 MiB   					progress = progress_index[witness_index]
   567    708.3 MiB      0.0 MiB   					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   568                             						break
   569    708.3 MiB      0.0 MiB   					if progress == number_of_vertices:
   570                             						done = True
   571                             						break
   572    708.3 MiB      0.0 MiB   					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   573    708.3 MiB      0.0 MiB   						if use_cliques:
   574                             							for i in xrange(progress):
   575                             								g.add_edge(d[witness_index][i].id_num, d[witness_index][progress].id_num)
   576                             						else:
   577    708.3 MiB      0.0 MiB   							if not store_top_simplices and progress > 0:
   578                             								for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], min(progress, dimension_cutoff)):
   579                             									new_subset = ImmutableSet(base + (d[witness_index][progress].id_num,))
   580                             									filtration.add(SimplexBirth(new_subset, q, sort_output))
   581    708.3 MiB      0.0 MiB   							add_simplex = True
   582    708.3 MiB      0.0 MiB   						progress_index[witness_index] += 1
   583                             					else:
   584    708.3 MiB      0.0 MiB   						break
   585    708.3 MiB      0.0 MiB   				if (not use_cliques) and store_top_simplices and add_simplex and progress >= 2:
   586    708.3 MiB      0.0 MiB   					list_o_landmarks = []
   587    708.3 MiB      0.0 MiB   					for landmark_index in xrange(progress):
   588    708.3 MiB      0.0 MiB   						list_o_landmarks.append(d[witness_index][landmark_index].id_num)
   589                             					#print 'At threshold %f, witness %d has landmark set %s' % (threshold, witness_index, str(list_o_landmarks))
   590    708.3 MiB      0.0 MiB   					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], q, sort_output))
   591    708.3 MiB      0.0 MiB   				if done:
   592                             					break
   593    708.3 MiB      0.0 MiB   			if use_cliques:
   594                             				filter_and_build()
   595    708.3 MiB      0.0 MiB   			if done:
   596                             				break
   597                             
   598                             
   599    708.3 MiB      0.0 MiB   	extra_data = (landmarks, witnesses)
   600    708.3 MiB      0.0 MiB   	if weak:
   601                             		max_epsilon = 0.0
   602                             		for w in xrange(number_of_datapoints):
   603                             			#print("type: %s" % type(d[w][max_filtration_param - 1].distance))
   604                             			#print("value: %f" % d[w][max_filtration_param - 1].distance)
   605                             			if (d[w][max_filtration_param - 1].distance) > max_epsilon:
   606                             				max_epsilon = d[w][max_filtration_param - 1].distance
   607                             		print("Done. Filtration contains %i top simplex birth events, with the largest epsilon equal to %f.\n" % (len(filtration), max_epsilon))
   608                             	else:
   609    708.3 MiB      0.0 MiB   		max_sb_length = 0
   610    708.3 MiB      0.0 MiB   		for sb in filtration:
   611    708.3 MiB      0.0 MiB   			if len(sb.landmark_set) > max_sb_length:
   612    708.3 MiB      0.0 MiB   				max_sb_length = len(sb.landmark_set)
   613    708.3 MiB      0.0 MiB   		print("Done. Filtration contains %i top simplex birth events, with the largest one comprised of %i landmarks.\nMax filtration parameter: %s.\n" % (len(filtration), max_sb_length, max_filtration_param))
   614                             
   615                             	## Write to output file
   616    708.3 MiB      0.0 MiB   	output_file_name = get_param("out")
   617                             
   618    708.3 MiB      0.0 MiB   	if not output_file_name is None:
   619                             		output_file = open(output_file_name, "w")
   620                             		output_file.truncate()
   621                             		program = get_param("program")
   622                             		if dimension_cutoff is None:
   623                             			print("Writing filtration for input into %s..." % program)
   624                             			dimension_cutoff = number_of_vertices
   625                             		else:
   626                             			print("Writing filtration to file %s for input into %s, ignoring simplices above dimension %i..." % (output_file_name,program, dimension_cutoff))
   627                             		num_lines = 0
   628                             		if program == "Perseus":
   629                             			sets_printed_so_far = Set()
   630                             			num_lines = len(filtration) + 1
   631                             			output_file.write("1\n")
   632                             			list_filtration = None
   633                             			if (sort_output):
   634                             				list_filtration = list(filtration)
   635                             				list_filtration.sort()
   636                             			for simplex_birth in (list_filtration if sort_output else filtration):
   637                             				dimension = len(simplex_birth.landmark_set) - 1
   638                             				if dimension > dimension_cutoff:
   639                             					for subtuple in itertools.combinations(simplex_birth.landmark_set, dimension_cutoff + 1):
   640                             						subset = ImmutableSet(subtuple)
   641                             						if not ((subset, simplex_birth.birth_time) in sets_printed_so_far):
   642                             							output_file.write(str(dimension_cutoff) + " ")
   643                             							for landmark in subset:
   644                             								output_file.write(str(landmark + 1) + " ")
   645                             							output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   646                             							sets_printed_so_far.add((subset, simplex_birth.birth_time))
   647                             				else:
   648                             					if not ((simplex_birth.landmark_set, simplex_birth.birth_time) in sets_printed_so_far):
   649                             						output_file.write(str(dimension) + " ")
   650                             						for landmark in (simplex_birth.sll if sort_output else simplex_birth.landmark_set):
   651                             							output_file.write(str(landmark + 1) + " ")
   652                             						output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   653                             						sets_printed_so_far.add((simplex_birth.landmark_set, simplex_birth.birth_time))
   654                             		elif program == "PHAT":
   655                             			line_map = {}
   656                             			for i in xrange(number_of_vertices - 1):
   657                             				output_file.write("0\n")
   658                             				line_map[ImmutableSet([i])] = i
   659                             			output_file.write("0")
   660                             			line_map[ImmutableSet([number_of_vertices - 1])] = number_of_vertices - 1
   661                             			simultaneous_additions = []
   662                             			class Context: # Note: if upgrading to Python 3, one could just use the nonlocal keyword (see below comment).
   663                             				line_number = number_of_vertices
   664                             			list_filtration = list(filtration)
   665                             			list_filtration.sort()
   666                             			last_birth_time = 0
   667                             			def process_and_get_line_number(s):
   668                             				#nonlocal line_number
   669                             				if s in line_map:
   670                             					return line_map[s]
   671                             				else:
   672                             					dimension = len(s) - 1
   673                             					if dimension > dimension_cutoff:
   674                             						for subset in itertools.combinations(s, dimension_cutoff + 1): # Take all subsets of size dimension_cutoff + 1
   675                             							process_and_get_line_number(ImmutableSet(subset))
   676                             					elif dimension > 0:
   677                             						subsets_line_numbers = []
   678                             						for e in s:
   679                             							subsets_line_numbers.append(process_and_get_line_number(ImmutableSet(s - Set([e]))))
   680                             						output_file.write("\n" + str(dimension))
   681                             						for l in subsets_line_numbers:
   682                             							output_file.write(" " + str(l))
   683                             						line_map[s] = Context.line_number
   684                             						Context.line_number += 1
   685                             						return Context.line_number - 1
   686                             					else:
   687                             						raise Exception("Should have already added single point for base case: " + str(s))
   688                             			for simplex_birth in list_filtration:
   689                             				if simplex_birth.birth_time > last_birth_time:
   690                             					simultaneous_additions.append((Context.line_number - 1, last_birth_time + 1)) # Every line up to and including that line number (indexing starts at 0) had that birth time or earlier (indexing starts at 1)
   691                             					last_birth_time = simplex_birth.birth_time
   692                             				process_and_get_line_number(simplex_birth.landmark_set)
   693                             			simultaneous_additions.append((sys.maxsize, last_birth_time))
   694                             			output_file.write("\n\n# Simultaneous additions: Every line up to and including __ (indexing starts at 0) has birth time __ (or earlier).")
   695                             			for addition in simultaneous_additions:
   696                             				output_file.write("\n# %20i %20i" % addition)
   697                             			extra_data = (extra_data[0], extra_data[1], simultaneous_additions)
   698                             			num_lines = Context.line_number
   699                             		else:
   700                             			raise Exception("Only supported programs are 'Perseus' and 'PHAT'")
   701                             		output_file.close()
   702                             		print("Done. File contains %i lines.\n" % num_lines)
   703    708.3 MiB      0.0 MiB   	print("Filtration has been successfully built!\n")
   704    708.3 MiB      0.0 MiB   	return (filtration, extra_data + (max_filtration_param,), epsilons)


Filename: /home/elliott/programming/PHETS/PH/BuildFiltration.py

Line #    Mem usage    Increment   Line Contents
================================================
    37    790.8 MiB      0.0 MiB   @profile(stream=f)
    38                             def build_filtration(input_file_name, parameter_set, silent=False):
    39    790.8 MiB      0.0 MiB   	num_threads = 2
    40                             	global d
    41    768.8 MiB    -22.0 MiB   	d = []
    42                             
    43                             
    44    789.8 MiB     21.0 MiB   	def get_param(key):
    45    789.8 MiB      0.0 MiB   		return parameter_set.get(key)
    46                             
    47    768.8 MiB    -21.0 MiB   	input_file = open(input_file_name)
    48    768.8 MiB      0.0 MiB   	speed_amplify = float(get_param("d_speed_amplify"))
    49    768.8 MiB      0.0 MiB   	orientation_amplify = float(get_param("d_orientation_amplify"))
    50    768.8 MiB      0.0 MiB   	stretch = float(get_param("d_stretch"))
    51    768.8 MiB      0.0 MiB   	ray_distance_amplify = get_param("d_ray_distance_amplify")
    52    768.8 MiB      0.0 MiB   	use_hamiltonian = get_param("d_use_hamiltonian")
    53    768.8 MiB      0.0 MiB   	print "use hamiltonian set to ", use_hamiltonian
    54    768.8 MiB      0.0 MiB   	m2_d = float(get_param("m2_d"))
    55    768.8 MiB      0.0 MiB   	straight_VB = float(get_param("straight_VB"))
    56    768.8 MiB      0.0 MiB   	d_cov = get_param("d_cov")
    57    768.8 MiB      0.0 MiB   	graph_induced = get_param("graph_induced")
    58                             	# always_euclidean = speed_amplify == orientation_amplify == stretch == ray_distance_amplify == 1.0 and use_hamiltonian == d_cov==0.
    59                             	# print "always_euclidean set to ", always_euclidean
    60    768.8 MiB      0.0 MiB   	always_euclidean = get_param('always_euclidean')
    61                             	# use_hamiltonian = -5
    62                             
    63    768.8 MiB      0.0 MiB   	filtration = Set()
    64    768.8 MiB      0.0 MiB   	extra_data = None
    65    768.8 MiB      0.0 MiB   	min_filtration_param = float(get_param("min_filtration_param"))
    66    768.8 MiB      0.0 MiB   	max_filtration_param = float(get_param("max_filtration_param"))
    67    768.8 MiB      0.0 MiB   	print "Max filtration parameter is ", max_filtration_param
    68    768.8 MiB      0.0 MiB   	if max_filtration_param < 0 and min_filtration_param != 0:
    69                             		raise Exception("Argument 'min_filtration_param' is incompatible with automatic max_filtration_param selection.")
    70    768.8 MiB      0.0 MiB   	number_of_vertices = 0
    71    768.8 MiB      0.0 MiB   	start = get_param("start")
    72    768.8 MiB      0.0 MiB   	worm_length = get_param("worm_length")
    73    768.8 MiB      0.0 MiB   	store_top_simplices = get_param("store_top_simplices")
    74    768.8 MiB      0.0 MiB   	sort_output = get_param("sort_output")
    75    768.8 MiB      0.0 MiB   	absolute = get_param("absolute")
    76    768.8 MiB      0.0 MiB   	num_divisions = get_param("num_divisions")
    77    768.8 MiB      0.0 MiB   	simplex_cutoff = get_param("simplex_cutoff")
    78                             
    79                             
    80                             	'''=============== This code written by Sam ======================'''
    81                             
    82                             	## Read data into witness and landmark lists.
    83    768.8 MiB      0.0 MiB   	witnesses = []
    84    768.8 MiB      0.0 MiB   	landmarks = []
    85    768.8 MiB      0.0 MiB   	landmark_indices = []
    86    768.8 MiB      0.0 MiB   	ls = get_param("landmark_selector")
    87    768.8 MiB      0.0 MiB   	downsample_rate = get_param("ds_rate")
    88    768.8 MiB      0.0 MiB   	maxmin = False
    89    768.8 MiB      0.0 MiB   	counter = 0
    90    768.8 MiB      0.0 MiB   	for i in xrange(start):           #  Where to start reading data
    91                             		input_file.readline()
    92                             		counter+=1
    93    768.8 MiB      0.0 MiB   	landmark_indices=[]
    94                             
    95                             
    96    986.3 MiB    217.6 MiB   	for line in input_file.read().split("\n"):
    97    986.3 MiB      0.0 MiB   		if line != "" and counter>=start:
    98    986.3 MiB      0.0 MiB   			string_witness = line.split(" ")
    99    986.3 MiB      0.0 MiB   			witness = []
   100    986.3 MiB      0.0 MiB   			d.append([])
   101    986.3 MiB      0.0 MiB   			for coordinate in string_witness:
   102    986.3 MiB      0.0 MiB   				if coordinate != "":
   103    986.3 MiB      0.0 MiB   					witness.append(float(coordinate))
   104    986.3 MiB      0.0 MiB   			witnesses.append(witness)
   105    986.3 MiB      0.0 MiB   			counter += 1
   106    986.3 MiB      0.0 MiB   			if counter == worm_length:
   107    768.8 MiB   -217.5 MiB   				break
   108                             
   109    768.8 MiB      0.0 MiB   	number_of_datapoints = len(witnesses)
   110    768.8 MiB      0.0 MiB   	number_of_vertices = int(number_of_datapoints/downsample_rate)
   111    768.8 MiB      0.0 MiB   	num_coordinates = len(witnesses[0])
   112    768.8 MiB      0.0 MiB   	stop = start + counter
   113                             
   114    768.8 MiB      0.0 MiB   	if max_filtration_param < 0:
   115                             		if float(number_of_vertices) < abs(max_filtration_param) + 1:
   116                             			print '''ERROR: 'max_filtration_param' ({}) and number of landmarks ({})
   117                             			are incompatible. Try decreasing 'ds_rate' or increasing 'worm_length'.'''.format(max_filtration_param,
   118                             			number_of_vertices)
   119                             			sys.exit()
   120                             
   121    768.8 MiB      0.0 MiB   	num_threads = 2
   122                             	# for more information about these parameters type ./find_landmarks --help in the terminal
   123                             	# the distance calculations are calculated and outputted to a file called find_landmarks.txt
   124    768.8 MiB      0.0 MiB   	print os.getcwd()
   125                             
   126    768.8 MiB      0.0 MiB   	if ls=="EST":
   127                             		if always_euclidean:
   128                             			if graph_induced:
   129                             				find_landmarks_cmd = [
   130                             					"./find_landmarks",
   131                             					"-n {}".format(num_threads),
   132                             					"-l {}".format(number_of_vertices),
   133                             					"-w {}-{}".format(start,stop),
   134                             					"-i{}".format(input_file_name),
   135                             					"-olandmark_outputs.txt",
   136                             					"-m {}".format(int(m2_d)),
   137                             					"-a {}".format(speed_amplify),
   138                             					"-y {}".format(orientation_amplify),
   139                             					"-h {}".format(use_hamiltonian),
   140                             					"-r {}".format(ray_distance_amplify),
   141                             					"-v {}".format(straight_VB),
   142                             					"-s {}".format(stretch),
   143                             					"-e {}".format(downsample_rate),
   144                             					"-x {}".format(d_cov),
   145                             					"-c",
   146                             					"-f {}".format(max_filtration_param)
   147                             				]
   148                             			else:
   149                             				find_landmarks_cmd = [
   150                             					"./find_landmarks",
   151                             					"-n {}".format(num_threads),
   152                             					"-l {}".format(number_of_vertices),
   153                             					"-w {}-{}".format(start,stop),
   154                             					"-i{}".format(input_file_name),
   155                             					"-olandmark_outputs.txt",
   156                             					"-m {}".format(int(m2_d)),
   157                             					"-a {}".format(speed_amplify),
   158                             					"-y {}".format(orientation_amplify),
   159                             					"-h {}".format(use_hamiltonian),
   160                             					"-r {}".format(ray_distance_amplify),
   161                             					"-v {}".format(straight_VB),
   162                             					"-s {}".format(stretch),
   163                             					"-e {}".format(downsample_rate),
   164                             					"-x {}".format(d_cov),
   165                             					"-c"
   166                             				]
   167                             		else:
   168                             			if graph_induced:
   169                             				find_landmarks_cmd = [
   170                             					"./find_landmarks",
   171                             					"-n {}".format(num_threads),
   172                             					"-l {}".format(number_of_vertices),
   173                             					"-w {}-{}".format(start,stop),
   174                             					"-i{}".format(input_file_name),
   175                             					"-olandmark_outputs.txt",
   176                             					"-m {}".format(int(m2_d)),
   177                             					"-a {}".format(speed_amplify),
   178                             					"-y {}".format(orientation_amplify),
   179                             					"-h {}".format(use_hamiltonian),
   180                             					"-r {}".format(ray_distance_amplify),
   181                             					"-v {}".format(straight_VB),
   182                             					"-s {}".format(stretch),
   183                             					"-x {}".format(d_cov),
   184                             					"-e {}".format(downsample_rate),
   185                             					"-f {}".format(max_filtration_param)
   186                             				]
   187                             			else:
   188                             				find_landmarks_cmd = [
   189                             					"./find_landmarks",
   190                             					"-n {}".format(num_threads),
   191                             					"-l {}".format(number_of_vertices),
   192                             					"-w {}-{}".format(start,stop),
   193                             					"-i{}".format(input_file_name),
   194                             					"-olandmark_outputs.txt",
   195                             					"-m {}".format(int(m2_d)),
   196                             					"-a {}".format(speed_amplify),
   197                             					"-y {}".format(orientation_amplify),
   198                             					"-h {}".format(use_hamiltonian),
   199                             					"-r {}".format(ray_distance_amplify),
   200                             					"-v {}".format(straight_VB),
   201                             					"-s {}".format(stretch),
   202                             					"-x {}".format(d_cov),
   203                             					"-e {}".format(downsample_rate)
   204                             				]
   205                             	else:
   206    768.8 MiB      0.0 MiB   		if always_euclidean and m2_d!=0:
   207                             			if graph_induced:
   208                             				find_landmarks_cmd = [
   209                             					"./find_landmarks",
   210                             					"-n {}".format(num_threads),
   211                             					"-l {}".format(number_of_vertices),
   212                             					"-w {}-{}".format(start,stop),
   213                             					"-i{}".format(input_file_name),
   214                             					"-olandmark_outputs.txt",
   215                             					"-m {}".format(int(m2_d)),
   216                             					"-a {}".format(speed_amplify),
   217                             					"-y {}".format(orientation_amplify),
   218                             					"-h {}".format(use_hamiltonian),
   219                             					"-r {}".format(ray_distance_amplify),
   220                             					"-v {}".format(straight_VB),
   221                             					"-x {}".format(d_cov),
   222                             					"-s {}".format(stretch),
   223                             					"-f {}".format(max_filtration_param)
   224                             				]
   225                             			else:
   226                             				find_landmarks_cmd = [
   227                             					"./find_landmarks",
   228                             					"-n {}".format(num_threads),
   229                             					"-l {}".format(number_of_vertices),
   230                             					"-w {}-{}".format(start,stop),
   231                             					"-i{}".format(input_file_name),
   232                             					"-olandmark_outputs.txt",
   233                             					"-m {}".format(int(m2_d)),
   234                             					"-a {}".format(speed_amplify),
   235                             					"-y {}".format(orientation_amplify),
   236                             					"-h {}".format(use_hamiltonian),
   237                             					"-r {}".format(ray_distance_amplify),
   238                             					"-v {}".format(straight_VB),
   239                             					"-x {}".format(d_cov),
   240                             					"-s {}".format(stretch)
   241                             				]
   242    768.8 MiB      0.0 MiB   		elif always_euclidean:
   243                             			if graph_induced:
   244                             				find_landmarks_cmd = [
   245                             					"./find_landmarks",
   246                             					"-n {}".format(num_threads),
   247                             					"-l {}".format(number_of_vertices),
   248                             					"-w {}-{}".format(start,stop),
   249                             					"-i{}".format(input_file_name),
   250                             					"-olandmark_outputs.txt",
   251                             					"-m {}".format(int(m2_d)),
   252                             					"-a {}".format(speed_amplify),
   253                             					"-y {}".format(orientation_amplify),
   254                             					"-h {}".format(use_hamiltonian),
   255                             					"-r {}".format(ray_distance_amplify),
   256                             					"-v {}".format(straight_VB),
   257                             					"-x {}".format(d_cov),
   258                             					"-s {}".format(stretch),
   259                             					"-c",
   260                             					"-f {}".format(max_filtration_param)
   261                             				]
   262                             			else:
   263                             				find_landmarks_cmd = [
   264                             					"./find_landmarks",
   265                             					"-n {}".format(num_threads),
   266                             					"-l {}".format(number_of_vertices),
   267                             					"-w {}-{}".format(start,stop),
   268                             					"-i{}".format(input_file_name),
   269                             					"-olandmark_outputs.txt",
   270                             					"-m {}".format(int(m2_d)),
   271                             					"-a {}".format(speed_amplify),
   272                             					"-y {}".format(orientation_amplify),
   273                             					"-h {}".format(use_hamiltonian),
   274                             					"-r {}".format(ray_distance_amplify),
   275                             					"-v {}".format(straight_VB),
   276                             					"-x {}".format(d_cov),
   277                             					"-s {}".format(stretch),
   278                             					"-c"
   279                             				]
   280                             		else:
   281    768.8 MiB      0.0 MiB   			if graph_induced:
   282                             				find_landmarks_cmd = [
   283                             					"./find_landmarks",
   284                             					"-n {}".format(num_threads),
   285                             					"-l {}".format(number_of_vertices),
   286                             					"-w {}-{}".format(start,stop),
   287                             					"-i{}".format(input_file_name),
   288                             					"-olandmark_outputs.txt",
   289                             					"-m {}".format(int(m2_d)),
   290                             					"-a {}".format(speed_amplify),
   291                             					"-y {}".format(orientation_amplify),
   292                             					"-h {}".format(use_hamiltonian),
   293                             					"-r {}".format(ray_distance_amplify),
   294                             					"-v {}".format(straight_VB),
   295                             					"-x {}".format(d_cov),
   296                             					"-s {}".format(stretch),
   297                             					"-f {}".format(max_filtration_param)
   298                             				]
   299                             			else:
   300                             				find_landmarks_cmd = [
   301    768.8 MiB      0.0 MiB   					"./find_landmarks",
   302    768.8 MiB      0.0 MiB   					"-n {}".format(num_threads),
   303    768.8 MiB      0.0 MiB   					"-l {}".format(number_of_vertices),
   304    768.8 MiB      0.0 MiB   					"-w {}-{}".format(start,stop),
   305    768.8 MiB      0.0 MiB   					"-i{}".format(input_file_name),
   306    768.8 MiB      0.0 MiB   					"-olandmark_outputs.txt",
   307    768.8 MiB      0.0 MiB   					"-m {}".format(int(m2_d)),
   308    768.8 MiB      0.0 MiB   					"-a {}".format(speed_amplify),
   309    768.8 MiB      0.0 MiB   					"-y {}".format(orientation_amplify),
   310    768.8 MiB      0.0 MiB   					"-h {}".format(use_hamiltonian),
   311    768.8 MiB      0.0 MiB   					"-r {}".format(ray_distance_amplify),
   312    768.8 MiB      0.0 MiB   					"-v {}".format(straight_VB),
   313    768.8 MiB      0.0 MiB   					"-x {}".format(d_cov),
   314    768.8 MiB      0.0 MiB   					"-s {}".format(stretch)
   315                             				]
   316                             
   317    768.8 MiB      0.0 MiB   	print find_landmarks_cmd
   318                             
   319    768.8 MiB      0.0 MiB   	if silent:
   320                             		p = subprocess.Popen(find_landmarks_cmd, stdout=subprocess.PIPE)
   321                             		out, err = p.communicate()
   322                             	else:
   323    768.8 MiB      0.0 MiB   		p = subprocess.Popen(find_landmarks_cmd)
   324    768.8 MiB      0.0 MiB   		p.communicate()
   325                             
   326                             	## Build and sort distance matrix.
   327    768.8 MiB      0.0 MiB   	landmarks_file = open("landmark_outputs.txt","rb")
   328                             
   329    768.8 MiB      0.0 MiB   	l = landmarks_file.readlines()
   330    768.8 MiB      0.0 MiB   	sys.stdout.write("Reading in distance calculations...")
   331    768.8 MiB      0.0 MiB   	sys.stdout.flush()
   332    768.8 MiB      0.0 MiB   	landmark_index = 0
   333    789.5 MiB     20.8 MiB   	for line in l:
   334    789.0 MiB     -0.5 MiB   		f = line.strip('\n')
   335    789.0 MiB      0.0 MiB   		if "#" not in f:
   336    789.0 MiB      0.0 MiB   			landmark = int(f.split(":")[0])
   337                             
   338    789.0 MiB      0.0 MiB   			distances = [float(i) for i in f.split(":")[1].split(",")]
   339    789.5 MiB      0.5 MiB   			for witness_index in range(0,len(distances)):
   340                             
   341    789.5 MiB      0.0 MiB   				d[witness_index].append(LandmarkDistance(landmark_index,distances[witness_index]))
   342    789.5 MiB      0.0 MiB   			landmarks.append(witnesses[landmark])
   343    789.5 MiB      0.0 MiB   			landmark_indices.append(landmark)
   344    789.5 MiB      0.0 MiB   			landmark_index+=1
   345                             
   346    789.5 MiB      0.0 MiB   	assert(len(d)>0)
   347    789.5 MiB      0.0 MiB   	sys.stdout.write("done\n")
   348    789.5 MiB      0.0 MiB   	sys.stdout.flush()
   349                             
   350                             
   351                             
   352    789.5 MiB      0.0 MiB   	sys.stdout.write("Sorting distances...")
   353    789.5 MiB      0.0 MiB   	sys.stdout.flush()
   354                             
   355                             	# p=multiprocessing.Pool(processes=4)   # commented out by Elliott 4/25
   356                             
   357    789.5 MiB      0.0 MiB   	inputs=[]
   358    789.5 MiB      0.0 MiB   	for w in range(0,len(witnesses)):
   359    789.5 MiB      0.0 MiB   		inputs.append(w)
   360    789.5 MiB      0.0 MiB   		d[w].sort()
   361                             
   362                             	# p.map(sort,inputs)  # was commented out as of 4/25
   363                             	# p.terminate()       # added by Elliott 4/25
   364                             
   365    789.5 MiB      0.0 MiB   	sys.stdout.write("done\n")
   366    789.5 MiB      0.0 MiB   	sys.stdout.flush()
   367    789.5 MiB      0.0 MiB   	assert len(landmarks) == number_of_vertices
   368                             
   369                             	'''=============== End code written by Sam ======================'''
   370                             
   371                             	'''============= Start code written by Elliott =================='''
   372    789.5 MiB      0.0 MiB   	if graph_induced:
   373                             		# import matplotlib.pyplot as plt
   374                             		import pandas as pd
   375                             
   376                             		g = nx.read_edgelist('edgelist.txt')
   377                             
   378                             		closest_wits = np.loadtxt('closest_wits.txt', dtype='int')	# witness, landmark
   379                             		wit_coords = np.array(witnesses)
   380                             		land_coords = np.array(landmarks)
   381                             
   382                             		# land = np.unique(closest_wits[:,1])
   383                             
   384                             		# closest_wits = pd.DataFrame(closest_wits, columns=('witness', 'landmark'))
   385                             		# print closest_wits
   386                             		# closest_wits = closest_wits.sort_values(by='landmark')
   387                             		# print closest_wits
   388                             		#
   389                             		# closest_wits = closest_wits.values
   390                             		# print closest_wits
   391                             
   392                             
   393                             
   394                             		# fig = plt.figure(figsize=(8, 8))
   395                             		# ax = fig.add_subplot(111)
   396                             		# ax.scatter(wit_coords[:, 0], wit_coords[:, 1], s=.1)
   397                             		# ax.scatter(land_coords[:, 0], land_coords[:, 1])
   398                             		# fig.savefig('veronoi_test.png')
   399                             
   400                             	'''=============== End code written by Elliott =================='''
   401                             
   402                             
   403                             
   404                             
   405    789.5 MiB      0.0 MiB   	print("Building filtration...")
   406                             	## Build filtration
   407    789.5 MiB      0.0 MiB   	weak = get_param("weak")
   408    789.5 MiB      0.0 MiB   	dimension_cutoff = get_param("dimension_cutoff")
   409    789.5 MiB      0.0 MiB   	reentry_filter = get_param("reentry_filter")
   410    789.5 MiB      0.0 MiB   	if get_param("connect_time_1_skeleton") or reentry_filter: # Connect time-1-skeleton
   411                             		for i in xrange(number_of_vertices - 1):
   412                             			filtration.add(SimplexBirth(ImmutableSet([i, i + 1]), 0, sort_output))
   413    789.5 MiB      0.0 MiB   	use_cliques = get_param("use_cliques")
   414    789.5 MiB      0.0 MiB   	use_twr = get_param("use_twr")
   415                             
   416    789.5 MiB      0.0 MiB   	print '%s' % use_twr
   417    789.5 MiB      0.0 MiB   	if use_cliques: # AKA "Lazy" witness relation.
   418                             		g = nx.Graph()
   419                             		for l in xrange(number_of_vertices):
   420                             			g.add_node(l)
   421    789.5 MiB      0.0 MiB   	def filter_and_build():
   422                             		g2 = None
   423                             		if reentry_filter:
   424                             			g2 = g.copy()
   425                             			to_remove = Set()
   426                             			for l1 in xrange(number_of_vertices):
   427                             				l2 = l1 + 2
   428                             				while l2 < number_of_vertices and g2.has_edge(l1, l2):
   429                             					to_remove.add(ImmutableSet([l1, l2]))
   430                             					l2 += 1
   431                             			for edge in to_remove:
   432                             				g2.remove_edge(*tuple(edge)) # May cause weird things to happen because removing edges doesn't remove them from the filtration.
   433                             		else:
   434                             			g2 = g
   435                             		for clique in nx.find_cliques(g2):
   436                             			filtration.add(SimplexBirth(clique, q, sort_output))
   437    789.5 MiB      0.0 MiB   	if weak: # Builds filtration based on k nearest neighbors.
   438                             		if max_filtration_param % 1 != 0:
   439                             			raise Exception("Argument 'max_filtration_param' must be an integer if using the weak witness relation.")
   440                             		max_filtration_param = int(max_filtration_param)
   441                             		for k in xrange(int(math.fabs(max_filtration_param))):
   442                             			for witness_index in xrange(number_of_datapoints):
   443                             				if use_cliques:
   444                             					for i in xrange(k):
   445                             						g.add_edge(d[witness_index][i].id_num, d[witness_index][k].id_num)
   446                             				elif store_top_simplices:
   447                             					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(k + 1)], k, sort_output))
   448                             				else:
   449                             					if progress > 0:
   450                             						for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(k)], min(k, dimension_cutoff)):
   451                             							new_subset = ImmutableSet(base + (d[witness_index][k].id_num,))
   452                             							filtration.add(SimplexBirth(new_subset, k, sort_output))
   453                             			if use_cliques:
   454                             				filter_and_build()
   455    789.5 MiB      0.0 MiB   	if use_twr:
   456                             		print 'Using TWR'
   457                             		if max_filtration_param < 0: # Automatically determine max.
   458                             			depth = int(-max_filtration_param)
   459                             			min_distance = None
   460                             			for w in xrange(number_of_datapoints):
   461                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   462                             				if min_distance is None or new_distance < min_distance:
   463                             					min_distance = new_distance
   464                             			max_filtration_param = min_distance
   465                             		print 'The max_filtration_param is %d ' % max_filtration_param
   466                             		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   467                             		print 'The step size is %f ' % step
   468                             		print 'There will be %d steps in the filtration' % num_divisions
   469                             		progress_index = [0]*number_of_datapoints
   470                             		done = False
   471                             
   472                             		good_landmarks = [[] for x in range(number_of_datapoints)]
   473                             
   474                             		epsilons = []
   475                             		for q in xrange(num_divisions):
   476                             			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   477                             			print 'The threshold is currently %f' % threshold
   478                             			epsilons.append(threshold)
   479                             			Pre_landmarks = []
   480                             			for witness_index in xrange(number_of_datapoints):
   481                             				pre_landmarks = []
   482                             				add_simplex = False
   483                             				progress = 0
   484                             				while True:
   485                             					progress = progress_index[witness_index]
   486                             					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   487                             						break
   488                             					if progress == number_of_vertices:
   489                             						done = True
   490                             						break
   491                             
   492                             					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   493                             						pre_landmarks.append(d[witness_index][progress].id_num) # PRE_LANDMARKS CONTAINS ID NUMBER
   494                             						progress_index[witness_index] += 1
   495                             					else:
   496                             
   497                             						pre_landmarks_size = len(pre_landmarks)
   498                             						pre_landmarks_string = str(pre_landmarks) # MAKE LIST TO STRING
   499                             						print 'At threshold value %f, witness %d has %d associated landmarks: %s ' % (threshold, witness_index, pre_landmarks_size, pre_landmarks_string)
   500                             						break
   501                             				Pre_landmarks.append(pre_landmarks)
   502                             				Pre_landmarks_size = len(Pre_landmarks)
   503                             
   504                             
   505                             
   506                             			for witness_index in xrange(number_of_datapoints - downsample_rate):
   507                             				if len(Pre_landmarks[witness_index]) == 1:
   508                             					set_range = 1
   509                             				else:
   510                             					set_range = len(Pre_landmarks[witness_index])
   511                             				for k in range(set_range):
   512                             					current_pre_landmark = Pre_landmarks[witness_index][k]
   513                             					next_pre_landmark = Pre_landmarks[witness_index][k]+1 # CHECKS ONE STEP UP FROM ID NUMBER *** SEE JAMIE'S COMMENTS ON SUCH OR CLARIFY ***
   514                             					#	print 'current pre landmark = %d , next pre landmark = %d' % (current_pre_landmark, next_pre_landmark)
   515                             					check_pre_landmark = str(Pre_landmarks[witness_index + downsample_rate]) # HMMMMM
   516                             					#		print 'We are considering the fate of landmark %d witnessed by witness %d...' % (current_pre_landmark, witness_index)
   517                             					#		print 'Should witness %d not witness landmark %d, it will be GONE!' % (witness_index + downsample_rate, current_pre_landmark + 1,)
   518                             					#		print 'Witness %d has landmark set %s' % (witness_index + downsample_rate, check_pre_landmark)
   519                             					print (Pre_landmarks[witness_index][k]) in Pre_landmarks[witness_index]
   520                             					if (Pre_landmarks[witness_index][k]+ 1) in Pre_landmarks[witness_index + downsample_rate]: # change from 1, downsample_rate to 0 to test!
   521                             						good_landmarks[witness_index].append(Pre_landmarks[witness_index][k])
   522                             				print 'Up to threshold value %f, witness %d has landmark set %s' % (threshold, witness_index, str(good_landmarks[witness_index]))
   523                             				if use_cliques:
   524                             					for i in xrange(len(good_landmarks[witness_index])):
   525                             						for j in xrange(i+1,len(good_landmarks[witness_index])):
   526                             							g.add_edge(good_landmarks[witness_index][i], good_landmarks[witness_index][j])
   527                             				else:
   528                             					if not store_top_simplices and len(good_landmarks[witness_index]) > 0:
   529                             						for base in itertools.combinations(good_landmarks[witness_index], min(len(good_landmarks[witness_index]), dimension_cutoff)):
   530                             							new_subset = ImmutableSet(base + (good_landmarks[witness_index][i],))
   531                             							filtration.add(SimplexBirth(new_subset, q, sort_output))
   532                             					add_simplex = True
   533                             				if (not use_cliques) and store_top_simplices and add_simplex and len(good_landmarks[witness_index])>= 2:
   534                             					filtration.add(SimplexBirth([good_landmarks[witness_index][i] for i in xrange(len(good_landmarks[witness_index]))], q, sort_output))
   535                             				if done:
   536                             					break
   537                             			if use_cliques:
   538                             				filter_and_build()
   539                             			if done:
   540                             				break
   541                             			#	print 'We are done with threshold %f' % threshold
   542                             	else:
   543    789.5 MiB      0.0 MiB   		if max_filtration_param < 0: # Automatically determine max.
   544                             			depth = int(-max_filtration_param)
   545                             			min_distance = None
   546                             			for w in xrange(number_of_datapoints):
   547                             				new_distance = d[w][depth].distance - (0 if absolute else d[w][0].distance)
   548                             				if min_distance is None or new_distance < min_distance:
   549                             					min_distance = new_distance
   550                             					if min_distance ==0:
   551                             						print "witness ",w
   552                             			max_filtration_param = min_distance
   553                             
   554    789.5 MiB      0.0 MiB   		step = float(max_filtration_param - min_filtration_param)/float(num_divisions) # Change in epsilon at each step.
   555    789.5 MiB      0.0 MiB   		progress_index = [0]*number_of_datapoints
   556    789.5 MiB      0.0 MiB   		done = False
   557    789.5 MiB      0.0 MiB   		epsilons = []
   558    789.8 MiB      0.2 MiB   		for q in xrange(num_divisions):
   559    789.8 MiB      0.0 MiB   			threshold = (max_filtration_param if q == num_divisions - 1 else float(q + 1)*step + min_filtration_param)
   560    789.8 MiB      0.0 MiB   			print 'The threshold is currently %f' % threshold
   561    789.8 MiB      0.0 MiB   			epsilons.append(threshold)
   562    789.8 MiB      0.0 MiB   			for witness_index in xrange(number_of_datapoints):
   563    789.8 MiB      0.0 MiB   				add_simplex = False
   564    789.8 MiB      0.0 MiB   				progress = 0
   565    789.8 MiB      0.0 MiB   				while True:
   566    789.8 MiB      0.0 MiB   					progress = progress_index[witness_index]
   567    789.8 MiB      0.0 MiB   					if simplex_cutoff > 0 and progress >= simplex_cutoff:
   568                             						break
   569    789.8 MiB      0.0 MiB   					if progress == number_of_vertices:
   570                             						done = True
   571                             						break
   572    789.8 MiB      0.0 MiB   					if d[witness_index][progress].distance < threshold + (0 if absolute else d[witness_index][0].distance):
   573    789.8 MiB      0.0 MiB   						if use_cliques:
   574                             							for i in xrange(progress):
   575                             								g.add_edge(d[witness_index][i].id_num, d[witness_index][progress].id_num)
   576                             						else:
   577    789.8 MiB      0.0 MiB   							if not store_top_simplices and progress > 0:
   578                             								for base in itertools.combinations([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], min(progress, dimension_cutoff)):
   579                             									new_subset = ImmutableSet(base + (d[witness_index][progress].id_num,))
   580                             									filtration.add(SimplexBirth(new_subset, q, sort_output))
   581    789.8 MiB      0.0 MiB   							add_simplex = True
   582    789.8 MiB      0.0 MiB   						progress_index[witness_index] += 1
   583                             					else:
   584    789.8 MiB      0.0 MiB   						break
   585    789.8 MiB      0.0 MiB   				if (not use_cliques) and store_top_simplices and add_simplex and progress >= 2:
   586    789.8 MiB      0.0 MiB   					list_o_landmarks = []
   587    789.8 MiB      0.0 MiB   					for landmark_index in xrange(progress):
   588    789.8 MiB      0.0 MiB   						list_o_landmarks.append(d[witness_index][landmark_index].id_num)
   589                             					#print 'At threshold %f, witness %d has landmark set %s' % (threshold, witness_index, str(list_o_landmarks))
   590    789.8 MiB      0.0 MiB   					filtration.add(SimplexBirth([d[witness_index][landmark_index].id_num for landmark_index in xrange(progress)], q, sort_output))
   591    789.8 MiB      0.0 MiB   				if done:
   592                             					break
   593    789.8 MiB      0.0 MiB   			if use_cliques:
   594                             				filter_and_build()
   595    789.8 MiB      0.0 MiB   			if done:
   596                             				break
   597                             
   598                             
   599    789.8 MiB      0.0 MiB   	extra_data = (landmarks, witnesses)
   600    789.8 MiB      0.0 MiB   	if weak:
   601                             		max_epsilon = 0.0
   602                             		for w in xrange(number_of_datapoints):
   603                             			#print("type: %s" % type(d[w][max_filtration_param - 1].distance))
   604                             			#print("value: %f" % d[w][max_filtration_param - 1].distance)
   605                             			if (d[w][max_filtration_param - 1].distance) > max_epsilon:
   606                             				max_epsilon = d[w][max_filtration_param - 1].distance
   607                             		print("Done. Filtration contains %i top simplex birth events, with the largest epsilon equal to %f.\n" % (len(filtration), max_epsilon))
   608                             	else:
   609    789.8 MiB      0.0 MiB   		max_sb_length = 0
   610    789.8 MiB      0.0 MiB   		for sb in filtration:
   611    789.8 MiB      0.0 MiB   			if len(sb.landmark_set) > max_sb_length:
   612    789.8 MiB      0.0 MiB   				max_sb_length = len(sb.landmark_set)
   613    789.8 MiB      0.0 MiB   		print("Done. Filtration contains %i top simplex birth events, with the largest one comprised of %i landmarks.\nMax filtration parameter: %s.\n" % (len(filtration), max_sb_length, max_filtration_param))
   614                             
   615                             	## Write to output file
   616    789.8 MiB      0.0 MiB   	output_file_name = get_param("out")
   617                             
   618    789.8 MiB      0.0 MiB   	if not output_file_name is None:
   619                             		output_file = open(output_file_name, "w")
   620                             		output_file.truncate()
   621                             		program = get_param("program")
   622                             		if dimension_cutoff is None:
   623                             			print("Writing filtration for input into %s..." % program)
   624                             			dimension_cutoff = number_of_vertices
   625                             		else:
   626                             			print("Writing filtration to file %s for input into %s, ignoring simplices above dimension %i..." % (output_file_name,program, dimension_cutoff))
   627                             		num_lines = 0
   628                             		if program == "Perseus":
   629                             			sets_printed_so_far = Set()
   630                             			num_lines = len(filtration) + 1
   631                             			output_file.write("1\n")
   632                             			list_filtration = None
   633                             			if (sort_output):
   634                             				list_filtration = list(filtration)
   635                             				list_filtration.sort()
   636                             			for simplex_birth in (list_filtration if sort_output else filtration):
   637                             				dimension = len(simplex_birth.landmark_set) - 1
   638                             				if dimension > dimension_cutoff:
   639                             					for subtuple in itertools.combinations(simplex_birth.landmark_set, dimension_cutoff + 1):
   640                             						subset = ImmutableSet(subtuple)
   641                             						if not ((subset, simplex_birth.birth_time) in sets_printed_so_far):
   642                             							output_file.write(str(dimension_cutoff) + " ")
   643                             							for landmark in subset:
   644                             								output_file.write(str(landmark + 1) + " ")
   645                             							output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   646                             							sets_printed_so_far.add((subset, simplex_birth.birth_time))
   647                             				else:
   648                             					if not ((simplex_birth.landmark_set, simplex_birth.birth_time) in sets_printed_so_far):
   649                             						output_file.write(str(dimension) + " ")
   650                             						for landmark in (simplex_birth.sll if sort_output else simplex_birth.landmark_set):
   651                             							output_file.write(str(landmark + 1) + " ")
   652                             						output_file.write(str(simplex_birth.birth_time + 1) + "\n")
   653                             						sets_printed_so_far.add((simplex_birth.landmark_set, simplex_birth.birth_time))
   654                             		elif program == "PHAT":
   655                             			line_map = {}
   656                             			for i in xrange(number_of_vertices - 1):
   657                             				output_file.write("0\n")
   658                             				line_map[ImmutableSet([i])] = i
   659                             			output_file.write("0")
   660                             			line_map[ImmutableSet([number_of_vertices - 1])] = number_of_vertices - 1
   661                             			simultaneous_additions = []
   662                             			class Context: # Note: if upgrading to Python 3, one could just use the nonlocal keyword (see below comment).
   663                             				line_number = number_of_vertices
   664                             			list_filtration = list(filtration)
   665                             			list_filtration.sort()
   666                             			last_birth_time = 0
   667                             			def process_and_get_line_number(s):
   668                             				#nonlocal line_number
   669                             				if s in line_map:
   670                             					return line_map[s]
   671                             				else:
   672                             					dimension = len(s) - 1
   673                             					if dimension > dimension_cutoff:
   674                             						for subset in itertools.combinations(s, dimension_cutoff + 1): # Take all subsets of size dimension_cutoff + 1
   675                             							process_and_get_line_number(ImmutableSet(subset))
   676                             					elif dimension > 0:
   677                             						subsets_line_numbers = []
   678                             						for e in s:
   679                             							subsets_line_numbers.append(process_and_get_line_number(ImmutableSet(s - Set([e]))))
   680                             						output_file.write("\n" + str(dimension))
   681                             						for l in subsets_line_numbers:
   682                             							output_file.write(" " + str(l))
   683                             						line_map[s] = Context.line_number
   684                             						Context.line_number += 1
   685                             						return Context.line_number - 1
   686                             					else:
   687                             						raise Exception("Should have already added single point for base case: " + str(s))
   688                             			for simplex_birth in list_filtration:
   689                             				if simplex_birth.birth_time > last_birth_time:
   690                             					simultaneous_additions.append((Context.line_number - 1, last_birth_time + 1)) # Every line up to and including that line number (indexing starts at 0) had that birth time or earlier (indexing starts at 1)
   691                             					last_birth_time = simplex_birth.birth_time
   692                             				process_and_get_line_number(simplex_birth.landmark_set)
   693                             			simultaneous_additions.append((sys.maxsize, last_birth_time))
   694                             			output_file.write("\n\n# Simultaneous additions: Every line up to and including __ (indexing starts at 0) has birth time __ (or earlier).")
   695                             			for addition in simultaneous_additions:
   696                             				output_file.write("\n# %20i %20i" % addition)
   697                             			extra_data = (extra_data[0], extra_data[1], simultaneous_additions)
   698                             			num_lines = Context.line_number
   699                             		else:
   700                             			raise Exception("Only supported programs are 'Perseus' and 'PHAT'")
   701                             		output_file.close()
   702                             		print("Done. File contains %i lines.\n" % num_lines)
   703    789.8 MiB      0.0 MiB   	print("Filtration has been successfully built!\n")
   704    789.8 MiB      0.0 MiB   	return (filtration, extra_data + (max_filtration_param,), epsilons)


